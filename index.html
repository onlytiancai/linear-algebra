<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>The Little Book of Linear Algebra</title>
  <link rel="stylesheet" href="css/github-markdown.css">
<link rel="stylesheet" href="libs//katex/katex.min.css">
<script defer src="libs//katex/katex.min.js"></script>
<script defer src="libs//katex/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>

</head>
<body>
<article class="markdown-body">
<p>https://github.com/little-book-of/linear-algebra</p>
<h1 id="the-little-book-of-linear-algebra">The Little Book of Linear
Algebra</h1>
<p>çº¿æ€§ä»£æ•°å°å†Œå­</p>
<p>A concise, beginner-friendly introduction to the core ideas of linear
algebra. ç®€æ´ã€é€‚åˆåˆå­¦è€…çš„çº¿æ€§ä»£æ•°æ ¸å¿ƒæ€æƒ³ä»‹ç»ã€‚</p>
<h1 id="chapter-1.-vectors">Chapter 1. Vectors</h1>
<p>ç¬¬ 1 ç«  å‘é‡</p>
<h2 id="scalars-and-vectors">1.1 Scalars and Vectors</h2>
<p>1.1 æ ‡é‡å’ŒçŸ¢é‡</p>
<p>A scalar is a single numerical quantity, most often taken from the
real numbers, denoted by <span
class="math inline">\(\mathbb{R}\)</span>. Scalars are the fundamental
building blocks of arithmetic: they can be added, subtracted,
multiplied, and, except in the case of zero, divided. In linear algebra,
scalars play the role of coefficients, scaling factors, and entries of
larger structures such as vectors and matrices. They provide the weights
by which more complex objects are measured and combined. A vector is an
ordered collection of scalars, arranged either in a row or a column.
When the scalars are real numbers, the vector is said to belong to
<em>real</em> <span class="math inline">\(n\)</span>-dimensional space,
written æ ‡é‡æ˜¯ä¸€ä¸ªå•ä¸€çš„æ•°å€¼ï¼Œé€šå¸¸å–è‡ªå®æ•°ï¼Œç”¨ <span
class="math inline">\(\mathbb{R}\)</span>
è¡¨ç¤ºã€‚æ ‡é‡æ˜¯ç®—æœ¯çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼šå®ƒä»¬å¯ä»¥è¿›è¡ŒåŠ ã€å‡ã€ä¹˜å’Œé™¤ï¼ˆé›¶é™¤å¤–ï¼‰ã€‚åœ¨çº¿æ€§ä»£æ•°ä¸­ï¼Œæ ‡é‡å……å½“ç³»æ•°ã€æ¯”ä¾‹å› å­ä»¥åŠå‘é‡å’ŒçŸ©é˜µç­‰æ›´å¤§ç»“æ„ä¸­çš„å…ƒç´ ã€‚å®ƒä»¬æä¾›æƒé‡ï¼Œç”¨äºæµ‹é‡å’Œç»„åˆæ›´å¤æ‚çš„å¯¹è±¡ã€‚å‘é‡æ˜¯æŒ‰è¡Œæˆ–åˆ—æ’åˆ—çš„æ ‡é‡çš„æœ‰åºé›†åˆã€‚å½“æ ‡é‡ä¸ºå®æ•°æ—¶ï¼Œè¯¥å‘é‡è¢«ç§°ä¸ºå±äº<em>å®</em>
<span class="math inline">\(n\)</span> ç»´ç©ºé—´ï¼Œå†™ä¸º</p>
<p><span class="math display">\[
\mathbb{R}^n = \{ (x_1, x_2, \dots, x_n) \mid x_i \in \mathbb{R} \}.
\]</span></p>
<p>An element of <span class="math inline">\(\mathbb{R}^n\)</span> is
called a vector of dimension <span class="math inline">\(n\)</span> or
an <em>n</em>-vector. The number <span class="math inline">\(n\)</span>
is called the dimension of the vector space. Thus <span
class="math inline">\(\mathbb{R}^2\)</span> is the space of all ordered
pairs of real numbers, <span class="math inline">\(\mathbb{R}^3\)</span>
the space of all ordered triples, and so on. <span
class="math inline">\(\mathbb{R}^n\)</span> ä¸­çš„ä¸€ä¸ªå…ƒç´ ç§°ä¸ºç»´åº¦ä¸º <span
class="math inline">\(n\)</span> çš„å‘é‡æˆ– <em>n</em> å‘é‡ã€‚æ•°å­— <span
class="math inline">\(n\)</span> ç§°ä¸ºå‘é‡ç©ºé—´çš„ç»´æ•°ã€‚å› æ­¤ï¼Œ <span
class="math inline">\(\mathbb{R}^2\)</span> æ˜¯æ‰€æœ‰æœ‰åºå®æ•°å¯¹çš„ç©ºé—´ï¼Œ
<span class="math inline">\(\mathbb{R}^3\)</span>
æ˜¯æ‰€æœ‰æœ‰åºä¸‰å…ƒç»„çš„ç©ºé—´ï¼Œç­‰ç­‰ã€‚</p>
<p>Example 1.1.1. ä¾‹ 1.1.1ã€‚</p>
<ul>
<li>A 2-dimensional vector: <span class="math inline">\((3, -1) \in
\mathbb{R}^2\)</span>. äºŒç»´å‘é‡ï¼š <span class="math inline">\((3, -1)
\in \mathbb{R}^2\)</span> ã€‚</li>
<li>A 3-dimensional vector: <span class="math inline">\((2, 0, 5) \in
\mathbb{R}^3\)</span>. ä¸‰ç»´å‘é‡ï¼š <span class="math inline">\((2, 0, 5)
\in \mathbb{R}^3\)</span> ã€‚</li>
<li>A 1-dimensional vector: <span class="math inline">\((7) \in
\mathbb{R}^1\)</span>, which corresponds to the scalar 7 itself.
ä¸€ç»´å‘é‡ï¼š <span class="math inline">\((7) \in \mathbb{R}^1\)</span>
ï¼Œå¯¹åº”äºæ ‡é‡ 7 æœ¬èº«ã€‚</li>
</ul>
<p>Vectors are often written vertically in column form, which emphasizes
their role in matrix multiplication:
å‘é‡é€šå¸¸ä»¥åˆ—çš„å½¢å¼å‚ç›´ä¹¦å†™ï¼Œè¿™å¼ºè°ƒäº†å®ƒä»¬åœ¨çŸ©é˜µä¹˜æ³•ä¸­çš„ä½œç”¨ï¼š</p>
<p><span class="math display">\[
\mathbf{v} = \begin{bmatrix}2 \\0 \\5 \end{bmatrix} \in \mathbb{R}^3.
\]</span></p>
<p>The vertical layout makes the structure clearer when we consider
linear combinations or multiply matrices by vectors.
å½“æˆ‘ä»¬è€ƒè™‘çº¿æ€§ç»„åˆæˆ–çŸ©é˜µä¹˜ä»¥å‘é‡æ—¶ï¼Œå‚ç›´å¸ƒå±€ä½¿ç»“æ„æ›´åŠ æ¸…æ™°ã€‚</p>
<h3 id="geometric-interpretation">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>In <span class="math inline">\(\mathbb{R}^2\)</span>, a vector <span
class="math inline">\((x_1, x_2)\)</span> can be visualized as an arrow
starting at the origin <span class="math inline">\((0,0)\)</span> and
ending at the point <span class="math inline">\((x_1, x_2)\)</span>. Its
length corresponds to the distance from the origin, and its orientation
gives a direction in the plane. In <span
class="math inline">\(\mathbb{R}^3\)</span>, the same picture extends
into three dimensions: a vector is an arrow from the origin to <span
class="math inline">\((x_1, x_2, x_3)\)</span>. Beyond three dimensions,
direct visualization is no longer possible, but the algebraic rules of
vectors remain identical. Even though we cannot draw a vector in <span
class="math inline">\(\mathbb{R}^{10}\)</span>, it behaves under
addition, scaling, and transformation exactly as a 2- or 3-dimensional
vector does. This abstract point of view is what allows linear algebra
to apply to data science, physics, and machine learning, where data
often lives in very high-dimensional spaces. Thus a vector may be
regarded in three complementary ways: åœ¨ <span
class="math inline">\(\mathbb{R}^2\)</span> ä¸­ï¼Œå‘é‡ <span
class="math inline">\((x_1, x_2)\)</span> å¯ä»¥å¯è§†åŒ–ä¸ºä¸€ä¸ªä»åŸç‚¹ <span
class="math inline">\((0,0)\)</span> å¼€å§‹åˆ°ç‚¹ <span
class="math inline">\((x_1, x_2)\)</span>
ç»“æŸçš„ç®­å¤´ã€‚å®ƒçš„é•¿åº¦å¯¹åº”äºä¸åŸç‚¹çš„è·ç¦»ï¼Œå…¶æ–¹å‘ç»™å‡ºäº†å¹³é¢å†…çš„æ–¹å‘ã€‚åœ¨
<span class="math inline">\(\mathbb{R}^3\)</span>
ä¸­ï¼ŒåŒæ ·çš„å›¾åƒå»¶ä¼¸åˆ°ä¸‰ç»´ç©ºé—´ï¼šå‘é‡æ˜¯ä¸€ä¸ªä»åŸç‚¹æŒ‡å‘ <span
class="math inline">\((x_1, x_2, x_3)\)</span>
çš„ç®­å¤´ã€‚è¶…è¿‡ä¸‰ç»´ç©ºé—´åï¼Œç›´æ¥å¯è§†åŒ–å°±ä¸å†å¯èƒ½ï¼Œä½†å‘é‡çš„ä»£æ•°è§„åˆ™ä¿æŒä¸å˜ã€‚å³ä½¿æˆ‘ä»¬æ— æ³•åœ¨
<span class="math inline">\(\mathbb{R}^{10}\)</span>
ä¸­ç»˜åˆ¶å‘é‡ï¼Œå®ƒåœ¨åŠ æ³•ã€ç¼©æ”¾å’Œå˜æ¢ä¸‹çš„è¡Œä¸ºä¸äºŒç»´æˆ–ä¸‰ç»´å‘é‡å®Œå…¨ç›¸åŒã€‚è¿™ç§æŠ½è±¡çš„è§‚ç‚¹ä½¿å¾—çº¿æ€§ä»£æ•°èƒ½å¤Ÿåº”ç”¨äºæ•°æ®ç§‘å­¦ã€ç‰©ç†å­¦å’Œæœºå™¨å­¦ä¹ ï¼Œè¿™äº›é¢†åŸŸçš„æ•°æ®é€šå¸¸å­˜åœ¨äºéâ€‹â€‹å¸¸é«˜ç»´çš„ç©ºé—´ä¸­ã€‚å› æ­¤ï¼Œå‘é‡å¯ä»¥ä»ä¸‰ä¸ªäº’è¡¥çš„è§’åº¦æ¥çœ‹å¾…ï¼š</p>
<ol type="1">
<li>As a point in space, described by its coordinates.
ä½œä¸ºç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œç”±å…¶åæ ‡æè¿°ã€‚</li>
<li>As a displacement or arrow, described by a direction and a length.
ä½œä¸ºä½ç§»æˆ–ç®­å¤´ï¼Œç”±æ–¹å‘å’Œé•¿åº¦æè¿°ã€‚</li>
<li>As an abstract element of a vector space, whose properties follow
algebraic rules independent of geometry.
ä½œä¸ºå‘é‡ç©ºé—´çš„æŠ½è±¡å…ƒç´ ï¼Œå…¶å±æ€§éµå¾ªä¸å‡ ä½•æ— å…³çš„ä»£æ•°è§„åˆ™ã€‚</li>
</ol>
<h3 id="notation">Notation</h3>
<p>ç¬¦å·</p>
<ul>
<li>Vectors are written in boldface lowercase letters: <span
class="math inline">\(\mathbf{v}, \mathbf{w}, \mathbf{x}\)</span>.
å‘é‡ä»¥ç²—ä½“å°å†™å­—æ¯è¡¨ç¤ºï¼š <span class="math inline">\(\mathbf{v},
\mathbf{w}, \mathbf{x}\)</span> ã€‚</li>
<li>The <em>i</em>-th entry of a vector <span
class="math inline">\(\mathbf{v}\)</span> is written <span
class="math inline">\(v_i\)</span>, where indices begin at 1. å‘é‡ <span
class="math inline">\(\mathbf{v}\)</span> çš„ç¬¬ - ä¸ªå…ƒç´ å†™ä¸º ğ‘£ ğ‘– v i â€‹
ï¼Œå…¶ä¸­ç´¢å¼•ä» 1 å¼€å§‹ã€‚</li>
<li>The set of all <em>n</em>-dimensional vectors over <span
class="math inline">\(\mathbb{R}\)</span> is denoted <span
class="math inline">\(\mathbb{R}^n\)</span>. <span
class="math inline">\(\mathbb{R}\)</span> ä¸Šçš„æ‰€æœ‰ <em>n</em>
ç»´å‘é‡çš„é›†åˆè®°ä¸º <span class="math inline">\(\mathbb{R}^n\)</span>
ã€‚</li>
<li>Column vectors will be the default form unless otherwise stated.
é™¤éå¦æœ‰è¯´æ˜ï¼Œåˆ—å‘é‡å°†æ˜¯é»˜è®¤å½¢å¼ã€‚</li>
</ul>
<h3 id="why-begin-here">Why begin here?</h3>
<p>ä¸ºä»€ä¹ˆä»è¿™é‡Œå¼€å§‹ï¼Ÿ</p>
<p>Scalars and vectors form the atoms of linear algebra. Every structure
we will build-vector spaces, linear transformations, matrices,
eigenvalues-relies on the basic notions of number and ordered collection
of numbers. Once vectors are understood, we can define operations such
as addition and scalar multiplication, then generalize to subspaces,
bases, and coordinate systems. Eventually, this framework grows into the
full theory of linear algebra, with powerful applications to geometry,
computation, and data.
æ ‡é‡å’Œå‘é‡æ„æˆäº†çº¿æ€§ä»£æ•°çš„åŸå­ã€‚æˆ‘ä»¬å°†è¦æ„å»ºçš„æ¯ä¸€ä¸ªç»“æ„â€”â€”å‘é‡ç©ºé—´ã€çº¿æ€§å˜æ¢ã€çŸ©é˜µã€ç‰¹å¾å€¼â€”â€”éƒ½ä¾èµ–äºæ•°å’Œæœ‰åºæ•°é›†çš„åŸºæœ¬æ¦‚å¿µã€‚ä¸€æ—¦ç†è§£äº†å‘é‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰è¯¸å¦‚åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ä¹‹ç±»çš„è¿ç®—ï¼Œç„¶åæ¨å¹¿åˆ°å­ç©ºé—´ã€åŸºå’Œåæ ‡ç³»ã€‚æœ€ç»ˆï¼Œè¿™ä¸ªæ¡†æ¶å°†å‘å±•æˆä¸ºå®Œæ•´çš„çº¿æ€§ä»£æ•°ç†è®ºï¼Œå¹¶åœ¨å‡ ä½•ã€è®¡ç®—å’Œæ•°æ®é¢†åŸŸæ‹¥æœ‰å¼ºå¤§çš„åº”ç”¨ã€‚</p>
<h3 id="exercises-1.1">Exercises 1.1</h3>
<p>ç»ƒä¹  1.1</p>
<ol type="1">
<li>Write three different vectors in <span
class="math inline">\(\mathbb{R}^2\)</span> and sketch them as arrows
from the origin. Identify their coordinates explicitly. åœ¨ <span
class="math inline">\(\mathbb{R}^2\)</span>
ä¸­å†™å‡ºä¸‰ä¸ªä¸åŒçš„å‘é‡ï¼Œå¹¶å°†å®ƒä»¬ç”»æˆä»åŸç‚¹å‡ºå‘çš„ç®­å¤´ã€‚æ˜ç¡®æŒ‡å‡ºå®ƒä»¬çš„åæ ‡ã€‚</li>
<li>Give an example of a vector in <span
class="math inline">\(\mathbb{R}^4\)</span>. Can you visualize it
directly? Explain why high-dimensional visualization is challenging.
ç»™å‡º <span class="math inline">\(\mathbb{R}^4\)</span>
ä¸­ä¸€ä¸ªå‘é‡çš„ä¾‹å­ã€‚ä½ èƒ½ç›´æ¥å°†å…¶å¯è§†åŒ–å—ï¼Ÿè§£é‡Šä¸ºä»€ä¹ˆé«˜ç»´å¯è§†åŒ–å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>Let <span class="math inline">\(\mathbf{v} = (4, -3, 2)\)</span>.
Write <span class="math inline">\(\mathbf{v}\)</span> in column form and
state <span class="math inline">\(v_1, v_2, v_3\)</span>. ä»¤ <span
class="math inline">\(\mathbf{v} = (4, -3, 2)\)</span> ã€‚å°† <span
class="math inline">\(\mathbf{v}\)</span> å†™æˆåˆ—å½¢å¼ï¼Œå¹¶è¯´æ˜ ğ‘£ 1 , ğ‘£ 2 ,
ğ‘£ 3 v 1 â€‹ ï¼Œv 2 â€‹ ï¼Œv 3 â€‹ .</li>
<li>In what sense is the set <span
class="math inline">\(\mathbb{R}^1\)</span> both a line and a vector
space? Illustrate with examples. åœ¨ä»€ä¹ˆæ„ä¹‰ä¸Šé›†åˆ <span
class="math inline">\(\mathbb{R}^1\)</span>
æ—¢æ˜¯çº¿ç©ºé—´åˆæ˜¯å‘é‡ç©ºé—´ï¼Ÿè¯·ä¸¾ä¾‹è¯´æ˜ã€‚</li>
<li>Consider the vector <span class="math inline">\(\mathbf{u} =
(1,1,\dots,1) \in \mathbb{R}^n\)</span>. What is special about this
vector when <span class="math inline">\(n\)</span> is large? What might
it represent in applications? è€ƒè™‘å‘é‡ <span
class="math inline">\(\mathbf{u} = (1,1,\dots,1) \in
\mathbb{R}^n\)</span> ã€‚å½“ <span class="math inline">\(n\)</span>
å¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªå‘é‡æœ‰ä»€ä¹ˆç‰¹æ®Šä¹‹å¤„ï¼Ÿå®ƒåœ¨åº”ç”¨ä¸­å¯èƒ½ä»£è¡¨ä»€ä¹ˆï¼Ÿ</li>
</ol>
<h2 id="vector-addition-and-scalar-multiplication">1.2 Vector Addition
and Scalar Multiplication</h2>
<p>1.2 å‘é‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•</p>
<p>Vectors in linear algebra are not static objects; their power comes
from the operations we can perform on them. Two fundamental operations
define the structure of vector spaces: addition and scalar
multiplication. These operations satisfy simple but far-reaching rules
that underpin the entire subject.
çº¿æ€§ä»£æ•°ä¸­çš„å‘é‡å¹¶éé™æ€å¯¹è±¡ï¼›å®ƒä»¬çš„åŠ›é‡æºäºæˆ‘ä»¬å¯ä»¥å¯¹å®ƒä»¬æ‰§è¡Œçš„è¿ç®—ã€‚ä¸¤ä¸ªåŸºæœ¬è¿ç®—å®šä¹‰äº†å‘é‡ç©ºé—´çš„ç»“æ„ï¼šåŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ã€‚è¿™ä¸¤ä¸ªè¿ç®—æ»¡è¶³ä¸€äº›ç®€å•å´å½±å“æ·±è¿œçš„è§„åˆ™ï¼Œè¿™äº›è§„åˆ™æ„æˆäº†æ•´ä¸ªçº¿æ€§ä»£æ•°å­¦ç§‘çš„åŸºç¡€ã€‚</p>
<h3 id="vector-addition">Vector Addition</h3>
<p>å‘é‡åŠ æ³•</p>
<p>Given two vectors of the same dimension, their sum is obtained by
adding corresponding entries. Formally, if
ç»™å®šä¸¤ä¸ªç›¸åŒç»´åº¦çš„å‘é‡ï¼Œå®ƒä»¬çš„å’Œå¯ä»¥é€šè¿‡æ·»åŠ ç›¸åº”çš„å…ƒç´ æ¥è·å¾—ã€‚å½¢å¼ä¸Šï¼Œå¦‚æœ</p>
<p><span class="math display">\[
\mathbf{u} = (u_1, u_2, \dots, u_n), \quad\mathbf{v} = (v_1, v_2, \dots,
v_n),
\]</span></p>
<p>then their sum is é‚£ä¹ˆå®ƒä»¬çš„æ€»å’Œæ˜¯</p>
<p><span class="math display">\[
\mathbf{u} + \mathbf{v} = (u_1+v_1, u_2+v_2, \dots, u_n+v_n).
\]</span></p>
<p>Example 1.2.1. Let <span class="math inline">\(\mathbf{u} = (2, -1,
3)\)</span> and <span class="math inline">\(\mathbf{v} = (4, 0,
-5)\)</span>. Then ä¾‹ 1.2.1ã€‚ è®¾ <span class="math inline">\(\mathbf{u}
= (2, -1, 3)\)</span> å’Œ <span class="math inline">\(\mathbf{v} = (4, 0,
-5)\)</span> ã€‚åˆ™</p>
<p><span class="math display">\[
\mathbf{u} + \mathbf{v} = (2+4, -1+0, 3+(-5)) = (6, -1, -2).
\]</span></p>
<p>Geometrically, vector addition corresponds to the <em>parallelogram
rule</em>. If we draw both vectors as arrows from the origin, then
placing the tail of one vector at the head of the other produces the
sum. The diagonal of the parallelogram they form represents the
resulting vector. ä»å‡ ä½•å­¦ä¸Šè®²ï¼Œå‘é‡åŠ æ³•å¯¹åº”äº<em>å¹³è¡Œå››è¾¹å½¢æ³•åˆ™</em>
ã€‚å¦‚æœæˆ‘ä»¬å°†ä¸¤ä¸ªå‘é‡éƒ½ç”»æˆä»åŸç‚¹å‡ºå‘çš„ç®­å¤´ï¼Œé‚£ä¹ˆå°†ä¸€ä¸ªå‘é‡çš„å°¾éƒ¨æ”¾åœ¨å¦ä¸€ä¸ªå‘é‡çš„å¤´éƒ¨ï¼Œå°±èƒ½å¾—åˆ°å‘é‡çš„å’Œã€‚å®ƒä»¬æ„æˆçš„å¹³è¡Œå››è¾¹å½¢çš„å¯¹è§’çº¿ä»£è¡¨æœ€ç»ˆçš„å‘é‡ã€‚</p>
<h3 id="scalar-multiplication">Scalar Multiplication</h3>
<p>æ ‡é‡ä¹˜æ³•</p>
<p>Multiplying a vector by a scalar stretches or shrinks the vector
while preserving its direction, unless the scalar is negative, in which
case the vector is also reversed. If <span class="math inline">\(c \in
\mathbb{R}\)</span> and
å°†çŸ¢é‡ä¹˜ä»¥æ ‡é‡ä¼šæ‹‰ä¼¸æˆ–æ”¶ç¼©çŸ¢é‡ï¼ŒåŒæ—¶ä¿æŒå…¶æ–¹å‘ï¼Œé™¤éæ ‡é‡
è´Ÿæ•°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹å‘é‡ä¹Ÿä¼šåè½¬ã€‚å¦‚æœ <span class="math inline">\(c \in
\mathbb{R}\)</span> å’Œ</p>
<p><span class="math display">\[
\mathbf{v} = (v_1, v_2, \dots, v_n),
\]</span></p>
<p>then ç„¶å</p>
<p><span class="math display">\[
c \mathbf{v} = (c v_1, c v_2, \dots, c v_n).
\]</span></p>
<p>Example 1.2.2. Let <span class="math inline">\(\mathbf{v} = (3,
-2)\)</span> and <span class="math inline">\(c = -2\)</span>. Then ä¾‹
1.2.2ã€‚ è®¾ <span class="math inline">\(\mathbf{v} = (3, -2)\)</span> å’Œ
<span class="math inline">\(c = -2\)</span> ã€‚åˆ™</p>
<p><span class="math display">\[
c\mathbf{v} = -2(3, -2) = (-6, 4).
\]</span></p>
<p>This corresponds to flipping the vector through the origin and
doubling its length. è¿™ç›¸å½“äºé€šè¿‡åŸç‚¹ç¿»è½¬å‘é‡å¹¶ä½¿å…¶é•¿åº¦åŠ å€ã€‚</p>
<h3 id="linear-combinations">Linear Combinations</h3>
<p>çº¿æ€§ç»„åˆ</p>
<p>The interaction of addition and scalar multiplication allows us to
form <em>linear combinations</em>. A linear combination of vectors <span
class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, \dots,
\mathbf{v}_k\)</span> is any vector of the form
åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•çš„ç›¸äº’ä½œç”¨ä½¿æˆ‘ä»¬èƒ½å¤Ÿå½¢æˆ<em>çº¿æ€§ç»„åˆ</em>
ã€‚å‘é‡ğ‘£çš„çº¿æ€§ç»„åˆ 1 , ğ‘£ 2 , â€¦ , ğ‘£ ğ‘˜ v 1 â€‹ ï¼Œv 2 â€‹ ï¼Œâ€¦ï¼Œv k â€‹
æ˜¯ä»»æ„å½¢å¼çš„å‘é‡</p>
<p><span class="math display">\[
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k, \quad
c_i \in \mathbb{R}.
\]</span></p>
<p>Linear combinations are the mechanism by which we generate new
vectors from existing ones. The span of a set of vectors-the collection
of all their linear combinations-will later lead us to the idea of a
subspace.
çº¿æ€§ç»„åˆæ˜¯ä¸€ç§ä»ç°æœ‰å‘é‡ç”Ÿæˆæ–°å‘é‡çš„æœºåˆ¶ã€‚ä¸€ç»„å‘é‡çš„è·¨åº¦â€”â€”å®ƒä»¬æ‰€æœ‰çº¿æ€§ç»„åˆçš„é›†åˆâ€”â€”ç¨åä¼šå¼•å‡ºå­ç©ºé—´çš„æ¦‚å¿µã€‚</p>
<p>Example 1.2.3. Let <span class="math inline">\(\mathbf{v}_1 =
(1,0)\)</span> and <span class="math inline">\(\mathbf{v}_2 =
(0,1)\)</span>. Then any vector <span
class="math inline">\((a,b)\in\mathbb{R}^2\)</span> can be expressed as
ä¾‹ 1.2.3ã€‚ è®¾ <span class="math inline">\(\mathbf{v}_1 = (1,0)\)</span>
å’Œ <span class="math inline">\(\mathbf{v}_2 = (0,1)\)</span>
ã€‚åˆ™ä»»æ„å‘é‡ <span class="math inline">\((a,b)\in\mathbb{R}^2\)</span>
å¯ä»¥è¡¨ç¤ºä¸º</p>
<p><span class="math display">\[
a\mathbf{v}_1 + b\mathbf{v}_2.
\]</span></p>
<p>Thus <span class="math inline">\((1,0)\)</span> and <span
class="math inline">\((0,1)\)</span> form the basic building blocks of
the plane. å› æ­¤ <span class="math inline">\((1,0)\)</span> å’Œ <span
class="math inline">\((0,1)\)</span> æ„æˆäº†å¹³é¢çš„åŸºæœ¬æ„é€ å—ã€‚</p>
<h3 id="notation-1">Notation</h3>
<p>ç¬¦å·</p>
<ul>
<li>Addition: <span class="math inline">\(\mathbf{u} +
\mathbf{v}\)</span> means component-wise addition. åŠ æ³•ï¼š <span
class="math inline">\(\mathbf{u} + \mathbf{v}\)</span>
è¡¨ç¤ºé€ä¸ªç»„ä»¶çš„åŠ æ³•ã€‚</li>
<li>Scalar multiplication: <span
class="math inline">\(c\mathbf{v}\)</span> scales each entry of <span
class="math inline">\(\mathbf{v}\)</span> by <span
class="math inline">\(c\)</span>. æ ‡é‡ä¹˜æ³•ï¼š <span
class="math inline">\(c\mathbf{v}\)</span> å°† <span
class="math inline">\(\mathbf{v}\)</span> çš„æ¯ä¸ªæ¡ç›®ä¹˜ä»¥ <span
class="math inline">\(c\)</span> ã€‚</li>
<li>Linear combination: a sum of the form <span
class="math inline">\(c_1 \mathbf{v}_1 + \cdots + c_k
\mathbf{v}_k\)</span>. çº¿æ€§ç»„åˆï¼šğ‘ å½¢å¼çš„å’Œ 1 ğ‘£ 1 + â‹¯ + ğ‘ ğ‘˜ ğ‘£ ğ‘˜ c 1 â€‹ v 1
â€‹+â‹¯+c k â€‹ v k â€‹ .</li>
</ul>
<h3 id="why-this-matters">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Vector addition and scalar multiplication are the defining operations
of linear algebra. They give structure to vector spaces, allow us to
describe geometric phenomena like translation and scaling, and provide
the foundation for solving systems of equations. Everything that
follows-basis, dimension, transformations-builds on these simple but
profound rules.
å‘é‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•æ˜¯çº¿æ€§ä»£æ•°çš„å®šä¹‰è¿ç®—ã€‚å®ƒä»¬èµ‹äºˆå‘é‡ç©ºé—´ç»“æ„ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæè¿°å¹³ç§»å’Œç¼©æ”¾ç­‰å‡ ä½•ç°è±¡ï¼Œå¹¶ä¸ºæ–¹ç¨‹ç»„çš„æ±‚è§£å¥ å®šåŸºç¡€ã€‚ä¹‹åçš„ä¸€åˆ‡â€”â€”åŸºã€ç»´åº¦ã€å˜æ¢â€”â€”éƒ½å»ºç«‹åœ¨è¿™äº›ç®€å•è€Œæ·±åˆ»çš„è§„åˆ™ä¹‹ä¸Šã€‚</p>
<h3 id="exercises-1.2">Exercises 1.2</h3>
<p>ç»ƒä¹  1.2</p>
<ol type="1">
<li>Compute <span class="math inline">\(\mathbf{u} + \mathbf{v}\)</span>
where <span class="math inline">\(\mathbf{u} = (1,2,3)\)</span> and
<span class="math inline">\(\mathbf{v} = (4, -1, 0)\)</span>. è®¡ç®— <span
class="math inline">\(\mathbf{u} + \mathbf{v}\)</span> ï¼Œå…¶ä¸­ <span
class="math inline">\(\mathbf{u} = (1,2,3)\)</span> å’Œ <span
class="math inline">\(\mathbf{v} = (4, -1, 0)\)</span> ã€‚</li>
<li>Find <span class="math inline">\(3\\mathbf{v}\)</span>where<span
class="math inline">\(\\mathbf{v} = (-2,5)\)</span>. Sketch both vectors
to illustrate the scaling. æ±‚ $3\mathbf{v} <span
class="math inline">\(where\)</span> \mathbf{v} =
(-2,5)$ã€‚ç”»å‡ºä¸¤ä¸ªå‘é‡çš„ç¤ºæ„å›¾ï¼Œä»¥è¯´æ˜ç¼©æ”¾å…³ç³»ã€‚</li>
<li>Show that <span class="math inline">\((5,7)\)</span> can be written
as a linear combination of <span class="math inline">\((1,0)\)</span>
and <span class="math inline">\((0,1)\)</span>. è¯æ˜ <span
class="math inline">\((5,7)\)</span> å¯ä»¥å†™æˆ <span
class="math inline">\((1,0)\)</span> å’Œ <span
class="math inline">\((0,1)\)</span> çš„çº¿æ€§ç»„åˆã€‚</li>
<li>Write <span class="math inline">\((4,4)\)</span> as a linear
combination of <span class="math inline">\((1,1)\)</span> and <span
class="math inline">\((1,-1)\)</span>. å°† <span
class="math inline">\((4,4)\)</span> å†™ä¸º <span
class="math inline">\((1,1)\)</span> å’Œ <span
class="math inline">\((1,-1)\)</span> çš„çº¿æ€§ç»„åˆã€‚</li>
<li>Prove that if <span class="math inline">\(\mathbf{u}, \mathbf{v} \in
\mathbb{R}^n\)</span>, then <span
class="math inline">\((c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} +
c\mathbf{v} + d\mathbf{u} + d\mathbf{v}\)</span> for scalars <span
class="math inline">\(c,d \in \mathbb{R}\)</span>. è¯æ˜å¦‚æœ <span
class="math inline">\(\mathbf{u}, \mathbf{v} \in \mathbb{R}^n\)</span>
ï¼Œåˆ™å¯¹äºæ ‡é‡ <span class="math inline">\(c,d \in \mathbb{R}\)</span> æœ‰
<span class="math inline">\((c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} +
c\mathbf{v} + d\mathbf{u} + d\mathbf{v}\)</span> ã€‚</li>
</ol>
<h2 id="dot-product-norms-and-angles">1.3 Dot Product, Norms, and
Angles</h2>
<p>1.3 ç‚¹ç§¯ã€èŒƒæ•°å’Œè§’</p>
<p>The dot product is the fundamental operation that links algebra and
geometry in vector spaces. It allows us to measure lengths, compute
angles, and determine orthogonality. From this single definition flow
the notions of <em>norm</em> and <em>angle</em>, which give geometry to
abstract vector spaces.
ç‚¹ç§¯æ˜¯å‘é‡ç©ºé—´ä¸­è¿æ¥ä»£æ•°å’Œå‡ ä½•çš„åŸºæœ¬è¿ç®—ã€‚å®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿæµ‹é‡é•¿åº¦ã€è®¡ç®—è§’åº¦å¹¶ç¡®å®šæ­£äº¤æ€§ã€‚ä»è¿™ä¸ªå•ä¸€å®šä¹‰ä¸­è¡ç”Ÿå‡º<em>èŒƒ</em>æ•°å’Œ
<em>è§’åº¦</em> ï¼Œå®ƒä¸ºæŠ½è±¡å‘é‡ç©ºé—´æä¾›å‡ ä½•å½¢çŠ¶ã€‚</p>
<h3 id="the-dot-product">The Dot Product</h3>
<p>ç‚¹ç§¯</p>
<p>For two vectors in <span class="math inline">\(\mathbb{R}^n\)</span>,
the dot product (also called the inner product) is defined by å¯¹äº <span
class="math inline">\(\mathbb{R}^n\)</span>
ä¸­çš„ä¸¤ä¸ªå‘é‡ï¼Œç‚¹ç§¯ï¼ˆä¹Ÿç§°ä¸ºå†…ç§¯ï¼‰å®šä¹‰ä¸º</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
\]</span></p>
<p>Equivalently, in matrix notation: ç­‰æ•ˆåœ°ï¼Œç”¨çŸ©é˜µè¡¨ç¤ºæ³•è¡¨ç¤ºï¼š</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T \mathbf{v}.
\]</span></p>
<p>Example 1.3.1. Let <span class="math inline">\(\mathbf{u} = (2, -1,
3)\)</span> and <span class="math inline">\(\mathbf{v} = (4, 0,
-2)\)</span>. Then ä¾‹ 1.3.1ã€‚ è®¾ <span class="math inline">\(\mathbf{u}
= (2, -1, 3)\)</span> å’Œ <span class="math inline">\(\mathbf{v} = (4, 0,
-2)\)</span> ã€‚åˆ™</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = 2\cdot 4 + (-1)\cdot 0 + 3\cdot (-2) = 8 -
6 = 2.
\]</span></p>
<p>The dot product outputs a single scalar, not another vector.
ç‚¹ç§¯è¾“å‡ºå•ä¸ªæ ‡é‡ï¼Œè€Œä¸æ˜¯å¦ä¸€ä¸ªå‘é‡ã€‚</p>
<h3 id="norms-length-of-a-vector">Norms (Length of a Vector)</h3>
<p>èŒƒæ•°ï¼ˆå‘é‡çš„é•¿åº¦ï¼‰</p>
<p>The <em>Euclidean norm</em> of a vector is the square root of its dot
product with itself:
å‘é‡çš„<em>æ¬§å‡ é‡Œå¾—èŒƒæ•°</em>æ˜¯å…¶ä¸è‡ªèº«çš„ç‚¹ç§¯çš„å¹³æ–¹æ ¹ï¼š</p>
<p><span class="math display">\[
\|\mathbf{v}\| = \sqrt{\mathbf{v} \cdot \mathbf{v}} = \sqrt{v_1^2 +
v_2^2 + \cdots + v_n^2}.
\]</span></p>
<p>This generalizes the Pythagorean theorem to arbitrary dimensions.
è¿™å°†å‹¾è‚¡å®šç†æ¨å¹¿åˆ°ä»»æ„ç»´åº¦ã€‚</p>
<p>Example 1.3.2. For <span class="math inline">\(\mathbf{v} = (3,
4)\)</span>, ä¾‹ 1.3.2ã€‚ å¯¹äº <span class="math inline">\(\mathbf{v} =
(3, 4)\)</span> ï¼Œ</p>
<p><span class="math display">\[
\|\mathbf{v}\| = \sqrt{3^2 + 4^2} = \sqrt{25} = 5.
\]</span></p>
<p>This is exactly the length of the vector as an arrow in the plane.
è¿™æ­£æ˜¯å¹³é¢ä¸­ç®­å¤´æ‰€æŒ‡çš„çŸ¢é‡çš„é•¿åº¦ã€‚</p>
<h3 id="angles-between-vectors">Angles Between Vectors</h3>
<p>å‘é‡ä¹‹é—´çš„è§’åº¦</p>
<p>The dot product also encodes the angle between two vectors. For
nonzero vectors <span class="math inline">\(\mathbf{u},
\mathbf{v}\)</span>, ç‚¹ç§¯ä¹Ÿç¼–ç äº†ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è§’åº¦ã€‚å¯¹äºéé›¶å‘é‡ <span
class="math inline">\(\mathbf{u}, \mathbf{v}\)</span> ï¼Œ</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \, \|\mathbf{v}\| \cos
\theta,
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the angle
between them. Thus, å…¶ä¸­ <span class="math inline">\(\theta\)</span>
æ˜¯å®ƒä»¬ä¹‹é—´çš„è§’åº¦ã€‚å› æ­¤ï¼Œ</p>
<p><span class="math display">\[
\cos \theta = \frac{\mathbf{u} \cdot
\mathbf{v}}{\|\mathbf{u}\|\|\mathbf{v}\|}.
\]</span></p>
<p>Example 1.3.3. Let <span class="math inline">\(\mathbf{u} =
(1,0)\)</span> and <span class="math inline">\(\mathbf{v} =
(0,1)\)</span>. Then ä¾‹ 1.3.3ã€‚ è®¾ <span
class="math inline">\(\mathbf{u} = (1,0)\)</span> å’Œ <span
class="math inline">\(\mathbf{v} = (0,1)\)</span> ã€‚åˆ™</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = 0, \quad \|\mathbf{u}\| = 1, \quad
\|\mathbf{v}\| = 1.
\]</span></p>
<p>Hence å› æ­¤</p>
<p><span class="math display">\[
\cos \theta = \frac{0}{1\cdot 1} = 0 \quad \Rightarrow \quad \theta =
\frac{\pi}{2}.
\]</span></p>
<p>The vectors are perpendicular. è¿™äº›å‘é‡æ˜¯å‚ç›´çš„ã€‚</p>
<h3 id="orthogonality">Orthogonality</h3>
<p>æ­£äº¤æ€§</p>
<p>Two vectors are said to be orthogonal if their dot product is zero:
å¦‚æœä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ä¸ºé›¶ï¼Œåˆ™ç§°å®ƒä»¬æ­£äº¤ï¼š</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = 0.
\]</span></p>
<p>Orthogonality generalizes the idea of perpendicularity from geometry
to higher dimensions. æ­£äº¤æ€§å°†å‚ç›´æ€§çš„æ¦‚å¿µä»å‡ ä½•å­¦æ¨å¹¿åˆ°æ›´é«˜ç»´åº¦ã€‚</p>
<h3 id="notation-2">Notation</h3>
<p>ç¬¦å·</p>
<ul>
<li>Dot product: <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v}\)</span>. ç‚¹ç§¯ï¼š <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v}\)</span> ã€‚</li>
<li>Norm (length): <span class="math inline">\(|\mathbf{v}|\)</span>.
è§„èŒƒï¼ˆé•¿åº¦ï¼‰ï¼š <span class="math inline">\(|\mathbf{v}|\)</span> ã€‚</li>
<li>Orthogonality: <span class="math inline">\(\mathbf{u} \perp
\mathbf{v}\)</span> if <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v} = 0\)</span>. æ­£äº¤æ€§ï¼šå¦‚æœä¸º <span
class="math inline">\(\mathbf{u} \cdot \mathbf{v} = 0\)</span> ï¼Œåˆ™ä¸º
<span class="math inline">\(\mathbf{u} \perp \mathbf{v}\)</span> ã€‚</li>
</ul>
<h3 id="why-this-matters-1">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>The dot product turns vector spaces into geometric objects: vectors
gain lengths, angles, and notions of perpendicularity. This foundation
will later support the study of orthogonal projections, Gramâ€“Schmidt
orthogonalization, eigenvectors, and least squares problems.
ç‚¹ç§¯å°†å‘é‡ç©ºé—´è½¬åŒ–ä¸ºå‡ ä½•å¯¹è±¡ï¼šå‘é‡è·å¾—é•¿åº¦ã€è§’åº¦å’Œå‚ç›´åº¦çš„æ¦‚å¿µã€‚è¿™ä¸€åŸºç¡€å°†ä¸ºåç»­çš„æ­£äº¤æŠ•å½±ã€æ ¼æ‹‰å§†-æ–½å¯†ç‰¹æ­£äº¤åŒ–ã€ç‰¹å¾å‘é‡å’Œæœ€å°äºŒä¹˜é—®é¢˜çš„ç ”ç©¶å¥ å®šåŸºç¡€ã€‚</p>
<h3 id="exercises-1.3">Exercises 1.3</h3>
<p>ç»ƒä¹  1.3</p>
<ol type="1">
<li>Compute <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v}\)</span> for <span class="math inline">\(\mathbf{u} =
(1,2,3)\)</span>, <span class="math inline">\(\mathbf{v} =
(4,5,6)\)</span>. è®¡ç®— <span class="math inline">\(\mathbf{u} =
(1,2,3)\)</span> ã€ <span class="math inline">\(\mathbf{v} =
(4,5,6)\)</span> çš„ <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v}\)</span> ã€‚</li>
<li>Find the norm of <span class="math inline">\(\mathbf{v} = (2, -2,
1)\)</span>. æ±‚å‡º <span class="math inline">\(\mathbf{v} = (2, -2,
1)\)</span> çš„èŒƒæ•°ã€‚</li>
<li>Determine whether <span class="math inline">\(\mathbf{u} =
(1,1,0)\)</span> and <span class="math inline">\(\mathbf{v} =
(1,-1,2)\)</span> are orthogonal. ç¡®å®š <span
class="math inline">\(\mathbf{u} = (1,1,0)\)</span> å’Œ <span
class="math inline">\(\mathbf{v} = (1,-1,2)\)</span> æ˜¯å¦æ­£äº¤ã€‚</li>
<li>Let <span class="math inline">\(\mathbf{u} = (3,4)\)</span>, <span
class="math inline">\(\mathbf{v} = (4,3)\)</span>. Compute the angle
between them. ä»¤ <span class="math inline">\(\mathbf{u} = (3,4)\)</span>
, <span class="math inline">\(\mathbf{v} = (4,3)\)</span>
ã€‚è®¡ç®—å®ƒä»¬ä¹‹é—´çš„è§’åº¦ã€‚</li>
<li>Prove that <span class="math inline">\(|\mathbf{u} + \mathbf{v}|^2 =
|\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}\)</span>.
This identity is the algebraic version of the Law of Cosines. è¯æ˜ <span
class="math inline">\(|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 +
|\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}\)</span>
ã€‚è¿™ä¸ªæ’ç­‰å¼æ˜¯ä½™å¼¦å®šç†çš„ä»£æ•°å½¢å¼ã€‚</li>
</ol>
<h2 id="orthogonality-1">1.4 Orthogonality</h2>
<p>1.4 æ­£äº¤æ€§</p>
<p>Orthogonality captures the notion of perpendicularity in vector
spaces. It is one of the most important geometric ideas in linear
algebra, allowing us to decompose vectors, define projections, and
construct special bases with elegant properties.
æ­£äº¤æ€§æ•æ‰äº†å‘é‡ç©ºé—´ä¸­å‚ç›´æ€§çš„æ¦‚å¿µã€‚å®ƒæ˜¯çº¿æ€§ä»£æ•°ä¸­æœ€é‡è¦çš„å‡ ä½•æ¦‚å¿µä¹‹ä¸€ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ†è§£å‘é‡ã€å®šä¹‰æŠ•å½±ï¼Œå¹¶æ„é€ å…·æœ‰ä¼˜é›…æ€§è´¨çš„ç‰¹æ®ŠåŸºã€‚</p>
<h3 id="definition">Definition</h3>
<p>å®šä¹‰</p>
<p>Two vectors <span class="math inline">\(\mathbf{u}, \mathbf{v} \in
\mathbb{R}^n\)</span> are said to be orthogonal if their dot product is
zero: å¦‚æœä¸¤ä¸ªå‘é‡ <span class="math inline">\(\mathbf{u}, \mathbf{v}
\in \mathbb{R}^n\)</span> çš„ç‚¹ç§¯ä¸ºé›¶ï¼Œåˆ™ç§°å®ƒä»¬æ­£äº¤ï¼š</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = 0.
\]</span></p>
<p>This condition ensures that the angle between them is <span
class="math inline">\(\pi/2\)</span> radians (90 degrees).
æ­¤æ¡ä»¶ç¡®ä¿å®ƒä»¬ä¹‹é—´çš„è§’åº¦ä¸º <span class="math inline">\(\pi/2\)</span>
å¼§åº¦ï¼ˆ90 åº¦ï¼‰ã€‚</p>
<p>Example 1.4.1. In <span class="math inline">\(\mathbb{R}^2\)</span>,
the vectors <span class="math inline">\((1,2)\)</span> and <span
class="math inline">\((2,-1)\)</span> are orthogonal since ä¾‹ 1.4.1ã€‚ åœ¨
<span class="math inline">\(\mathbb{R}^2\)</span> ä¸­ï¼Œå‘é‡ <span
class="math inline">\((1,2)\)</span> å’Œ <span
class="math inline">\((2,-1)\)</span> æ˜¯æ­£äº¤çš„ï¼Œå› ä¸º</p>
<p><span class="math display">\[
(1,2) \cdot (2,-1) = 1\cdot 2 + 2\cdot (-1) = 0.
\]</span></p>
<h3 id="orthogonal-sets">Orthogonal Sets</h3>
<p>æ­£äº¤é›†</p>
<p>A collection of vectors is called orthogonal if every distinct pair
of vectors in the set is orthogonal. If, in addition, each vector has
norm 1, the set is called orthonormal.
å¦‚æœä¸€ç»„å‘é‡ä¸­æ¯å¯¹ä¸åŒçš„å‘é‡éƒ½æ˜¯æ­£äº¤çš„ï¼Œåˆ™ç§°è¯¥é›†åˆä¸ºæ­£äº¤å‘é‡ã€‚æ­¤å¤–ï¼Œå¦‚æœæ¯ä¸ªå‘é‡çš„èŒƒæ•°å‡ä¸º
1ï¼Œåˆ™è¯¥é›†åˆç§°ä¸ºæ ‡å‡†æ­£äº¤å‘é‡é›†ã€‚</p>
<p>Example 1.4.2. In <span class="math inline">\(\mathbb{R}^3\)</span>,
the standard basis vectors ä¾‹ 1.4.2ã€‚ åœ¨ <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­ï¼Œæ ‡å‡†åŸºå‘é‡</p>
<p><span class="math display">\[
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3
= (0,0,1)
\]</span></p>
<p>form an orthonormal set: each has length 1, and their dot products
vanish when the indices differ. å½¢æˆä¸€ä¸ªæ­£äº¤é›†ï¼šæ¯ä¸ªé›†çš„é•¿åº¦ä¸º
1ï¼Œå¹¶ä¸”å½“ç´¢å¼•ä¸åŒæ—¶ï¼Œå®ƒä»¬çš„ç‚¹ç§¯æ¶ˆå¤±ã€‚</p>
<h3 id="projections">Projections</h3>
<p>é¢„æµ‹</p>
<p>Orthogonality makes possible the decomposition of a vector into two
components: one parallel to another vector, and one orthogonal to it.
Given a nonzero vector <span class="math inline">\(\mathbf{u}\)</span>
and any vector <span class="math inline">\(\mathbf{v}\)</span>, the
projection of <span class="math inline">\(\mathbf{v}\)</span> onto <span
class="math inline">\(\mathbf{u}\)</span> is
æ­£äº¤æ€§ä½¿å¾—å°†ä¸€ä¸ªå‘é‡åˆ†è§£ä¸ºä¸¤ä¸ªåˆ†é‡æˆä¸ºå¯èƒ½ï¼šä¸€ä¸ªä¸å¦ä¸€ä¸ªå‘é‡å¹³è¡Œï¼Œå¦ä¸€ä¸ª
ä¸å…¶æ­£äº¤ã€‚ç»™å®šä¸€ä¸ªéé›¶å‘é‡ <span
class="math inline">\(\mathbf{u}\)</span> å’Œä»»æ„å‘é‡ <span
class="math inline">\(\mathbf{v}\)</span> ï¼Œåˆ™ <span
class="math inline">\(\mathbf{v}\)</span> çš„æŠ•å½± åˆ° <span
class="math inline">\(\mathbf{u}\)</span> æ˜¯</p>
<p><span class="math display">\[
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\mathbf{u} \cdot
\mathbf{v}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}.
\]</span></p>
<p>The difference åŒºåˆ«</p>
<p><span class="math display">\[
\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})
\]</span></p>
<p>is orthogonal to <span class="math inline">\(\mathbf{u}\)</span>.
Thus every vector can be decomposed uniquely into a parallel and
perpendicular part with respect to another vector. ä¸ <span
class="math inline">\(\mathbf{u}\)</span>
æ­£äº¤ã€‚å› æ­¤ï¼Œæ¯ä¸ªå‘é‡éƒ½å¯ä»¥å”¯ä¸€åœ°åˆ†è§£ä¸ºç›¸å¯¹äºå¦ä¸€ä¸ªå‘é‡å¹³è¡Œå’Œå‚ç›´çš„éƒ¨åˆ†ã€‚</p>
<p>Example 1.4.3. Let <span class="math inline">\(\mathbf{u} =
(1,0)\)</span>, <span class="math inline">\(\mathbf{v} = (2,3)\)</span>.
Then ä¾‹ 1.4.3ã€‚ ä»¤ <span class="math inline">\(\mathbf{u} =
(1,0)\)</span> ï¼Œ <span class="math inline">\(\mathbf{v} =
(2,3)\)</span> ã€‚ç„¶å</p>
<p><span class="math display">\[
\text{proj}_{\mathbf{u}}(\mathbf{v}) =
\frac{(1,0)\cdot(2,3)}{(1,0)\cdot(1,0)} (1,0)= \frac{2}{1}(1,0) = (2,0).
\]</span></p>
<p>Thus å› æ­¤</p>
<p><span class="math display">\[
\mathbf{v} = (2,3) = (2,0) + (0,3),
\]</span></p>
<p>where <span class="math inline">\((2,0)\)</span> is parallel to <span
class="math inline">\((1,0)\)</span> and <span
class="math inline">\((0,3)\)</span> is orthogonal to it. å…¶ä¸­ <span
class="math inline">\((2,0)\)</span> ä¸ <span
class="math inline">\((1,0)\)</span> å¹³è¡Œï¼Œ <span
class="math inline">\((0,3)\)</span> ä¸ <span
class="math inline">\((1,0)\)</span> æ­£äº¤ã€‚</p>
<h3 id="orthogonal-decomposition">Orthogonal Decomposition</h3>
<p>æ­£äº¤åˆ†è§£</p>
<p>In general, if <span class="math inline">\(\mathbf{u} \neq
\mathbf{0}\)</span> and <span class="math inline">\(\mathbf{v} \in
\mathbb{R}^n\)</span>, then ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœ <span
class="math inline">\(\mathbf{u} \neq \mathbf{0}\)</span> å’Œ <span
class="math inline">\(\mathbf{v} \in \mathbb{R}^n\)</span> ï¼Œé‚£ä¹ˆ</p>
<p><span class="math display">\[
\mathbf{v} = \text{proj}\_{\mathbf{u}}(\mathbf{v}) + \big(\mathbf{v} -
\text{proj}\_{\mathbf{u}}(\mathbf{v})\big),
\]</span></p>
<p>where the first term is parallel to <span
class="math inline">\(\mathbf{u}\)</span> and the second term is
orthogonal. This decomposition underlies methods such as least squares
approximation and the Gramâ€“Schmidt process. å…¶ä¸­ç¬¬ä¸€é¡¹å¹³è¡Œäº <span
class="math inline">\(\mathbf{u}\)</span>
ï¼Œç¬¬äºŒé¡¹æ­£äº¤ã€‚è¿™ç§åˆ†è§£æ˜¯æœ€å°äºŒä¹˜è¿‘ä¼¼å’Œæ ¼æ‹‰å§†-æ–½å¯†ç‰¹è¿‡ç¨‹ç­‰æ–¹æ³•çš„åŸºç¡€ã€‚</p>
<h3 id="notation-3">Notation</h3>
<p>ç¬¦å·</p>
<ul>
<li><span class="math inline">\(\mathbf{u} \perp \mathbf{v}\)</span>:
vectors <span class="math inline">\(\mathbf{u}\)</span> and <span
class="math inline">\(\mathbf{v}\)</span> are orthogonal. <span
class="math inline">\(\mathbf{u} \perp \mathbf{v}\)</span> ï¼šå‘é‡ <span
class="math inline">\(\mathbf{u}\)</span> å’Œ <span
class="math inline">\(\mathbf{v}\)</span> æ­£äº¤ã€‚</li>
<li>An orthogonal set: vectors pairwise orthogonal.
æ­£äº¤é›†ï¼šå‘é‡ä¸¤ä¸¤æ­£äº¤ã€‚</li>
<li>An orthonormal set: pairwise orthogonal, each of norm 1.
æ­£äº¤é›†ï¼šä¸¤ä¸¤æ­£äº¤ï¼Œæ¯ç»„èŒƒæ•°ä¸º 1ã€‚</li>
</ul>
<h3 id="why-this-matters-2">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Orthogonality gives structure to vector spaces. It provides a way to
separate independent directions cleanly, simplify computations, and
minimize errors in approximations. Many powerful algorithms in numerical
linear algebra and data science (QR decomposition, least squares
regression, PCA) rely on orthogonality.
æ­£äº¤æ€§èµ‹äºˆå‘é‡ç©ºé—´ç»“æ„ã€‚å®ƒæä¾›äº†ä¸€ç§æ¸…æ™°åœ°åˆ†ç¦»ç‹¬ç«‹æ–¹å‘ã€ç®€åŒ–è®¡ç®—å¹¶æœ€å°åŒ–è¿‘ä¼¼è¯¯å·®çš„æ–¹æ³•ã€‚æ•°å€¼çº¿æ€§ä»£æ•°å’Œæ•°æ®ç§‘å­¦ä¸­è®¸å¤šå¼ºå¤§çš„ç®—æ³•ï¼ˆä¾‹å¦‚
QR åˆ†è§£ã€æœ€å°äºŒä¹˜å›å½’ã€ä¸»æˆåˆ†åˆ†æï¼‰éƒ½ä¾èµ–äºæ­£äº¤æ€§ã€‚</p>
<h3 id="exercises-1.4">Exercises 1.4</h3>
<p>ç»ƒä¹  1.4</p>
<ol type="1">
<li>Verify that the vectors <span class="math inline">\((1,2,2)\)</span>
and <span class="math inline">\((2,0,-1)\)</span> are orthogonal.
éªŒè¯å‘é‡ <span class="math inline">\((1,2,2)\)</span> å’Œ <span
class="math inline">\((2,0,-1)\)</span> æ˜¯å¦æ­£äº¤ã€‚</li>
<li>Find the projection of <span class="math inline">\((3,4)\)</span>
onto <span class="math inline">\((1,1)\)</span>. æ‰¾åˆ° <span
class="math inline">\((3,4)\)</span> åˆ° <span
class="math inline">\((1,1)\)</span> çš„æŠ•å½±ã€‚</li>
<li>Show that any two distinct standard basis vectors in <span
class="math inline">\(\mathbb{R}^n\)</span> are orthogonal. è¯æ˜ <span
class="math inline">\(\mathbb{R}^n\)</span>
ä¸­çš„ä»»æ„ä¸¤ä¸ªä¸åŒçš„æ ‡å‡†åŸºå‘é‡éƒ½æ˜¯æ­£äº¤çš„ã€‚</li>
<li>Decompose <span class="math inline">\((5,2)\)</span> into components
parallel and orthogonal to <span class="math inline">\((2,1)\)</span>.
å°† <span class="math inline">\((5,2)\)</span> åˆ†è§£ä¸ºä¸ <span
class="math inline">\((2,1)\)</span> å¹³è¡Œä¸”æ­£äº¤çš„åˆ†é‡ã€‚</li>
<li>Let <span class="math inline">\(\mathbf{u}, \mathbf{v}\)</span> be
orthogonal nonzero vectors. (a) Show that <span
class="math inline">\((\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=\lVert
\mathbf{u}\rVert^2-\lVert \mathbf{v}\rVert^2.\)</span> (b) For what
condition on <span class="math inline">\(\mathbf{u}\)</span> and <span
class="math inline">\(\mathbf{v}\)</span> does <span
class="math inline">\((\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=0\)</span>?
ä»¤ <span class="math inline">\(\mathbf{u}, \mathbf{v}\)</span>
ä¸ºæ­£äº¤éé›¶å‘é‡ã€‚ï¼ˆaï¼‰è¯æ˜ <span
class="math inline">\((\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=\lVert
\mathbf{u}\rVert^2-\lVert \mathbf{v}\rVert^2.\)</span> ï¼ˆbï¼‰ <span
class="math inline">\((\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=0\)</span>
å¯¹ <span class="math inline">\(\mathbf{u}\)</span> å’Œ <span
class="math inline">\(\mathbf{v}\)</span> æ»¡è¶³ä»€ä¹ˆæ¡ä»¶ï¼Ÿ</li>
</ol>
<h1 id="chapter-2.-matrices">Chapter 2. Matrices</h1>
<p>ç¬¬ 2 ç« çŸ©é˜µ</p>
<h2 id="definition-and-notation">2.1 Definition and Notation</h2>
<p>2.1 å®šä¹‰å’Œç¬¦å·</p>
<p>Matrices are the central objects of linear algebra, providing a
compact way to represent and manipulate linear transformations, systems
of equations, and structured data. A matrix is a rectangular array of
numbers arranged in rows and columns.
çŸ©é˜µæ˜¯çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒå¯¹è±¡ï¼Œå®ƒæä¾›äº†ä¸€ç§ç®€æ´çš„æ–¹å¼æ¥è¡¨ç¤ºå’Œæ“ä½œçº¿æ€§å˜æ¢ã€æ–¹ç¨‹ç»„å’Œç»“æ„åŒ–æ•°æ®ã€‚çŸ©é˜µæ˜¯ç”±æŒ‰è¡Œå’Œåˆ—æ’åˆ—çš„æ•°å­—ç»„æˆçš„çŸ©å½¢é˜µåˆ—ã€‚</p>
<h3 id="formal-definition">Formal Definition</h3>
<p>æ­£å¼å®šä¹‰</p>
<p>An <span class="math inline">\(m \times n\)</span> matrix is an array
with <span class="math inline">\(m\)</span> rows and <span
class="math inline">\(n\)</span> columns, written <span
class="math inline">\(m \times n\)</span> çŸ©é˜µæ˜¯å…·æœ‰ <span
class="math inline">\(m\)</span> è¡Œå’Œ <span
class="math inline">\(n\)</span> åˆ—çš„æ•°ç»„ï¼Œå†™ä¸º</p>
<p><span class="math display">\[
A =\begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\a_{21}
&amp; a_{22} &amp; \cdots &amp; a_{2n} \\\vdots &amp; \vdots &amp;
\ddots &amp; \vdots \\a_{m1} &amp; a_{m2} &amp; \cdots &amp;
a_{mn}\end{bmatrix}.
\]</span></p>
<p>Each entry <span class="math inline">\(a_{ij}\)</span> is a scalar,
located in the <em>i</em>-th row and <em>j</em>-th column. The size (or
dimension) of the matrix is denoted by <span class="math inline">\(m
\times n\)</span>. æ¯ä¸ªæ¡ç›®ğ‘ ğ‘– ğ‘— a ä¼Šå¥‡ â€‹ æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œä½äºç¬¬ - è¡Œå’Œç¬¬ -
åˆ—ã€‚çŸ©é˜µçš„å¤§å°ï¼ˆæˆ–ç»´åº¦ï¼‰ç”¨ <span class="math inline">\(m \times
n\)</span> è¡¨ç¤ºã€‚</p>
<ul>
<li>If <span class="math inline">\(m = n\)</span>, the matrix is square.
å¦‚æœä¸º <span class="math inline">\(m = n\)</span> ï¼Œåˆ™çŸ©é˜µä¸ºæ–¹é˜µã€‚</li>
<li>If <span class="math inline">\(m = 1\)</span>, the matrix is a row
vector. å¦‚æœä¸º <span class="math inline">\(m = 1\)</span>
ï¼Œåˆ™è¯¥çŸ©é˜µä¸ºè¡Œå‘é‡ã€‚</li>
<li>If <span class="math inline">\(n = 1\)</span>, the matrix is a
column vector. å¦‚æœä¸º <span class="math inline">\(n = 1\)</span>
ï¼Œåˆ™çŸ©é˜µä¸ºåˆ—å‘é‡ã€‚</li>
</ul>
<p>Thus, vectors are simply special cases of matrices.
å› æ­¤ï¼Œå‘é‡åªæ˜¯çŸ©é˜µçš„ç‰¹æ®Šæƒ…å†µã€‚</p>
<h3 id="examples">Examples</h3>
<p>ç¤ºä¾‹</p>
<p>Example 2.1.1. A 2Ã—3 matrix: ä¾‹ 2.1.1. 2Ã—3 çŸ©é˜µï¼š</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; -2 &amp; 4 \\0 &amp; 3 &amp; 5\end{bmatrix}.
\]</span></p>
<p>Here, <span class="math inline">\(a_{12} = -2\)</span>, <span
class="math inline">\(a_{23} = 5\)</span>, and the matrix has 2 rows, 3
columns. è¿™é‡Œï¼Œ <span class="math inline">\(a_{12} = -2\)</span> ï¼Œ
<span class="math inline">\(a_{23} = 5\)</span> ï¼ŒçŸ©é˜µæœ‰ 2 è¡Œï¼Œ3
åˆ—ã€‚</p>
<p>Example 2.1.2. A 3Ã—3 square matrix: ä¾‹ 2.1.2. 3Ã—3 æ–¹é˜µï¼š</p>
<p><span class="math display">\[
B = \begin{bmatrix}2 &amp; 0 &amp; 1 \\-1 &amp; 3 &amp; 4 \\0 &amp; 5
&amp; -2\end{bmatrix}.
\]</span></p>
<p>This will later serve as the representation of a linear
transformation on <span class="math inline">\(\mathbb{R}^3\)</span>.
è¿™ç¨åå°†ä½œä¸º <span class="math inline">\(\mathbb{R}^3\)</span>
çš„çº¿æ€§å˜æ¢çš„è¡¨ç¤ºã€‚</p>
<h3 id="indexing-and-notation">Indexing and Notation</h3>
<p>ç´¢å¼•å’Œç¬¦å·</p>
<ul>
<li>Matrices are denoted by uppercase bold letters: <span
class="math inline">\(A, B, C\)</span>. çŸ©é˜µç”¨å¤§å†™ç²—ä½“å­—æ¯è¡¨ç¤ºï¼š <span
class="math inline">\(A, B, C\)</span> ã€‚</li>
<li>Entries are written as <span class="math inline">\(a_{ij}\)</span>,
with the row index first, column index second. æ¡ç›®å†™ä¸ºğ‘ ğ‘– ğ‘— a ä¼Šå¥‡ â€‹
ï¼Œå…¶ä¸­è¡Œç´¢å¼•åœ¨å‰ï¼Œåˆ—ç´¢å¼•åœ¨åã€‚</li>
<li>The set of all real <span class="math inline">\(m \times n\)</span>
matrices is denoted <span class="math inline">\(\mathbb{R}^{m \times
n}\)</span>. æ‰€æœ‰å®æ•° <span class="math inline">\(m \times n\)</span>
çŸ©é˜µçš„é›†åˆè¡¨ç¤ºä¸º <span class="math inline">\(\mathbb{R}^{m \times
n}\)</span> ã€‚</li>
</ul>
<p>Thus, a matrix is a function <span class="math inline">\(A:
{1,\dots,m} \times {1,\dots,n} \to \mathbb{R}\)</span>, assigning a
scalar to each row-column position. å› æ­¤ï¼ŒçŸ©é˜µæ˜¯ä¸€ä¸ªå‡½æ•° <span
class="math inline">\(A: {1,\dots,m} \times {1,\dots,n} \to
\mathbb{R}\)</span> ï¼Œä¸ºæ¯ä¸ªè¡Œåˆ—ä½ç½®åˆ†é…ä¸€ä¸ªæ ‡é‡ã€‚</p>
<h3 id="why-this-matters-3">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Matrices generalize vectors and give us a language for describing
linear operations systematically. They encode systems of equations,
rotations, projections, and transformations of data. With matrices,
algebra and geometry come together: a single compact object can
represent both numerical data and functional rules.
çŸ©é˜µæ¨å¹¿äº†å‘é‡ï¼Œå¹¶ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§ç³»ç»Ÿåœ°æè¿°çº¿æ€§è¿ç®—çš„è¯­è¨€ã€‚å®ƒä»¬å¯¹æ–¹ç¨‹ç»„ã€æ—‹è½¬ã€æŠ•å½±å’Œæ•°æ®å˜æ¢è¿›è¡Œç¼–ç ã€‚çŸ©é˜µå°†ä»£æ•°å’Œå‡ ä½•ç»“åˆåœ¨ä¸€èµ·ï¼šä¸€ä¸ªç´§å‡‘çš„å¯¹è±¡æ—¢å¯ä»¥è¡¨ç¤ºæ•°å€¼æ•°æ®ï¼Œåˆå¯ä»¥è¡¨ç¤ºå‡½æ•°è§„åˆ™ã€‚</p>
<h3 id="exercises-2.1">Exercises 2.1</h3>
<p>ç»ƒä¹  2.1</p>
<ol type="1">
<li>Write a <span class="math inline">\(3 \\times 2\)</span>matrix of
your choice and identify its entries<span
class="math inline">\(a\_{ij}\)</span>. å†™å‡º $3 \times 2 <span
class="math inline">\(matrix of your choice and identify its
entries\)</span> a_{ij}$ã€‚</li>
<li>Is every vector a matrix? Is every matrix a vector? Explain.
æ¯ä¸ªå‘é‡éƒ½æ˜¯çŸ©é˜µå—ï¼Ÿæ¯ä¸ªçŸ©é˜µéƒ½æ˜¯å‘é‡å—ï¼Ÿè¯·è§£é‡Šã€‚</li>
<li>Which of the following are square matrices: <span
class="math inline">\(A \in \mathbb{R}^{4\times4}\)</span>, <span
class="math inline">\(B \in \mathbb{R}^{3\times5}\)</span>, <span
class="math inline">\(C \in \mathbb{R}^{1\times1}\)</span>?
ä¸‹åˆ—å“ªäº›æ˜¯æ­£æ–¹å½¢ çŸ©é˜µï¼š <span class="math inline">\(A \in
\mathbb{R}^{4\times4}\)</span> ï¼Œ <span class="math inline">\(B \in
\mathbb{R}^{3\times5}\)</span> ï¼Œ <span class="math inline">\(C \in
\mathbb{R}^{1\times1}\)</span> ï¼Ÿ</li>
<li>Let è®©</li>
</ol>
<p><span class="math display">\[
D = \begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>What kind of matrix is this? 5. Consider the matrix
è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„çŸ©é˜µï¼Ÿ5. è€ƒè™‘çŸ©é˜µ</p>
<p><span class="math display">\[
E = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}
\]</span></p>
<p>Express <span class="math inline">\(e_{11}, e_{12}, e_{21},
e_{22}\)</span> explicitly. å¿«é€’ğ‘’ 11 , ğ‘’ 12 , ğ‘’ 21 , ğ‘’ 22 e 11 â€‹ ï¼Œe 12 â€‹
ï¼Œe 21 â€‹ ï¼Œe 22 â€‹ æ˜ç¡®åœ°ã€‚</p>
<h2 id="matrix-addition-and-multiplication">2.2 Matrix Addition and
Multiplication</h2>
<p>2.2 çŸ©é˜µåŠ æ³•å’Œä¹˜æ³•</p>
<p>Once matrices are defined, the next step is to understand how they
combine. Just as vectors gain meaning through addition and scalar
multiplication, matrices become powerful through two operations:
addition and multiplication.
å®šä¹‰å¥½çŸ©é˜µåï¼Œä¸‹ä¸€æ­¥å°±æ˜¯ç†è§£å®ƒä»¬æ˜¯å¦‚ä½•ç»„åˆçš„ã€‚æ­£å¦‚å‘é‡é€šè¿‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•è·å¾—æ„ä¹‰ä¸€æ ·ï¼ŒçŸ©é˜µä¹Ÿé€šè¿‡ä¸¤ç§è¿ç®—å˜å¾—å¼ºå¤§ï¼šåŠ æ³•å’Œä¹˜æ³•ã€‚</p>
<h3 id="matrix-addition">Matrix Addition</h3>
<p>çŸ©é˜µåŠ æ³•</p>
<p>Two matrices of the same size are added by adding corresponding
entries. If ä¸¤ä¸ªå¤§å°ç›¸åŒçš„çŸ©é˜µå¯ä»¥é€šè¿‡æ·»åŠ ç›¸åº”çš„å…ƒç´ æ¥ç›¸åŠ ã€‚å¦‚æœ</p>
<p><span class="math display">\[
A = [a_{ij}] \in \mathbb{R}^{m \times n}, \quad B = [b_{ij}] \in
\mathbb{R}^{m \times n},
\]</span></p>
<p>then ç„¶å</p>
<p><span class="math display">\[
A + B = [a_{ij} + b_{ij}] \in \mathbb{R}^{m \times n}.
\]</span></p>
<p>Example 2.2.1. Let ä¾‹ 2.2.1. è®¾</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\3 &amp; 4\end{bmatrix}, \quad B =
\begin{bmatrix}-1 &amp; 0 \\5 &amp; 2\end{bmatrix}.
\]</span></p>
<p>Then ç„¶å</p>
<p><span class="math display">\[
A + B = \begin{bmatrix}1 + (-1) &amp; 2 + 0 \\3 + 5 &amp; 4 +
2\end{bmatrix} =\begin{bmatrix}0 &amp; 2 \\8 &amp; 6\end{bmatrix}.
\]</span></p>
<p>Matrix addition is commutative (<span class="math inline">\(A+B =
B+A\)</span>) and associative (<span class="math inline">\((A+B)+C =
A+(B+C)\)</span>). The zero matrix, with all entries 0, acts as the
additive identity. çŸ©é˜µåŠ æ³•æ»¡è¶³äº¤æ¢å¾‹ ( <span class="math inline">\(A+B
= B+A\)</span> ) å’Œç»“åˆå¾‹ ( <span class="math inline">\((A+B)+C =
A+(B+C)\)</span> )ã€‚é›¶çŸ©é˜µï¼ˆæ‰€æœ‰å…ƒç´ å‡ä¸º 0ï¼‰å……å½“åŠ æ³•æ’ç­‰å¼ã€‚</p>
<h3 id="scalar-multiplication-1">Scalar Multiplication</h3>
<p>æ ‡é‡ä¹˜æ³•</p>
<p>For a scalar <span class="math inline">\(c \in \mathbb{R}\)</span>
and a matrix <span class="math inline">\(A = [[a_{ij}]\)</span>, we
define å¯¹äºæ ‡é‡ <span class="math inline">\(c \in \mathbb{R}\)</span>
å’ŒçŸ©é˜µ <span class="math inline">\(A = [[a_{ij}]\)</span> ï¼Œæˆ‘ä»¬å®šä¹‰</p>
<p><span class="math display">\[
cA = [c \cdot a_{ij}].
\]</span></p>
<p>This stretches or shrinks all entries of the matrix uniformly.
è¿™ä¼šå‡åŒ€åœ°æ‹‰ä¼¸æˆ–æ”¶ç¼©çŸ©é˜µçš„æ‰€æœ‰æ¡ç›®ã€‚</p>
<p>Example 2.2.2. If ä¾‹ 2.2.2. å¦‚æœ</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; -1 \\0 &amp; 3\end{bmatrix}, \quad c = -2,
\]</span></p>
<p>then ç„¶å</p>
<p><span class="math display">\[
cA = \begin{bmatrix}-4 &amp; 2 \\0 &amp; -6\end{bmatrix}.
\]</span></p>
<h3 id="matrix-multiplication">Matrix Multiplication</h3>
<p>çŸ©é˜µä¹˜æ³•</p>
<p>The defining operation of matrices is multiplication. If
çŸ©é˜µçš„å®šä¹‰è¿ç®—æ˜¯ä¹˜æ³•ã€‚å¦‚æœ</p>
<p><span class="math display">\[
A \in \mathbb{R}^{m \times n}, \quad B \in \mathbb{R}^{n \times p},
\]</span></p>
<p>then their product is the <span class="math inline">\(m \times
p\)</span> matrix é‚£ä¹ˆå®ƒä»¬çš„ä¹˜ç§¯å°±æ˜¯ <span class="math inline">\(m
\times p\)</span> çŸ©é˜µ</p>
<p><span class="math display">\[
AB = C = [c_{ij}], \quad c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}.
\]</span></p>
<p>Thus, the entry in the <span class="math inline">\(i\)</span>-th row
and <span class="math inline">\(j\)</span>-th column of <span
class="math inline">\(AB\)</span> is the dot product of the <span
class="math inline">\(i\)</span>-th row of <span
class="math inline">\(A\)</span> with the <span
class="math inline">\(j\)</span>-th column of <span
class="math inline">\(B\)</span>. å› æ­¤ï¼Œ <span
class="math inline">\(AB\)</span> ç¬¬ <span
class="math inline">\(i\)</span> è¡Œã€ç¬¬ <span
class="math inline">\(j\)</span> åˆ—çš„æ¡ç›®æ˜¯ <span
class="math inline">\(A\)</span> ç¬¬ <span
class="math inline">\(i\)</span> è¡Œä¸ <span
class="math inline">\(B\)</span> ç¬¬ <span
class="math inline">\(j\)</span> åˆ—çš„ç‚¹ç§¯ã€‚</p>
<p>Example 2.2.3. Let ä¾‹ 2.2.3. è®¾</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\0 &amp; 3\end{bmatrix}, \quad B =
\begin{bmatrix}4 &amp; -1 \\2 &amp; 5\end{bmatrix}.
\]</span></p>
<p>Then ç„¶å</p>
<p><span class="math display">\[
AB = \begin{bmatrix}1\cdot4 + 2\cdot2 &amp; 1\cdot(-1) + 2\cdot5
\\0\cdot4 + 3\cdot2 &amp; 0\cdot(-1) + 3\cdot5\end{bmatrix}
=\begin{bmatrix}8 &amp; 9 \\6 &amp; 15\end{bmatrix}.
\]</span></p>
<p>Notice that matrix multiplication is not commutative in general:
<span class="math inline">\(AB \neq BA\)</span>. Sometimes <span
class="math inline">\(BA\)</span> may not even be defined if dimensions
do not align. è¯·æ³¨æ„ï¼ŒçŸ©é˜µä¹˜æ³•é€šå¸¸ä¸æ»¡è¶³äº¤æ¢å¾‹ï¼š <span
class="math inline">\(AB \neq BA\)</span>
ã€‚å¦‚æœç»´åº¦ä¸ä¸€è‡´ï¼Œæœ‰æ—¶ç”šè‡³å¯èƒ½æ— æ³•å®šä¹‰ <span
class="math inline">\(BA\)</span> ã€‚</p>
<h3 id="geometric-meaning">Geometric Meaning</h3>
<p>å‡ ä½•æ„ä¹‰</p>
<p>Matrix multiplication corresponds to the composition of linear
transformations. If <span class="math inline">\(A\)</span> transforms
vectors in <span class="math inline">\(\mathbb{R}^n\)</span> and <span
class="math inline">\(B\)</span> transforms vectors in <span
class="math inline">\(\mathbb{R}^p\)</span>, then <span
class="math inline">\(AB\)</span> represents applying <span
class="math inline">\(B\)</span> first, then <span
class="math inline">\(A\)</span>. This makes matrices the algebraic
language of transformations. çŸ©é˜µä¹˜æ³•å¯¹åº”äºçº¿æ€§å˜æ¢çš„å¤åˆã€‚å¦‚æœ <span
class="math inline">\(A\)</span> å˜æ¢ <span
class="math inline">\(\mathbb{R}^n\)</span> ä¸­çš„å‘é‡ï¼Œ <span
class="math inline">\(B\)</span> å˜æ¢ <span
class="math inline">\(\mathbb{R}^p\)</span> ä¸­çš„å‘é‡ï¼Œé‚£ä¹ˆ <span
class="math inline">\(AB\)</span> è¡¨ç¤ºå…ˆåº”ç”¨ <span
class="math inline">\(B\)</span> ï¼Œç„¶åå†åº”ç”¨ <span
class="math inline">\(A\)</span> ã€‚è¿™ä½¿å¾—çŸ©é˜µæˆä¸ºå˜æ¢çš„ä»£æ•°è¯­è¨€ã€‚</p>
<h3 id="notation-4">Notation</h3>
<p>ç¬¦å·</p>
<ul>
<li>Matrix sum: <span class="math inline">\(A+B\)</span>. çŸ©é˜µå’Œï¼š <span
class="math inline">\(A+B\)</span> ã€‚</li>
<li>Scalar multiple: <span class="math inline">\(cA\)</span>. æ ‡é‡å€æ•°ï¼š
<span class="math inline">\(cA\)</span> ã€‚</li>
<li>Product: <span class="math inline">\(AB\)</span>, defined only when
the number of columns of <span class="math inline">\(A\)</span> equals
the number of rows of <span class="math inline">\(B\)</span>. ä¹˜ç§¯ï¼š
<span class="math inline">\(AB\)</span> ï¼Œä»…å½“ <span
class="math inline">\(A\)</span> çš„åˆ—æ•°ç­‰äº <span
class="math inline">\(B\)</span> çš„è¡Œæ•°æ—¶æ‰å®šä¹‰ã€‚</li>
</ul>
<h3 id="why-this-matters-4">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Matrix multiplication is the core mechanism of linear algebra: it
encodes how transformations combine, how systems of equations are
solved, and how data flows in modern algorithms. Addition and scalar
multiplication make matrices into a vector space, while multiplication
gives them an algebraic structure rich enough to model geometry,
computation, and networks.
çŸ©é˜µä¹˜æ³•æ˜¯çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒæœºåˆ¶ï¼šå®ƒç¼–ç äº†å˜æ¢çš„ç»„åˆæ–¹å¼ã€æ–¹ç¨‹ç»„çš„æ±‚è§£æ–¹å¼ä»¥åŠç°ä»£ç®—æ³•ä¸­æ•°æ®æµåŠ¨çš„æ–¹å¼ã€‚åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•å°†çŸ©é˜µè½¬åŒ–ä¸ºå‘é‡ç©ºé—´ï¼Œè€Œä¹˜æ³•åˆ™èµ‹äºˆçŸ©é˜µä¸°å¯Œçš„ä»£æ•°ç»“æ„ï¼Œä½¿å…¶èƒ½å¤Ÿå¯¹å‡ ä½•ã€è®¡ç®—å’Œç½‘ç»œè¿›è¡Œå»ºæ¨¡ã€‚</p>
<h3 id="exercises-2.2">Exercises 2.2</h3>
<p>ç»ƒä¹  2.2</p>
<ol type="1">
<li>Compute <span class="math inline">\(A+B\)</span> for è®¡ç®— <span
class="math inline">\(A+B\)</span></li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 3 \\-1 &amp; 0 \end{bmatrix}, \quad B =
\begin{bmatrix} 4 &amp; -2 \\5 &amp; 7 \end{bmatrix}.
\]</span></p>
<ol start="2" type="1">
<li>Find 3A where æŸ¥æ‰¾ 3A</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; -4 \\2 &amp; 6 \end{bmatrix}.
\]</span></p>
<ol start="3" type="1">
<li>Multiply ä¹˜</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 0 &amp; 2 \\-1 &amp; 3 &amp; 1
\end{bmatrix}, \quad B = \begin{bmatrix} 2 &amp; 1 \\0 &amp; -1 \\3
&amp; 4 \end{bmatrix}.
\]</span></p>
<ol start="4" type="1">
<li>Verify with an explicit example that <span class="math inline">\(AB
\neq BA\)</span>. é€šè¿‡æ˜ç¡®çš„ä¾‹å­æ¥éªŒè¯ <span class="math inline">\(AB
\neq BA\)</span> ã€‚</li>
<li>Prove that matrix multiplication is distributive: <span
class="math inline">\(A(B+C) = AB + AC\)</span>. è¯æ˜çŸ©é˜µä¹˜æ³•æ˜¯åˆ†é…çš„ï¼š
<span class="math inline">\(A(B+C) = AB + AC\)</span> ã€‚</li>
</ol>
<h2 id="transpose-and-inverse">2.3 Transpose and Inverse</h2>
<p>2.3 è½¬ç½®å’Œé€†</p>
<p>Two special operations on matrices-the transpose and the inverse-give
rise to deep algebraic and geometric properties. The transpose
rearranges a matrix by flipping it across its main diagonal, while the
inverse, when it exists, acts as the undo operation for matrix
multiplication.
çŸ©é˜µçš„ä¸¤ç§ç‰¹æ®Šè¿ç®—â€”â€”è½¬ç½®å’Œé€†â€”â€”å¼•å‡ºäº†æ·±åˆ»çš„ä»£æ•°å’Œå‡ ä½•æ€§è´¨ã€‚è½¬ç½®é€šè¿‡æ²¿çŸ©é˜µä¸»å¯¹è§’çº¿ç¿»è½¬æ¥é‡æ–°æ’åˆ—çŸ©é˜µï¼Œè€Œé€†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰åˆ™å……å½“çŸ©é˜µä¹˜æ³•çš„æ’¤æ¶ˆæ“ä½œã€‚</p>
<h3 id="the-transpose">The Transpose</h3>
<p>è½¬ç½®</p>
<p>The transpose of an <span class="math inline">\(m \times n\)</span>
matrix <span class="math inline">\(A = [a_{ij}]\)</span> is the <span
class="math inline">\(n \times m\)</span> matrix <span
class="math inline">\(A^T = [a_{ji}]\)</span>, obtained by swapping rows
and columns. <span class="math inline">\(m \times n\)</span> çŸ©é˜µ <span
class="math inline">\(A = [a_{ij}]\)</span> çš„è½¬ç½®æ˜¯é€šè¿‡äº¤æ¢è¡Œå’Œåˆ—è·å¾—çš„
<span class="math inline">\(n \times m\)</span> çŸ©é˜µ <span
class="math inline">\(A^T = [a_{ji}]\)</span> ã€‚</p>
<p>Formally, æ­£å¼åœ°ï¼Œ</p>
<p><span class="math display">\[
(A^T)\_{ij} = a\_{ji}.
\]</span></p>
<p>Example 2.3.1. If ä¾‹ 2.3.1. å¦‚æœ</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 4 &amp; -2 \\0 &amp; 3 &amp; 5\end{bmatrix},
\]</span></p>
<p>then ç„¶å</p>
<p><span class="math display">\[
A^T = \begin{bmatrix}1 &amp; 0 \\4 &amp; 3 \\-2 &amp; 5\end{bmatrix}.
\]</span></p>
<p>Properties of the Transpose. è½¬ç½®çš„å±æ€§ã€‚</p>
<ol type="1">
<li><span class="math inline">\((A^T)^T = A\)</span>.</li>
<li><span class="math inline">\((A+B)^T = A^T + B^T\)</span>.</li>
<li><span class="math inline">\((cA)^T = cA^T\)</span>, for scalar <span
class="math inline">\(c\)</span>. <span class="math inline">\((cA)^T =
cA^T\)</span> ï¼Œå¯¹äºæ ‡é‡ <span class="math inline">\(c\)</span> ã€‚</li>
<li><span class="math inline">\((AB)^T = B^T A^T\)</span>.</li>
</ol>
<p>The last rule is crucial: the order reverses.
æœ€åä¸€æ¡è§„åˆ™è‡³å…³é‡è¦ï¼šé¡ºåºåè½¬ã€‚</p>
<h3 id="the-inverse">The Inverse</h3>
<p>é€†å‘</p>
<p>A square matrix <span class="math inline">\(A \in \mathbb{R}^{n
\times n}\)</span> is said to be invertible (or nonsingular) if there
exists another matrix <span class="math inline">\(A^{-1}\)</span> such
that å¦‚æœå­˜åœ¨å¦ä¸€ä¸ªçŸ©é˜µ <span class="math inline">\(A^{-1}\)</span>
æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™ç§°æ–¹é˜µ <span class="math inline">\(A \in \mathbb{R}^{n
\times n}\)</span> å¯é€†ï¼ˆæˆ–éå¥‡å¼‚ï¼‰</p>
<p><span class="math display">\[
AA^{-1} = A^{-1}A = I_n,
\]</span></p>
<p>where <span class="math inline">\(I_n\)</span> is the <span
class="math inline">\(n \times n\)</span> identity matrix. In this case,
<span class="math inline">\(A^{-1}\)</span> is called the inverse of
<span class="math inline">\(A\)</span>. å…¶ä¸­ğ¼ ğ‘› I n â€‹ æ˜¯ <span
class="math inline">\(n \times n\)</span> å•ä½çŸ©é˜µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ <span
class="math inline">\(A^{-1}\)</span> è¢«ç§°ä¸º <span
class="math inline">\(A\)</span> çš„é€†ã€‚</p>
<p>Not every matrix is invertible. A necessary condition is that <span
class="math inline">\(\det(A) \neq 0\)</span>, a fact that will be
developed in Chapter 6. å¹¶éæ‰€æœ‰çŸ©é˜µéƒ½æ˜¯å¯é€†çš„ã€‚å¿…è¦æ¡ä»¶æ˜¯ <span
class="math inline">\(\det(A) \neq 0\)</span> ï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬ 6
ç« ä¸­è¿›ä¸€æ­¥é˜è¿°ã€‚</p>
<p>Example 2.3.2. Let ä¾‹ 2.3.2. è®¾</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\3 &amp; 4\end{bmatrix}.
\]</span></p>
<p>Its determinant is <span class="math inline">\(\det(A) = (1)(4) -
(2)(3) = -2 \neq 0\)</span>. The inverse is å®ƒçš„è¡Œåˆ—å¼æ˜¯ <span
class="math inline">\(\det(A) = (1)(4) - (2)(3) = -2 \neq 0\)</span>
ã€‚é€†æ˜¯</p>
<p><span class="math display">\[
A^{-1} = \frac{1}{\det(A)} \begin{bmatrix}4 &amp; -2 \\-3 &amp;
1\end{bmatrix} =\begin{bmatrix}-2 &amp; 1 \\1.5 &amp; -0.5\end{bmatrix}.
\]</span></p>
<p>Verification: ç¡®è®¤ï¼š</p>
<p><span class="math display">\[
AA^{-1} = \begin{bmatrix}1 &amp; 2 \\3 &amp;
4\end{bmatrix}\begin{bmatrix}-2 &amp; 1 \\1.5 &amp; -0.5\end{bmatrix}
=\begin{bmatrix}1 &amp; 0 \\0 &amp; 1\end{bmatrix}.
\]</span></p>
<h3 id="geometric-meaning-1">Geometric Meaning</h3>
<p>å‡ ä½•æ„ä¹‰</p>
<ul>
<li>The transpose corresponds to reflecting a linear transformation
across the diagonal. For vectors, it switches between row and column
forms.
è½¬ç½®å¯¹åº”äºæ²¿å¯¹è§’çº¿åæ˜ çº¿æ€§å˜æ¢ã€‚å¯¹äºå‘é‡ï¼Œå®ƒåœ¨è¡Œå’Œåˆ—å½¢å¼ä¹‹é—´åˆ‡æ¢ã€‚</li>
<li>The inverse, when it exists, corresponds to reversing a linear
transformation. For example, if <span class="math inline">\(A\)</span>
scales and rotates vectors, <span class="math inline">\(A^{-1}\)</span>
rescales and rotates them back.
å¦‚æœå­˜åœ¨é€†å˜æ¢ï¼Œåˆ™å®ƒå¯¹åº”äºçº¿æ€§å˜æ¢çš„é€†å˜æ¢ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ <span
class="math inline">\(A\)</span> ç¼©æ”¾å¹¶æ—‹è½¬äº†çŸ¢é‡ï¼Œåˆ™ <span
class="math inline">\(A^{-1}\)</span> ä¼šå°†å…¶é‡æ–°ç¼©æ”¾å¹¶æ—‹è½¬å›å»ã€‚</li>
</ul>
<h3 id="notation-5">Notation</h3>
<p>ç¬¦å·</p>
<ul>
<li>Transpose: <span class="math inline">\(A^T\)</span>. è½¬ç½®ï¼š <span
class="math inline">\(A^T\)</span> ã€‚</li>
<li>Inverse: <span class="math inline">\(A^{-1}\)</span>, defined only
for invertible square matrices. é€†ï¼š <span
class="math inline">\(A^{-1}\)</span> ï¼Œä»…ä¸ºå¯é€†æ–¹é˜µå®šä¹‰ã€‚</li>
<li>Identity: <span class="math inline">\(I_n\)</span>, acts as the
multiplicative identity. èº«ä»½ï¼šğ¼ ğ‘› I n â€‹ ï¼Œå……å½“ä¹˜æ³•æ’ç­‰å¼ã€‚</li>
</ul>
<h3 id="why-this-matters-5">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>The transpose allows us to define symmetric and orthogonal matrices,
central to geometry and numerical methods. The inverse underlies the
solution of linear systems, encoding the idea of undoing a
transformation. Together, these operations set the stage for
determinants, eigenvalues, and orthogonalization.
è½¬ç½®ä½¿æˆ‘ä»¬èƒ½å¤Ÿå®šä¹‰å¯¹ç§°çŸ©é˜µå’Œæ­£äº¤çŸ©é˜µï¼Œè¿™æ˜¯å‡ ä½•å’Œæ•°å€¼æ–¹æ³•çš„æ ¸å¿ƒã€‚é€†çŸ©é˜µæ˜¯çº¿æ€§ç³»ç»Ÿè§£çš„åŸºç¡€ï¼Œå®ƒè•´å«ç€æ’¤é”€å˜æ¢çš„æ€æƒ³ã€‚è¿™äº›è¿ç®—å…±åŒä¸ºè¡Œåˆ—å¼ã€ç‰¹å¾å€¼å’Œæ­£äº¤åŒ–å¥ å®šäº†åŸºç¡€ã€‚</p>
<h3 id="exercises-2.3">Exercises 2.3</h3>
<p>ç»ƒä¹  2.3</p>
<ol type="1">
<li>Compute the transpose of è®¡ç®—è½¬ç½®</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; -1 &amp; 3 \\ 0 &amp; 4 &amp; 5
\end{bmatrix}.
\]</span></p>
<ol start="2" type="1">
<li>Verify that <span class="math inline">\((AB)^T = B^T A^T\)</span>
for éªŒè¯ <span class="math inline">\((AB)^T = B^T A^T\)</span></li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\0 &amp; 1 \end{bmatrix}, \quad B =
\begin{bmatrix}3 &amp; 4 \\5 &amp; 6 \end{bmatrix}.
\]</span></p>
<ol start="3" type="1">
<li>Determine whether ç¡®å®šæ˜¯å¦</li>
</ol>
<p><span class="math display">\[
C = \begin{bmatrix}2 &amp; 1 \\4 &amp; 2 \end{bmatrix}
\]</span></p>
<p>is invertible. If so, find <span
class="math inline">\(C^{-1}\)</span>. å¯é€†ã€‚å¦‚æœå¯é€†ï¼Œåˆ™æ±‚ <span
class="math inline">\(C^{-1}\)</span> ã€‚</p>
<ol start="4" type="1">
<li>Find the inverse of æ±‚é€†</li>
</ol>
<p><span class="math display">\[
D = \begin{bmatrix}0 &amp; 1 \\-1 &amp; 0 \end{bmatrix},
\]</span></p>
<p>and explain its geometric action on vectors in the plane.
å¹¶è§£é‡Šå…¶å¯¹å¹³é¢å‘é‡çš„å‡ ä½•ä½œç”¨ã€‚</p>
<ol start="5" type="1">
<li>Prove that if <span class="math inline">\(A\)</span> is invertible,
then so is <span class="math inline">\(A^T\)</span>, and <span
class="math inline">\((A^T)^{-1} = (A^{-1})^T\)</span>. è¯æ˜å¦‚æœ <span
class="math inline">\(A\)</span> å¯é€†ï¼Œåˆ™ <span
class="math inline">\(A^T\)</span> å’Œ <span
class="math inline">\((A^T)^{-1} = (A^{-1})^T\)</span> ä¹Ÿå¯é€†ã€‚</li>
</ol>
<h2 id="special-matrices">2.4 Special Matrices</h2>
<p>2.4 ç‰¹æ®ŠçŸ©é˜µ</p>
<p>Certain matrices occur so frequently in theory and applications that
they are given special names. Recognizing their properties allows us to
simplify computations and understand the structure of linear
transformations more clearly.
æŸäº›çŸ©é˜µåœ¨ç†è®ºå’Œåº”ç”¨ä¸­å‡ºç°é¢‘ç‡å¾ˆé«˜ï¼Œå› æ­¤è¢«èµ‹äºˆäº†ç‰¹æ®Šçš„åç§°ã€‚äº†è§£å®ƒä»¬çš„æ€§è´¨å¯ä»¥ç®€åŒ–è®¡ç®—ï¼Œå¹¶æ›´æ¸…æ¥šåœ°ç†è§£çº¿æ€§å˜æ¢çš„ç»“æ„ã€‚</p>
<h3 id="the-identity-matrix">The Identity Matrix</h3>
<p>èº«ä»½çŸ©é˜µ</p>
<p>The identity matrix <span class="math inline">\(I_n\)</span> is the
<span class="math inline">\(n \times n\)</span> matrix with ones on the
diagonal and zeros elsewhere: å•ä½çŸ©é˜µğ¼ ğ‘› I n â€‹ æ˜¯ <span
class="math inline">\(n \times n\)</span> çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸Šä¸º 1ï¼Œå…¶ä»–ä½ç½®ä¸º
0ï¼š</p>
<p><span class="math display">\[
I_n = \begin{bmatrix}1 &amp; 0 &amp; \cdots &amp; 0 \\0 &amp; 1 &amp;
\cdots &amp; 0 \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\0 &amp;
0 &amp; \cdots &amp; 1\end{bmatrix}.
\]</span></p>
<p>It acts as the multiplicative identity: å®ƒå……å½“ä¹˜æ³•æ’ç­‰å¼ï¼š</p>
<p><span class="math display">\[
AI_n = I_nA = A, \quad \text{for all } A \in \mathbb{R}^{n \times n}.
\]</span></p>
<p>Geometrically, <span class="math inline">\(I_n\)</span> represents
the transformation that leaves every vector unchanged. ä»å‡ ä½•å­¦ä¸Šè®²ï¼Œğ¼ ğ‘›
I n â€‹ è¡¨ç¤ºä¿æŒæ¯ä¸ªå‘é‡ä¸å˜çš„å˜æ¢ã€‚</p>
<h3 id="diagonal-matrices">Diagonal Matrices</h3>
<p>å¯¹è§’çŸ©é˜µ</p>
<p>A diagonal matrix has all off-diagonal entries zero:
å¯¹è§’çŸ©é˜µçš„æ‰€æœ‰éå¯¹è§’å…ƒç´ å‡ä¸ºé›¶ï¼š</p>
<p><span class="math display">\[
D = \begin{bmatrix}d_{11} &amp; 0 &amp; \cdots &amp; 0 \\0 &amp; d_{22}
&amp; \cdots &amp; 0 \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\0
&amp; 0 &amp; \cdots &amp; d_{nn}\end{bmatrix}.
\]</span></p>
<p>Multiplication by a diagonal matrix scales each coordinate
independently: ä¸å¯¹è§’çŸ©é˜µç›¸ä¹˜å¯ç‹¬ç«‹ç¼©æ”¾æ¯ä¸ªåæ ‡ï¼š</p>
<p><span class="math display">\[
D\mathbf{x} = (d_{11}x_1, d_{22}x_2, \dots, d_{nn}x_n).
\]</span></p>
<p>Example 2.4.1. Let ä¾‹ 2.4.1. è®¾</p>
<p><span class="math display">\[
D = \begin{bmatrix} 2 &amp; 0 &amp; 0 \\0 &amp; 3 &amp; 0 \\0 &amp; 0
&amp; -1 \end{bmatrix}, \quad\mathbf{x} = \begin{bmatrix}1 \\4 \\-2
\end{bmatrix}.
\]</span></p>
<p>Then ç„¶å</p>
<p><span class="math display">\[
D\mathbf{x} = \begin{bmatrix}2 \\12 \\2 \end{bmatrix}.
\]</span></p>
<h3 id="permutation-matrices">Permutation Matrices</h3>
<p>ç½®æ¢çŸ©é˜µ</p>
<p>A permutation matrix is obtained by permuting the rows of the
identity matrix. Multiplying a vector by a permutation matrix reorders
its coordinates.
ç½®æ¢çŸ©é˜µæ˜¯é€šè¿‡å¯¹å•ä½çŸ©é˜µçš„è¡Œè¿›è¡Œç½®æ¢è€Œå¾—åˆ°çš„ã€‚å°†å‘é‡ä¹˜ä»¥ç½®æ¢çŸ©é˜µä¼šé‡æ–°æ’åºå…¶åæ ‡ã€‚</p>
<p>Example 2.4.2. Let ä¾‹ 2.4.2. è®¾</p>
<p><span class="math display">\[
P = \begin{bmatrix}0 &amp; 1 &amp; 0 \\1 &amp; 0 &amp; 0 \\0 &amp; 0
&amp; 1\end{bmatrix}.
\]</span></p>
<p>Then ç„¶å</p>
<p><span class="math display">\[
P\begin{bmatrix}a \\b \\c \end{bmatrix} =\begin{bmatrix} b \\a \\c
\end{bmatrix}.
\]</span></p>
<p>Thus, <span class="math inline">\(P\)</span> swaps the first two
coordinates. å› æ­¤ï¼Œ <span class="math inline">\(P\)</span>
äº¤æ¢å‰ä¸¤ä¸ªåæ ‡ã€‚</p>
<p>Permutation matrices are always invertible; their inverses are simply
their transposes. ç½®æ¢çŸ©é˜µæ€»æ˜¯å¯é€†çš„ï¼›å®ƒä»¬çš„é€†åªæ˜¯å®ƒä»¬çš„è½¬ç½®ã€‚</p>
<h3 id="symmetric-and-skew-symmetric-matrices">Symmetric and
Skew-Symmetric Matrices</h3>
<p>å¯¹ç§°çŸ©é˜µå’Œæ–œå¯¹ç§°çŸ©é˜µ</p>
<p>A matrix is symmetric if å¦‚æœçŸ©é˜µæ˜¯å¯¹ç§°çš„</p>
<p><span class="math display">\[
A^T = A,
\]</span></p>
<p>and skew-symmetric if Symmetric matrices appear in quadratic forms
and optimization, while skew-symmetric matrices describe rotations and
cross products in geometry.
å¦‚æœå¯¹ç§°çŸ©é˜µå‡ºç°åœ¨äºŒæ¬¡å½¢å¼å’Œä¼˜åŒ–ä¸­ï¼Œåˆ™ä¸ºæ–œå¯¹ç§°ï¼Œè€Œæ–œå¯¹ç§°çŸ©é˜µæè¿°å‡ ä½•ä¸­çš„æ—‹è½¬å’Œå‰ç§¯ã€‚</p>
<h3 id="orthogonal-matrices">Orthogonal Matrices</h3>
<p>æ­£äº¤çŸ©é˜µ</p>
<p>A square matrix <span class="math inline">\(Q\)</span> is orthogonal
if æ–¹é˜µ <span class="math inline">\(Q\)</span> æ˜¯æ­£äº¤çš„ï¼Œå¦‚æœ</p>
<p><span class="math display">\[
Q^T Q = QQ^T = I.
\]</span></p>
<p>Equivalently, the rows (and columns) of <span
class="math inline">\(Q\)</span> form an orthonormal set. Orthogonal
matrices preserve lengths and angles; they represent rotations and
reflections. ç­‰ä»·åœ°ï¼Œ <span class="math inline">\(Q\)</span>
çš„è¡Œï¼ˆå’Œåˆ—ï¼‰æ„æˆä¸€ä¸ªæ­£äº¤é›†ã€‚æ­£äº¤çŸ©é˜µä¿ç•™é•¿åº¦å’Œè§’åº¦ï¼›å®ƒä»¬è¡¨ç¤ºæ—‹è½¬å’Œåå°„ã€‚</p>
<p>Example 2.4.3. The rotation matrix in the plane: ä¾‹2.4.3.
å¹³é¢å†…çš„æ—‹è½¬çŸ©é˜µ:</p>
<p><span class="math display">\[
R(\theta) = \begin{bmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta
&amp; \cos\theta\end{bmatrix}
\]</span></p>
<p>is orthogonal, since æ˜¯æ­£äº¤çš„ï¼Œå› ä¸º</p>
<p><span class="math display">\[
R(\theta)^T R(\theta) = I_2.
\]</span></p>
<h3 id="why-this-matters-6">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Special matrices serve as the building blocks of linear algebra.
Identity matrices define the neutral element, diagonal matrices simplify
computations, permutation matrices reorder data, symmetric and
orthogonal matrices describe fundamental geometric structures. Much of
modern applied mathematics reduces complex problems to operations
involving these simple forms.
ç‰¹æ®ŠçŸ©é˜µæ˜¯çº¿æ€§ä»£æ•°çš„åŸºçŸ³ã€‚å•ä½çŸ©é˜µå®šä¹‰ä¸­æ€§å…ƒç´ ï¼Œå¯¹è§’çŸ©é˜µç®€åŒ–è®¡ç®—ï¼Œç½®æ¢çŸ©é˜µé‡æ–°æ’åºæ•°æ®ï¼Œå¯¹ç§°çŸ©é˜µå’Œæ­£äº¤çŸ©é˜µæè¿°åŸºæœ¬å‡ ä½•ç»“æ„ã€‚è®¸å¤šç°ä»£åº”ç”¨æ•°å­¦å°†å¤æ‚é—®é¢˜ç®€åŒ–ä¸ºæ¶‰åŠè¿™äº›ç®€å•å½¢å¼çš„è¿ç®—ã€‚</p>
<h3 id="exercises-2.4">Exercises 2.4</h3>
<p>ç»ƒä¹  2.4</p>
<ol type="1">
<li>Show that the product of two diagonal matrices is diagonal, and
compute an example.
è¯æ˜ä¸¤ä¸ªå¯¹è§’çŸ©é˜µçš„ä¹˜ç§¯æ˜¯å¯¹è§’çš„ï¼Œå¹¶è®¡ç®—ä¸€ä¸ªä¾‹å­ã€‚</li>
<li>Find the permutation matrix that cycles <span
class="math inline">\((a,b,c)\)</span> into <span
class="math inline">\((b,c,a)\)</span>. æ‰¾åˆ°å°† <span
class="math inline">\((a,b,c)\)</span> å¾ªç¯åˆ° <span
class="math inline">\((b,c,a)\)</span> çš„ç½®æ¢çŸ©é˜µã€‚</li>
<li>Prove that every permutation matrix is invertible and its inverse is
its transpose. è¯æ˜æ¯ä¸ªç½®æ¢çŸ©é˜µéƒ½æ˜¯å¯é€†çš„ï¼Œå¹¶ä¸”å®ƒçš„é€†æ˜¯å®ƒçš„è½¬ç½®ã€‚</li>
<li>Verify that éªŒè¯</li>
</ol>
<p><span class="math display">\[
Q = \begin{bmatrix}0 &amp; 1 \\-1 &amp; 0 \end{bmatrix}
\]</span></p>
<p>is orthogonal. What geometric transformation does it represent? 5.
Determine whether æ˜¯æ­£äº¤çš„ã€‚å®ƒä»£è¡¨ä»€ä¹ˆå‡ ä½•å˜æ¢ï¼Ÿ5. åˆ¤æ–­</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 3 \\3 &amp; 2 \end{bmatrix}, \quad B =
\begin{bmatrix}0 &amp; 5 \\-5 &amp; 0 \end{bmatrix}
\]</span></p>
<p>are symmetric, skew-symmetric, or neither.
æ˜¯å¯¹ç§°çš„ã€æ–œå¯¹ç§°çš„ï¼Œæˆ–è€…éƒ½ä¸æ˜¯ã€‚</p>
<h1 id="chapter-3.-systems-of-linear-equations">Chapter 3. Systems of
Linear Equations</h1>
<p>ç¬¬ 3 ç« çº¿æ€§æ–¹ç¨‹ç»„</p>
<h2 id="linear-systems-and-solutions">3.1 Linear Systems and
Solutions</h2>
<p>3.1 çº¿æ€§ç³»ç»ŸåŠå…¶è§£</p>
<p>One of the central motivations for linear algebra is solving systems
of linear equations. These systems arise naturally in science,
engineering, and data analysis whenever multiple constraints interact.
Matrices provide a compact language for expressing and solving them.
çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒåŠ¨æœºä¹‹ä¸€æ˜¯æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ã€‚åœ¨ç§‘å­¦ã€å·¥ç¨‹å’Œæ•°æ®åˆ†æé¢†åŸŸï¼Œå½“å¤šä¸ªçº¦æŸç›¸äº’ä½œç”¨æ—¶ï¼Œè¿™ç±»æ–¹ç¨‹ç»„è‡ªç„¶è€Œç„¶åœ°å‡ºç°ã€‚çŸ©é˜µæä¾›äº†ä¸€ç§ç®€æ´çš„è¯­è¨€æ¥è¡¨è¾¾å’Œæ±‚è§£å®ƒä»¬ã€‚</p>
<h3 id="linear-systems">Linear Systems</h3>
<p>çº¿æ€§ç³»ç»Ÿ</p>
<p>A linear system consists of equations where each unknown appears only
to the first power and with no products between variables. A general
system of <span class="math inline">\(m\)</span> equations in <span
class="math inline">\(n\)</span> unknowns can be written as:
çº¿æ€§ç³»ç»Ÿç”±æ–¹ç¨‹ç»„æˆï¼Œå…¶ä¸­æ¯ä¸ªæœªçŸ¥æ•°ä»…å‡ºç°ä¸€æ¬¡æ–¹ï¼Œå¹¶ä¸”ä¹‹é—´æ²¡æœ‰ä¹˜ç§¯
å˜é‡ã€‚åŒ…å« <span class="math inline">\(n\)</span> ä¸ªæœªçŸ¥æ•°çš„ <span
class="math inline">\(m\)</span> ä¸ªæ–¹ç¨‹çš„ä¸€èˆ¬ç³»ç»Ÿå¯ä»¥å†™æˆï¼š</p>
<p><span class="math display">\[
\begin{aligned}a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &amp;= b_1,
\\a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &amp;= b_2, \\&amp;\vdots
\\a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &amp;= b_m.\end{aligned}
\]</span></p>
<p>Here the coefficients <span class="math inline">\(a_{ij}\)</span> and
constants <span class="math inline">\(b_i\)</span> are scalars, and the
unknowns are <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>.
è¿™é‡Œç³»æ•°ğ‘ ğ‘– ğ‘— a ä¼Šå¥‡ â€‹ å’Œå¸¸æ•°ğ‘ ğ‘– b i â€‹ æ˜¯æ ‡é‡ï¼ŒæœªçŸ¥æ•°æ˜¯ğ‘¥ 1 , ğ‘¥ 2 , â€¦ , ğ‘¥ ğ‘›
x 1 â€‹ ï¼Œx 2 â€‹ ï¼Œâ€¦ï¼Œx n â€‹ .</p>
<h3 id="matrix-form">Matrix Form</h3>
<p>çŸ©é˜µå½¢å¼</p>
<p>The system can be expressed compactly as:
è¯¥ç³»ç»Ÿå¯ä»¥ç®€æ´åœ°è¡¨ç¤ºä¸ºï¼š</p>
<p><span class="math display">\[
A\mathbf{x} = \mathbf{b},
\]</span></p>
<p>where åœ¨å“ªé‡Œ</p>
<ul>
<li><span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>
is the coefficient matrix <span class="math inline">\([a_{ij}]\)</span>,
<span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>
æ˜¯ç³»æ•°çŸ©é˜µ <span class="math inline">\([a_{ij}]\)</span> ï¼Œ</li>
<li><span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> is
the column vector of unknowns, <span class="math inline">\(\mathbf{x}
\in \mathbb{R}^n\)</span> æ˜¯æœªçŸ¥æ•°çš„åˆ—å‘é‡ï¼Œ</li>
<li><span class="math inline">\(\mathbf{b} \in \mathbb{R}^m\)</span> is
the column vector of constants. <span class="math inline">\(\mathbf{b}
\in \mathbb{R}^m\)</span> æ˜¯å¸¸æ•°åˆ—å‘é‡ã€‚</li>
</ul>
<p>This formulation turns the problem of solving equations into
analyzing the action of a matrix.
è¿™ä¸ªå…¬å¼å°†è§£æ–¹ç¨‹çš„é—®é¢˜è½¬åŒ–ä¸ºåˆ†æçŸ©é˜µçš„ä½œç”¨ã€‚</p>
<p>Example 3.1.1. The system ä¾‹ 3.1.1. ç³»ç»Ÿ</p>
<p><span class="math display">\[
\begin{cases}x + 2y = 5, \\3x - y = 4\end{cases}
\]</span></p>
<p>can be written as å¯ä»¥å†™æˆ</p>
<p><span class="math display">\[
\begin{bmatrix} 1 &amp; 2 \\ 3 &amp; -1 \end{bmatrix}\begin{bmatrix} x
\\ y \end{bmatrix}=\begin{bmatrix} 5 \\ 4 \end{bmatrix}.
\]</span></p>
<h3 id="types-of-solutions">Types of Solutions</h3>
<p>è§£å†³æ–¹æ¡ˆç±»å‹</p>
<p>A linear system may have: çº¿æ€§ç³»ç»Ÿå¯èƒ½æœ‰ï¼š</p>
<ol type="1">
<li>No solution (inconsistent): The equations conflict. Example:
æ— è§£ï¼ˆä¸ä¸€è‡´ï¼‰ï¼šæ–¹ç¨‹å¼ç›¸äº’çŸ›ç›¾ã€‚ä¾‹å¦‚ï¼š</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y = 1 \\x + y = 2\end{cases}
\]</span></p>
<p>This system has no solution. è¿™ä¸ªç³»ç»Ÿæ²¡æœ‰è§£å†³æ–¹æ¡ˆã€‚</p>
<ol start="2" type="1">
<li>Exactly one solution (unique): The systemâ€™s equations intersect at a
single point. åªæœ‰ä¸€ä¸ªè§£ï¼ˆå”¯ä¸€ï¼‰ï¼šç³»ç»Ÿæ–¹ç¨‹åœ¨ä¸€ä¸ªç‚¹ç›¸äº¤ã€‚ Example: The
following coefficient matrix: ä¾‹å¦‚ï¼šä»¥ä¸‹ç³»æ•°çŸ©é˜µï¼š</li>
</ol>
<p><span class="math display">\[
\begin{bmatrix}1 &amp; 2 \\3 &amp; -1\end{bmatrix}
\]</span></p>
<p>has a unique solution. æœ‰ä¸€ä¸ªç‹¬ç‰¹çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<ol start="3" type="1">
<li>Infinitely many solutions: The equations describe overlapping
constraints (e.g., multiple equations representing the same line or
plane).
æ— æ•°ä¸ªè§£ï¼šæ–¹ç¨‹æè¿°é‡å çš„çº¦æŸï¼ˆä¾‹å¦‚ï¼Œè¡¨ç¤ºåŒä¸€æ¡çº¿æˆ–å¹³é¢çš„å¤šä¸ªæ–¹ç¨‹ï¼‰ã€‚</li>
</ol>
<p>The nature of the solution depends on the rank of <span
class="math inline">\(A\)</span> and its relation to the augmented
matrix <span class="math inline">\((A|\mathbf{b})\)</span>, which we
will study later. è§£çš„æ€§è´¨å–å†³äº <span class="math inline">\(A\)</span>
çš„ç§©åŠå…¶ä¸å¢å¹¿çŸ©é˜µ <span class="math inline">\((A|\mathbf{b})\)</span>
çš„å…³ç³»ï¼Œæˆ‘ä»¬ç¨åä¼šç ”ç©¶ã€‚</p>
<h3 id="geometric-interpretation-1">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<ul>
<li>In <span class="math inline">\(\mathbb{R}^2\)</span>, each linear
equation represents a line. Solving a system means finding intersection
points of lines. åœ¨ <span class="math inline">\(\mathbb{R}^2\)</span>
ä¸­ï¼Œæ¯ä¸ªçº¿æ€§æ–¹ç¨‹ä»£è¡¨ä¸€æ¡ç›´çº¿ã€‚æ±‚è§£æ–¹ç¨‹ç»„æ„å‘³ç€æ‰¾åˆ°ç›´çº¿çš„äº¤ç‚¹ã€‚</li>
<li>In <span class="math inline">\(\mathbb{R}^3\)</span>, each equation
represents a plane. A system may have no solution (parallel planes), one
solution (a unique intersection point), or infinitely many (a line of
intersection). åœ¨ <span class="math inline">\(\mathbb{R}^3\)</span>
ä¸­ï¼Œæ¯ä¸ªæ–¹ç¨‹ä»£è¡¨ä¸€ä¸ªå¹³é¢ã€‚ä¸€ä¸ªæ–¹ç¨‹ç»„å¯èƒ½æ²¡æœ‰è§£ï¼ˆå¹³è¡Œå¹³é¢ï¼‰ï¼Œå¯èƒ½æœ‰ä¸€ä¸ªè§£ï¼ˆå”¯ä¸€çš„äº¤ç‚¹ï¼‰ï¼Œä¹Ÿå¯èƒ½æœ‰æ— æ•°ä¸ªè§£ï¼ˆä¸€æ¡äº¤çº¿ï¼‰ã€‚</li>
<li>In higher dimensions, the picture generalizes: solutions form
intersections of hyperplanes.
åœ¨æ›´é«˜ç»´åº¦ä¸­ï¼Œè¯¥å›¾æ¦‚æ‹¬ä¸ºï¼šè§£å†³æ–¹æ¡ˆå½¢æˆè¶…å¹³é¢çš„äº¤ç‚¹ã€‚</li>
</ul>
<h3 id="why-this-matters-7">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Linear systems are the practical foundation of linear algebra. They
appear in balancing chemical reactions, circuit analysis, least-squares
regression, optimization, and computer graphics. Understanding how to
represent and classify their solutions is the first step toward
systematic solution methods like Gaussian elimination.
çº¿æ€§ç³»ç»Ÿæ˜¯çº¿æ€§ä»£æ•°çš„å®è·µåŸºç¡€ã€‚å®ƒä»¬å‡ºç°åœ¨å¹³è¡¡åŒ–å­¦ååº”ã€ç”µè·¯åˆ†æã€æœ€å°äºŒä¹˜å›å½’ã€ä¼˜åŒ–å’Œè®¡ç®—æœºå›¾å½¢å­¦ä¸­ã€‚äº†è§£å¦‚ä½•è¡¨ç¤ºå’Œåˆ†ç±»å®ƒä»¬çš„è§£æ˜¯è¿ˆå‘é«˜æ–¯æ¶ˆå…ƒæ³•ç­‰ç³»ç»Ÿæ±‚è§£æ–¹æ³•çš„ç¬¬ä¸€æ­¥ã€‚</p>
<h3 id="exercises-3.1">Exercises 3.1</h3>
<p>ç»ƒä¹ 3.1</p>
<ol type="1">
<li>Write the following system in matrix form:
å°†ä»¥ä¸‹ç³»ç»Ÿå†™æˆçŸ©é˜µå½¢å¼ï¼š</li>
</ol>
<p><span class="math display">\[
\begin{cases}2x + 3y - z = 7, \\x - y + 4z = 1, \\3x + 2y + z =
5\end{cases}
\]</span></p>
<ol start="2" type="1">
<li>Determine whether the system ç¡®å®šç³»ç»Ÿæ˜¯å¦</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y = 1, \\2x + 2y = 2\end{cases}
\]</span></p>
<p>has no solution, one solution, or infinitely many solutions.
æœ‰æ— è§£ã€æœ‰ä¸€ä¸ªè§£æˆ–æœ‰æ— æ•°ä¸ªè§£ã€‚</p>
<ol start="3" type="1">
<li>Geometrically interpret the system å‡ ä½•è§£é‡Šç³»ç»Ÿ</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y = 3, \\x - y = 1\end{cases}
\]</span></p>
<p>in the plane. åœ¨é£æœºä¸Šã€‚</p>
<ol start="4" type="1">
<li>Solve the system è§£å†³ç³»ç»Ÿ</li>
</ol>
<p><span class="math display">\[
\begin{cases}2x + y = 1, \\x - y = 4\end{cases}
\]</span></p>
<p>and check your solution. å¹¶æ£€æŸ¥æ‚¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<ol start="5" type="1">
<li>In <span class="math inline">\(\mathbb{R}^3\)</span>, describe the
solution set of åœ¨ <span class="math inline">\(\mathbb{R}^3\)</span>
ä¸­ï¼Œæè¿°</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y + z = 0, \\2x + 2y + 2z = 0\end{cases}
\]</span></p>
<p>What geometric object does it represent? å®ƒä»£è¡¨ä»€ä¹ˆå‡ ä½•å¯¹è±¡ï¼Ÿ</p>
<h2 id="gaussian-elimination">3.2 Gaussian Elimination</h2>
<p>3.2 é«˜æ–¯æ¶ˆå…ƒæ³•</p>
<p>To solve linear systems efficiently, we use Gaussian elimination: a
systematic method of transforming a system into a simpler equivalent one
whose solutions are easier to see. The method relies on elementary row
operations that preserve the solution set.
ä¸ºäº†é«˜æ•ˆåœ°æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯æ¶ˆå…ƒæ³•ï¼šè¿™æ˜¯ä¸€ç§å°†æ–¹ç¨‹ç»„è½¬åŒ–ä¸ºæ›´ç®€å•ã€æ›´æ˜“è§£çš„ç­‰æ•ˆæ–¹ç¨‹çš„ç³»ç»Ÿæ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¾èµ–äºä¿ç•™è§£é›†çš„åŸºæœ¬è¡Œè¿ç®—ã€‚</p>
<h3 id="elementary-row-operations">Elementary Row Operations</h3>
<p>åˆç­‰è¡Œè¿ç®—</p>
<p>On an augmented matrix <span
class="math inline">\((A|\mathbf{b})\)</span>, we are allowed three
operations: å¯¹äºå¢å¹¿çŸ©é˜µ <span
class="math inline">\((A|\mathbf{b})\)</span>
ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸‰ç§è¿ç®—ï¼š</p>
<ol type="1">
<li>Row swapping: interchange two rows. æ¢è¡Œï¼šäº¤æ¢ä¸¤è¡Œã€‚</li>
<li>Row scaling: multiply a row by a nonzero scalar.
è¡Œç¼©æ”¾ï¼šå°†ä¸€è¡Œä¹˜ä»¥éé›¶æ ‡é‡ã€‚</li>
<li>Row replacement: replace one row by itself plus a multiple of
another row. è¡Œæ›¿æ¢ï¼šç”¨ä¸€è¡Œæœ¬èº«åŠ ä¸Šå¦ä¸€è¡Œçš„å€æ•°æ¥æ›¿æ¢ä¸€è¡Œã€‚</li>
</ol>
<p>These operations correspond to re-expressing equations in different
but equivalent forms. è¿™äº›è¿ç®—å¯¹åº”äºä»¥ä¸åŒä½†ç­‰æ•ˆçš„å½¢å¼é‡æ–°è¡¨è¾¾æ–¹ç¨‹ã€‚</p>
<h3 id="row-echelon-form">Row Echelon Form</h3>
<p>è¡Œæ¢¯é˜Ÿå½¢å¼</p>
<p>A matrix is in row echelon form (REF) if:
å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™çŸ©é˜µä¸ºè¡Œé˜¶æ¢¯å½¢çŸ©é˜µï¼ˆREFï¼‰ï¼š</p>
<ol type="1">
<li>All nonzero rows are above any zero rows.
æ‰€æœ‰éé›¶è¡Œå‡ä½äºä»»ä½•é›¶è¡Œä¹‹ä¸Šã€‚</li>
<li>Each leading entry (the first nonzero number from the left in a row)
is to the right of the leading entry in the row above.
æ¯ä¸ªå‰å¯¼æ¡ç›®ï¼ˆä¸€è¡Œä¸­ä»å·¦è¾¹å¼€å§‹çš„ç¬¬ä¸€ä¸ªéé›¶æ•°å­—ï¼‰ä½äºä¸Šä¸€è¡Œå‰å¯¼æ¡ç›®çš„å³ä¾§ã€‚</li>
<li>All entries below a leading entry are zero.
å‰å¯¼æ¡ç›®ä¸‹é¢çš„æ‰€æœ‰æ¡ç›®éƒ½ä¸ºé›¶ã€‚</li>
</ol>
<p>Further, if each leading entry is 1 and is the only nonzero entry in
its column, the matrix is in reduced row echelon form (RREF).
æ­¤å¤–ï¼Œå¦‚æœæ¯ä¸ªå‰å¯¼é¡¹éƒ½æ˜¯
1ï¼Œå¹¶ä¸”æ˜¯å…¶åˆ—ä¸­å”¯ä¸€çš„éé›¶é¡¹ï¼Œåˆ™çŸ©é˜µä¸ºç®€åŒ–è¡Œé˜¶æ¢¯å½¢å¼ (RREF)ã€‚</p>
<h3 id="algorithm-of-gaussian-elimination">Algorithm of Gaussian
Elimination</h3>
<p>é«˜æ–¯æ¶ˆå…ƒæ³•</p>
<ol type="1">
<li>Write the augmented matrix for the system. å†™å‡ºç³»ç»Ÿçš„å¢å¹¿çŸ©é˜µã€‚</li>
<li>Use row operations to create zeros below each pivot (the leading
entry in a row).
ä½¿ç”¨è¡Œè¿ç®—åœ¨æ¯ä¸ªæ¢è½´ï¼ˆä¸€è¡Œä¸­çš„å‰å¯¼æ¡ç›®ï¼‰ä¸‹æ–¹åˆ›å»ºé›¶ã€‚</li>
<li>Continue column by column until the matrix is in echelon form.
ç»§ç»­é€åˆ—è¿›è¡Œï¼Œç›´åˆ°çŸ©é˜µå‘ˆé˜¶æ¢¯å½¢å¼ã€‚</li>
<li>Solve by back substitution: starting from the last pivot equation
and working upward.
é€šè¿‡åå‘ä»£å…¥æ¥æ±‚è§£ï¼šä»æœ€åä¸€ä¸ªæ¢è½´æ–¹ç¨‹å¼€å§‹å‘ä¸Šæ±‚è§£ã€‚</li>
</ol>
<p>If we continue to RREF, the solution can be read off directly.
å¦‚æœæˆ‘ä»¬ç»§ç»­ RREFï¼Œåˆ™å¯ä»¥ç›´æ¥è¯»å‡ºè§£å†³æ–¹æ¡ˆã€‚</p>
<h3 id="example">Example</h3>
<p>ä¾‹å­</p>
<p>Example 3.2.1. Solve ä¾‹ 3.2.1. æ±‚è§£</p>
<p><span class="math display">\[
\begin{cases}x + 2y - z = 3, \\2x + y + z = 7, \\3x - y + 2z =
4.\end{cases}
\]</span></p>
<p>Step 1. Augmented matrix æ­¥éª¤1.å¢å¹¿çŸ©é˜µ</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 2 &amp; -1 &amp; 3 \\2 &amp; 1 &amp; 1
&amp; 7 \\3 &amp; -1 &amp; 2 &amp; 4\end{array}\right].
\]</span></p>
<p>Step 2. Eliminate below the first pivot æ­¥éª¤ 2.
æ¶ˆé™¤ç¬¬ä¸€ä¸ªæ¢è½´ä»¥ä¸‹</p>
<p>Subtract 2 times row 1 from row 2, and 3 times row 1 from row 3: ä»ç¬¬
2 è¡Œå‡å»ç¬¬ 1 è¡Œçš„ 2 å€ï¼Œä»ç¬¬ 3 è¡Œå‡å»ç¬¬ 1 è¡Œçš„ 3 å€ï¼š</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 2 &amp; -1 &amp; 3 \\0 &amp; -3 &amp;
3 &amp; 1 \\0 &amp; -7 &amp; 5 &amp; -5\end{array}\right].
\]</span></p>
<p>Step 3. Pivot in column 2 æ­¥éª¤ 3. åœ¨ç¬¬ 2 åˆ—ä¸­è¿›è¡Œé€è§†</p>
<p>Divide row 2 by -3: å°†ç¬¬ 2 è¡Œé™¤ä»¥ -3ï¼š</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 2 &amp; -1 &amp; 3 \\0 &amp; 1 &amp;
-1 &amp; -\tfrac{1}{3} \\0 &amp; -7 &amp; 5 &amp; -5\end{array}\right].
\]</span></p>
<p>Add 7 times row 2 to row 3: å°†ç¬¬ 2 è¡Œçš„ 7 å€åŠ åˆ°ç¬¬ 3 è¡Œï¼š</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 2 &amp; -1 &amp; 3 \\0 &amp; 1 &amp;
-1 &amp; -\tfrac{1}{3} \\0 &amp; 0 &amp; -2 &amp;
-\tfrac{22}{3}\end{array}\right].
\]</span></p>
<p>Step 4. Pivot in column 3 æ­¥éª¤ 4. åœ¨ç¬¬ 3 åˆ—ä¸­è¿›è¡Œé€è§†</p>
<p>Divide row 3 by -2: å°†ç¬¬ 3 è¡Œé™¤ä»¥ -2ï¼š</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 2 &amp; -1 &amp; 3 \\0 &amp; 1 &amp;
-1 &amp; -\tfrac{1}{3} \\0 &amp; 0 &amp; 1 &amp;
\tfrac{11}{3}\end{array}\right].
\]</span></p>
<p>Step 5. Back substitution æ­¥éª¤ 5. å›ä»£</p>
<p>From the last row: ä»æœ€åä¸€è¡Œå¼€å§‹ï¼š</p>
<p><span class="math display">\[
z = \tfrac{11}{3}.
\]</span></p>
<p>Second row: ç¬¬äºŒè¡Œï¼š</p>
<p><span class="math display">\[
y - z = -\tfrac{1}{3} \implies y = -\tfrac{1}{3} + \tfrac{11}{3} =
\tfrac{10}{3}.
\]</span></p>
<p>First row: ç¬¬ä¸€è¡Œï¼š</p>
<p><span class="math display">\[
x + 2y - z = 3 \implies x + 2\cdot\tfrac{10}{3} - \tfrac{11}{3} = 3.
\]</span></p>
<p>So æ‰€ä»¥</p>
<p><span class="math display">\[
x + \tfrac{20}{3} - \tfrac{11}{3} = 3 \implies x + 3 = 3 \implies x = 0.
\]</span></p>
<p>Solution: è§£å†³æ–¹æ¡ˆï¼š</p>
<p><span class="math display">\[
(x,y,z) = \big(0, \tfrac{10}{3}, \tfrac{11}{3}\big).
\]</span></p>
<h3 id="why-this-matters-8">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Gaussian elimination is the foundation of computational linear
algebra. It reduces complex systems to a form where solutions are
visible, and it forms the basis for algorithms used in numerical
analysis, scientific computing, and machine learning.
é«˜æ–¯æ¶ˆå…ƒæ³•æ˜¯è®¡ç®—çº¿æ€§ä»£æ•°çš„åŸºç¡€ã€‚å®ƒå°†å¤æ‚ç³»ç»Ÿç®€åŒ–ä¸ºå¯è§è§£çš„å½¢å¼ï¼Œå¹¶æ„æˆæ•°å€¼åˆ†æã€ç§‘å­¦è®¡ç®—å’Œæœºå™¨å­¦ä¹ ä¸­ä½¿ç”¨çš„ç®—æ³•çš„åŸºç¡€ã€‚</p>
<h3 id="exercises-3.2">Exercises 3.2</h3>
<p>ç»ƒä¹  3.2</p>
<ol type="1">
<li>Solve by Gaussian elimination: é€šè¿‡é«˜æ–¯æ¶ˆå…ƒæ³•æ±‚è§£ï¼š</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y = 2, \\2x - y = 0.\end{cases}
\]</span></p>
<ol start="2" type="1">
<li>Reduce the following augmented matrix to REF: å°†ä»¥ä¸‹å¢å¹¿çŸ©é˜µç®€åŒ–ä¸º
REFï¼š</li>
</ol>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 1 &amp; 1 &amp; 6 \\2 &amp; -1 &amp; 3
&amp; 14 \\1 &amp; 4 &amp; -2 &amp; -2\end{array}\right].
\]</span></p>
<ol start="3" type="1">
<li>Show that Gaussian elimination always produces either:
è¯æ˜é«˜æ–¯æ¶ˆå…ƒæ³•æ€»æ˜¯äº§ç”Ÿä»¥ä¸‹ç»“æœï¼š</li>
</ol>
<ul>
<li>a unique solution, ä¸€ä¸ªç‹¬ç‰¹çš„è§£å†³æ–¹æ¡ˆï¼Œ</li>
<li>infinitely many solutions, or æ— ç©·å¤šä¸ªè§£ï¼Œæˆ–è€…</li>
<li>a contradiction (no solution). çŸ›ç›¾ï¼ˆæ— è§£ï¼‰ã€‚</li>
</ul>
<ol start="4" type="1">
<li>Use Gaussian elimination to find all solutions of
ä½¿ç”¨é«˜æ–¯æ¶ˆå…ƒæ³•æ‰¾åˆ°æ‰€æœ‰è§£</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y + z = 0, \\2x + y + z = 1.\end{cases}
\]</span></p>
<ol start="5" type="1">
<li>Explain why pivoting (choosing the largest available pivot element)
is useful in numerical computation.
è§£é‡Šä¸ºä»€ä¹ˆæ¢è½´æ—‹è½¬ï¼ˆé€‰æ‹©æœ€å¤§çš„å¯ç”¨æ¢è½´å…ƒç´ ï¼‰åœ¨æ•°å€¼è®¡ç®—ä¸­å¾ˆæœ‰ç”¨ã€‚</li>
</ol>
<h2 id="rank-and-consistency">3.3 Rank and Consistency</h2>
<p>3.3 ç­‰çº§å’Œä¸€è‡´æ€§</p>
<p>Gaussian elimination not only provides solutions but also reveals the
structure of a linear system. Two key ideas are the rank of a matrix and
the consistency of a system. Rank measures the amount of independent
information in the equations, while consistency determines whether the
system has at least one solution.
é«˜æ–¯æ¶ˆå…ƒæ³•ä¸ä»…èƒ½æä¾›è§£ï¼Œè¿˜èƒ½æ­ç¤ºçº¿æ€§ç³»ç»Ÿçš„ç»“æ„ã€‚ä¸¤ä¸ªå…³é”®æ¦‚å¿µæ˜¯çŸ©é˜µçš„ç§©å’Œç³»ç»Ÿçš„ä¸€è‡´æ€§ã€‚ç§©è¡¡é‡æ–¹ç¨‹ä¸­ç‹¬ç«‹ä¿¡æ¯çš„æ•°é‡ï¼Œè€Œä¸€è‡´æ€§åˆ™å†³å®šç³»ç»Ÿæ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªè§£ã€‚</p>
<h3 id="rank-of-a-matrix">Rank of a Matrix</h3>
<p>çŸ©é˜µçš„ç§©</p>
<p>The rank of a matrix is the number of leading pivots in its row
echelon form. Equivalently, it is the maximum number of linearly
independent rows or columns.
çŸ©é˜µçš„ç§©æ˜¯å…¶è¡Œé˜¶æ¢¯å½¢ä¸­å‰å¯¼ä¸»å…ƒçš„ä¸ªæ•°ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯çº¿æ€§æ— å…³çš„è¡Œæˆ–åˆ—çš„æœ€å¤§æ•°é‡ã€‚</p>
<p>Formally, æ­£å¼åœ°ï¼Œ</p>
<p><span class="math display">\[
\text{rank}(A) = \dim(\text{row space of } A) = \dim(\text{column space
of } A).
\]</span></p>
<p>The rank tells us the effective dimension of the space spanned by the
rows (or columns). ç§©å‘Šè¯‰æˆ‘ä»¬è¡Œï¼ˆæˆ–åˆ—ï¼‰æ‰€è·¨è¶Šçš„ç©ºé—´çš„æœ‰æ•ˆç»´åº¦ã€‚</p>
<p>Example 3.3.1. For ä¾‹ 3.3.1. å¯¹äº</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 &amp; 3 \\2 &amp; 4 &amp; 6 \\3 &amp; 6
&amp; 9\end{bmatrix},
\]</span></p>
<p>row reduction gives è¡Œå‡å°‘ç»™å‡º</p>
<p><span class="math display">\[
\begin{bmatrix}1 &amp; 2 &amp; 3 \\0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp;
0\end{bmatrix}.
\]</span></p>
<p>Thus, <span class="math inline">\(\text{rank}(A) = 1\)</span>, since
all rows are multiples of the first. å› æ­¤ï¼Œ <span
class="math inline">\(\text{rank}(A) = 1\)</span>
ï¼Œå› ä¸ºæ‰€æœ‰è¡Œéƒ½æ˜¯ç¬¬ä¸€è¡Œçš„å€æ•°ã€‚</p>
<h3 id="consistency-of-linear-systems">Consistency of Linear
Systems</h3>
<p>çº¿æ€§ç³»ç»Ÿçš„ä¸€è‡´æ€§</p>
<p>Consider the system <span class="math inline">\(A\mathbf{x} =
\mathbf{b}\)</span>. The system is consistent (has at least one
solution) if and only if è€ƒè™‘ç³»ç»Ÿ <span
class="math inline">\(A\mathbf{x} = \mathbf{b}\)</span>
ã€‚è¯¥ç³»ç»Ÿæ˜¯ä¸€è‡´çš„ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªè§£ï¼‰ï¼Œå½“ä¸”ä»…å½“</p>
<p><span class="math display">\[
\text{rank}(A) = \text{rank}(A|\mathbf{b}),
\]</span></p>
<p>where <span class="math inline">\((A|\mathbf{b})\)</span> is the
augmented matrix. If the ranks differ, the system is inconsistent. å…¶ä¸­
<span class="math inline">\((A|\mathbf{b})\)</span>
æ˜¯å¢å¹¿çŸ©é˜µã€‚å¦‚æœç§©ä¸åŒï¼Œåˆ™ç³»ç»Ÿä¸ä¸€è‡´ã€‚</p>
<ul>
<li>If <span class="math inline">\(\text{rank}(A) =
\text{rank}(A|\mathbf{b}) = n\)</span> (number of unknowns), the system
has a unique solution. å¦‚æœ <span class="math inline">\(\text{rank}(A) =
\text{rank}(A|\mathbf{b}) = n\)</span>
ï¼ˆæœªçŸ¥æ•°ï¼‰ï¼Œåˆ™ç³»ç»Ÿæœ‰ä¸€ä¸ªå”¯ä¸€çš„è§£ã€‚</li>
<li>If <span class="math inline">\(\text{rank}(A) =
\text{rank}(A|\mathbf{b}) &lt; n\)</span>, the system has infinitely
many solutions. å¦‚æœ <span class="math inline">\(\text{rank}(A) =
\text{rank}(A|\mathbf{b}) &lt; n\)</span> ï¼Œåˆ™ç³»ç»Ÿæœ‰æ— æ•°ä¸ªè§£ã€‚</li>
</ul>
<h3 id="example-1">Example</h3>
<p>ä¾‹å­</p>
<p>Example 3.3.2. Consider ä¾‹ 3.3.2. è€ƒè™‘</p>
<p><span class="math display">\[
\begin{cases}x + y + z = 1, \\2x + 2y + 2z = 2, \\x + y + z =
3.\end{cases}
\]</span></p>
<p>The augmented matrix is å¢å¹¿çŸ©é˜µæ˜¯</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 1 &amp; 1 &amp; 1 \\2 &amp; 2 &amp; 2
&amp; 2 \\1 &amp; 1 &amp; 1 &amp; 3\end{array}\right].
\]</span></p>
<p>Row reduction gives è¡Œå‡å°‘ç»™å‡º</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 1 &amp; 1 &amp; 1 \\0 &amp; 0 &amp; 0
&amp; 0 \\0 &amp; 0 &amp; 0 &amp; 2\end{array}\right].
\]</span></p>
<p>Here, <span class="math inline">\(\text{rank}(A) = 1\)</span>, but
<span class="math inline">\(\text{rank}(A|\mathbf{b}) = 2\)</span>.
Since the ranks differ, the system is inconsistent: no solution exists.
è¿™é‡Œï¼Œ <span class="math inline">\(\text{rank}(A) = 1\)</span> ï¼Œä½†
<span class="math inline">\(\text{rank}(A|\mathbf{b}) = 2\)</span>
ã€‚ç”±äºç§©ä¸åŒï¼Œç³»ç»Ÿä¸ä¸€è‡´ï¼šä¸å­˜åœ¨è§£ã€‚</p>
<h3 id="example-with-infinite-solutions">Example with Infinite
Solutions</h3>
<p>æ— é™è§£çš„ä¾‹å­</p>
<p>Example 3.3.3. For ä¾‹ 3.3.3. å¯¹äº</p>
<p><span class="math display">\[
\begin{cases}x + y = 2, \\2x + 2y = 4,\end{cases}
\]</span></p>
<p>the augmented matrix reduces to å¢å¹¿çŸ©é˜µç®€åŒ–ä¸º</p>
<p><span class="math display">\[
\left[\begin{array}{cc|c}1 &amp; 1 &amp; 2 \\0 &amp; 0 &amp;
0\end{array}\right].
\]</span></p>
<p>Here, <span class="math inline">\(\text{rank}(A) =
\text{rank}(A|\mathbf{b}) = 1 &lt; 2\)</span>. Thus, infinitely many
solutions exist, forming a line. è¿™é‡Œï¼Œ <span
class="math inline">\(\text{rank}(A) = \text{rank}(A|\mathbf{b}) = 1
&lt; 2\)</span> ã€‚å› æ­¤ï¼Œå­˜åœ¨æ— æ•°ä¸ªè§£ï¼Œå½¢æˆä¸€æ¡çº¿ã€‚</p>
<h3 id="why-this-matters-9">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Rank is a measure of independence: it tells us how many truly
distinct equations or directions are present. Consistency explains when
equations align versus when they contradict. These concepts connect
linear systems to vector spaces and prepare for the ideas of dimension,
basis, and the Rankâ€“Nullity Theorem.
ç§©æ˜¯ç‹¬ç«‹æ€§çš„åº¦é‡ï¼šå®ƒå‘Šè¯‰æˆ‘ä»¬æœ‰å¤šå°‘ä¸ªçœŸæ­£ä¸åŒçš„æ–¹ç¨‹æˆ–æ–¹å‘ã€‚ä¸€è‡´æ€§è§£é‡Šäº†æ–¹ç¨‹ä½•æ—¶ä¸€è‡´ï¼Œä½•æ—¶çŸ›ç›¾ã€‚è¿™äº›æ¦‚å¿µå°†çº¿æ€§ç³»ç»Ÿä¸å‘é‡ç©ºé—´è”ç³»èµ·æ¥ï¼Œå¹¶ä¸ºç»´åº¦ã€åŸºå’Œç§©é›¶å®šç†çš„æ¦‚å¿µåšå¥½å‡†å¤‡ã€‚</p>
<h3 id="exercises-3.3">Exercises 3.3</h3>
<p>ç»ƒä¹  3.3</p>
<ol type="1">
<li>Compute the rank of è®¡ç®—</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 &amp; 1 \\0 &amp; 1 &amp; -1 \\2 &amp; 5
&amp; -1\end{bmatrix}.
\]</span></p>
<ol start="2" type="1">
<li>Determine whether the system ç¡®å®šç³»ç»Ÿ</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y + z = 1, \\2x + 3y + z = 2, \\3x + 5y + 2z =
3\end{cases}
\]</span></p>
<p>is consistent. æ˜¯ä¸€è‡´çš„ã€‚</p>
<ol start="3" type="1">
<li><p>Show that the rank of the identity matrix <span
class="math inline">\(I_n\)</span> is <span
class="math inline">\(n\)</span>. è¯æ˜å•ä½çŸ©é˜µğ¼çš„ç§© ğ‘› I n â€‹ æ˜¯ <span
class="math inline">\(n\)</span> ã€‚</p></li>
<li><p>Give an example of a system in <span
class="math inline">\(\mathbb{R}^3\)</span> with infinitely many
solutions, and explain why it satisfies the rank condition. ç»™å‡º <span
class="math inline">\(\mathbb{R}^3\)</span>
ä¸­å…·æœ‰æ— ç©·å¤šä¸ªè§£çš„ç³»ç»Ÿçš„ä¾‹å­ï¼Œå¹¶è§£é‡Šå®ƒä¸ºä»€ä¹ˆæ»¡è¶³ç§©æ¡ä»¶ã€‚</p></li>
<li><p>Prove that for any matrix <span class="math inline">\(A \in
\mathbb{R}^{m \times n}\)</span>, <span
class="math inline">\(\text{rank}(A) \leq \min(m,n).\)</span>
è¯æ˜å¯¹äºä»»æ„çŸ©é˜µ <span class="math inline">\(A \in \mathbb{R}^{m \times
n}\)</span> ï¼Œ <span class="math inline">\(\text{rank}(A) \leq
\min(m,n).\)</span></p></li>
</ol>
<h2 id="homogeneous-systems">3.4 Homogeneous Systems</h2>
<p>3.4 å‡è´¨ç³»ç»Ÿ</p>
<p>A homogeneous system is a linear system in which all constant terms
are zero: é½æ¬¡ç³»ç»Ÿæ˜¯æ‰€æœ‰å¸¸æ•°é¡¹éƒ½ä¸ºé›¶çš„çº¿æ€§ç³»ç»Ÿï¼š</p>
<p><span class="math display">\[
A\mathbf{x} = \mathbf{0},
\]</span></p>
<p>where <span class="math inline">\(A \in \mathbb{R}^{m \times
n}\)</span>, and <span class="math inline">\(\mathbf{0}\)</span> is the
zero vector in <span class="math inline">\(\mathbb{R}^m\)</span>. å…¶ä¸­
<span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> ï¼Œä¸”
<span class="math inline">\(\mathbf{0}\)</span> æ˜¯ <span
class="math inline">\(\mathbb{R}^m\)</span> ä¸­çš„é›¶å‘é‡ã€‚</p>
<h3 id="the-trivial-solution">The Trivial Solution</h3>
<p>ç®€å•çš„è§£å†³æ–¹æ¡ˆ</p>
<p>Every homogeneous system has at least one solution:
æ¯ä¸ªåŒè´¨ç³»ç»Ÿè‡³å°‘æœ‰ä¸€ä¸ªè§£ï¼š</p>
<p><span class="math display">\[
\mathbf{x} = \mathbf{0}.
\]</span></p>
<p>This is called the trivial solution. The interesting question is
whether <em>nontrivial solutions</em> (nonzero vectors) exist.
è¿™è¢«ç§°ä¸ºå¹³å‡¡è§£ã€‚æœ‰è¶£çš„é—®é¢˜æ˜¯æ˜¯å¦å­˜åœ¨<em>éå¹³å‡¡è§£</em> ï¼ˆéé›¶å‘é‡ï¼‰ã€‚</p>
<h3 id="existence-of-nontrivial-solutions">Existence of Nontrivial
Solutions</h3>
<p>éå¹³å‡¡è§£çš„å­˜åœ¨æ€§</p>
<p>Nontrivial solutions exist precisely when the number of unknowns
exceeds the rank of the coefficient matrix:
å½“æœªçŸ¥æ•°çš„æ•°é‡è¶…è¿‡ç³»æ•°çŸ©é˜µçš„ç§©æ—¶ï¼Œå°±ä¼šå­˜åœ¨éå¹³å‡¡è§£ï¼š</p>
<p><span class="math display">\[
\text{rank}(A) &lt; n.
\]</span></p>
<p>In this case, there are infinitely many solutions, forming a subspace
of <span class="math inline">\(\mathbb{R}^n\)</span>. The dimension of
this solution space is åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ‰æ— ç©·å¤šä¸ªè§£ï¼Œå½¢æˆä¸€ä¸ª <span
class="math inline">\(\mathbb{R}^n\)</span>
çš„å­ç©ºé—´ã€‚è¿™ä¸ªè§£ç©ºé—´çš„ç»´åº¦æ˜¯</p>
<p><span class="math display">\[
\dim(\text{null}(A)) = n - \text{rank}(A),
\]</span></p>
<p>where null(A) is the set of all solutions to <span
class="math inline">\(A\mathbf{x} = 0\)</span>. This set is called the
null space or kernel of <span class="math inline">\(A\)</span>. å…¶ä¸­
null(A) æ˜¯ <span class="math inline">\(A\mathbf{x} = 0\)</span>
æ‰€æœ‰è§£çš„é›†åˆã€‚è¯¥é›†åˆç§°ä¸º <span class="math inline">\(A\)</span>
çš„é›¶ç©ºé—´æˆ–é›¶æ ¸ã€‚</p>
<h3 id="example-2">Example</h3>
<p>ä¾‹å­</p>
<p>Example 3.4.1. Consider ä¾‹ 3.4.1. è€ƒè™‘</p>
<p><span class="math display">\[
\begin{cases}x + y + z = 0, \\2x + y - z = 0.\end{cases}
\]</span></p>
<p>The augmented matrix is å¢å¹¿çŸ©é˜µæ˜¯</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 1 &amp; 1 &amp; 0 \\2 &amp; 1 &amp; -1
&amp; 0\end{array}\right].
\]</span></p>
<p>Row reduction: è¡Œå‡å°‘ï¼š</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 1 &amp; 1 &amp; 0 \\0 &amp; -1 &amp;
-3 &amp; 0\end{array}\right]\quad\to\quad\left[\begin{array}{ccc|c}1
&amp; 1 &amp; 1 &amp; 0 \\0 &amp; 1 &amp; 3 &amp; 0\end{array}\right].
\]</span></p>
<p>So the system is equivalent to: å› æ­¤è¯¥ç³»ç»Ÿç­‰åŒäºï¼š</p>
<p><span class="math display">\[
\begin{cases}x + y + z = 0, \\y + 3z = 0.\end{cases}
\]</span></p>
<p>From the second equation, <span class="math inline">\(y =
-3z\)</span>. Substituting into the first: <span class="math inline">\(x
- 3z + z = 0 \implies x = 2z.\)</span> ä»ç¬¬äºŒä¸ªæ–¹ç¨‹å¾—å‡º <span
class="math inline">\(y = -3z\)</span> ã€‚ä»£å…¥ç¬¬ä¸€ä¸ªæ–¹ç¨‹ï¼š <span
class="math inline">\(x - 3z + z = 0 \implies x = 2z.\)</span></p>
<p>Thus solutions are: å› æ­¤è§£å†³æ–¹æ¡ˆæ˜¯ï¼š</p>
<p><span class="math display">\[
(x,y,z) = z(2, -3, 1), \quad z \in \mathbb{R}.
\]</span></p>
<p>The null space is the line spanned by the vector <span
class="math inline">\((2, -3, 1)\)</span>. é›¶ç©ºé—´æ˜¯å‘é‡ <span
class="math inline">\((2, -3, 1)\)</span> æ‰€è·¨è¶Šçš„çº¿ã€‚</p>
<h3 id="geometric-interpretation-2">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>The solution set of a homogeneous system is always a subspace of
<span class="math inline">\(\mathbb{R}^n\)</span>. åŒè´¨ç³»ç»Ÿçš„è§£é›†å§‹ç»ˆæ˜¯
<span class="math inline">\(\mathbb{R}^n\)</span> çš„å­ç©ºé—´ã€‚</p>
<ul>
<li>If <span class="math inline">\(\text{rank}(A) = n\)</span>, the only
solution is the zero vector. å¦‚æœä¸º <span
class="math inline">\(\text{rank}(A) = n\)</span>
ï¼Œåˆ™å”¯ä¸€çš„è§£å°±æ˜¯é›¶å‘é‡ã€‚</li>
<li>If <span class="math inline">\(\text{rank}(A) = n-1\)</span>, the
solution set is a line through the origin. å¦‚æœä¸º <span
class="math inline">\(\text{rank}(A) = n-1\)</span>
ï¼Œåˆ™è§£é›†æ˜¯ä¸€æ¡è¿‡åŸç‚¹çš„çº¿ã€‚</li>
<li>If <span class="math inline">\(\text{rank}(A) = n-2\)</span>, the
solution set is a plane through the origin. å¦‚æœä¸º <span
class="math inline">\(\text{rank}(A) = n-2\)</span>
ï¼Œåˆ™è§£é›†æ˜¯é€šè¿‡åŸç‚¹çš„å¹³é¢ã€‚</li>
</ul>
<p>More generally, the null space has dimension <span
class="math inline">\(n - \text{rank}(A)\)</span>, known as the nullity.
æ›´ä¸€èˆ¬åœ°ï¼Œé›¶ç©ºé—´çš„ç»´åº¦ä¸º <span class="math inline">\(n -
\text{rank}(A)\)</span> ï¼Œç§°ä¸ºé›¶åº¦ã€‚</p>
<h3 id="why-this-matters-10">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Homogeneous systems are central to understanding vector spaces,
subspaces, and dimension. They lead directly to the concepts of kernel,
null space, and linear dependence. In applications, homogeneous systems
appear in equilibrium problems, eigenvalue equations, and computer
graphics transformations.
é½æ¬¡ç³»ç»Ÿæ˜¯ç†è§£å‘é‡ç©ºé—´ã€å­ç©ºé—´å’Œç»´åº¦çš„æ ¸å¿ƒã€‚å®ƒä»¬ç›´æ¥å¼•å‡ºæ ¸ã€é›¶ç©ºé—´å’Œçº¿æ€§ç›¸å…³æ€§çš„æ¦‚å¿µã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé½æ¬¡ç³»ç»Ÿå‡ºç°åœ¨å¹³è¡¡é—®é¢˜ã€ç‰¹å¾å€¼æ–¹ç¨‹å’Œè®¡ç®—æœºå›¾å½¢å˜æ¢ä¸­ã€‚</p>
<h3 id="exercises-3.4">Exercises 3.4</h3>
<p>ç»ƒä¹  3.4</p>
<ol type="1">
<li>Solve the homogeneous system è§£å†³å‡è´¨ç³»ç»Ÿ</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + 2y - z = 0, \\2x + 4y - 2z = 0.\end{cases}
\]</span></p>
<p>What is the dimension of its solution space?
å…¶è§£ç©ºé—´çš„ç»´æ•°æ˜¯å¤šå°‘ï¼Ÿ</p>
<ol start="2" type="1">
<li>Find all solutions of æ‰¾åˆ°æ‰€æœ‰è§£å†³æ–¹æ¡ˆ</li>
</ol>
<p><span class="math display">\[
\begin{cases}x - y + z = 0, \\2x + y - z = 0.\end{cases}
\]</span></p>
<ol start="3" type="1">
<li><p>Show that the solution set of any homogeneous system is a
subspace of <span class="math inline">\(\mathbb{R}^n\)</span>.
è¯æ˜ä»»ä½•åŒè´¨ç³»ç»Ÿçš„è§£é›†éƒ½æ˜¯ <span
class="math inline">\(\mathbb{R}^n\)</span> çš„å­ç©ºé—´ã€‚</p></li>
<li><p>Suppose <span class="math inline">\(A\)</span> is a <span
class="math inline">\(3 \\times 3\)</span>matrix with<span
class="math inline">\(\\text{rank}(A) = 2\)</span>. What is the
dimension of the null space of <span class="math inline">\(A\)</span>?
å‡è®¾ <span class="math inline">\(A\)</span> æ˜¯ $3 \times 3 <span
class="math inline">\(matrix with\)</span> \text{rank}(A) = 2 $. What is
the dimension of the null space of $ A$ï¼Ÿ</p></li>
<li><p>For ä¸ºäº†</p></li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 2 &amp; -1 \\ 0 &amp; 1 &amp; 3
\end{bmatrix},
\]</span></p>
<p>compute a basis for the null space of <span
class="math inline">\(A\)</span>. è®¡ç®— <span
class="math inline">\(A\)</span> çš„é›¶ç©ºé—´çš„åŸºç¡€ã€‚</p>
<h1 id="chapter-4.-vector-spaces">Chapter 4. Vector Spaces</h1>
<p>ç¬¬ 4 ç«  å‘é‡ç©ºé—´</p>
<h2 id="definition-of-a-vector-space">4.1 Definition of a Vector
Space</h2>
<p>4.1 å‘é‡ç©ºé—´çš„å®šä¹‰</p>
<p>Up to now we have studied vectors and matrices concretely in <span
class="math inline">\(\mathbb{R}^n\)</span>. The next step is to move
beyond coordinates and define vector spaces in full generality. A vector
space is an abstract setting where the familiar rules of addition and
scalar multiplication hold, regardless of whether the elements are
geometric vectors, polynomials, functions, or other objects.
åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»åœ¨ <span class="math inline">\(\mathbb{R}^n\)</span>
ä¸­å…·ä½“å­¦ä¹ äº†å‘é‡å’ŒçŸ©é˜µã€‚ä¸‹ä¸€æ­¥æ˜¯è¶…è¶Šåæ ‡ï¼Œå…¨é¢å®šä¹‰å‘é‡ç©ºé—´ã€‚å‘é‡ç©ºé—´æ˜¯ä¸€ä¸ªæŠ½è±¡çš„åœºæ™¯ï¼Œå…¶ä¸­ç†Ÿæ‚‰çš„åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•è§„åˆ™å§‹ç»ˆæˆç«‹ï¼Œæ— è®ºå…ƒç´ æ˜¯å‡ ä½•å‘é‡ã€å¤šé¡¹å¼ã€å‡½æ•°è¿˜æ˜¯å…¶ä»–å¯¹è±¡ã€‚</p>
<h3 id="formal-definition-1">Formal Definition</h3>
<p>æ­£å¼å®šä¹‰</p>
<p>A vector space over the real numbers <span
class="math inline">\(\mathbb{R}\)</span> is a set <span
class="math inline">\(V\)</span> equipped with two operations: å®æ•°
<span class="math inline">\(\mathbb{R}\)</span>
ä¸Šçš„å‘é‡ç©ºé—´æ˜¯å…·æœ‰ä¸¤ä¸ªè¿ç®—çš„é›†åˆ <span class="math inline">\(V\)</span>
ï¼š</p>
<ol type="1">
<li>Vector addition: For any <span class="math inline">\(\mathbf{u},
\mathbf{v} \in V\)</span>, there is a vector <span
class="math inline">\(\mathbf{u} + \mathbf{v} \in V\)</span>.
å‘é‡åŠ æ³•ï¼šå¯¹äºä»»ä½• <span class="math inline">\(\mathbf{u}, \mathbf{v}
\in V\)</span> ï¼Œéƒ½æœ‰å‘é‡ <span class="math inline">\(\mathbf{u} +
\mathbf{v} \in V\)</span> ã€‚</li>
<li>Scalar multiplication: For any scalar <span class="math inline">\(c
\in \mathbb{R}\)</span> and any <span class="math inline">\(\mathbf{v}
\in V\)</span>, there is a vector <span
class="math inline">\(c\mathbf{v} \in V\)</span>. æ ‡é‡ä¹˜æ³•ï¼šå¯¹äºä»»ä½•æ ‡é‡
<span class="math inline">\(c \in \mathbb{R}\)</span> å’Œä»»ä½• <span
class="math inline">\(\mathbf{v} \in V\)</span> ï¼Œéƒ½æœ‰ä¸€ä¸ªå‘é‡ <span
class="math inline">\(c\mathbf{v} \in V\)</span> ã€‚</li>
</ol>
<p>These operations must satisfy the following axioms (for all <span
class="math inline">\(\mathbf{u}, \mathbf{v}, \mathbf{w} \in V\)</span>
and all scalars <span class="math inline">\(a,b \in
\mathbb{R}\)</span>): è¿™äº›è¿ç®—å¿…é¡»æ»¡è¶³ä»¥ä¸‹å…¬ç†ï¼ˆå¯¹äºæ‰€æœ‰ <span
class="math inline">\(\mathbf{u}, \mathbf{v}, \mathbf{w} \in V\)</span>
å’Œæ‰€æœ‰æ ‡é‡ <span class="math inline">\(a,b \in \mathbb{R}\)</span>
ï¼‰ï¼š</p>
<ol type="1">
<li>Commutativity of addition: <span class="math inline">\(\mathbf{u} +
\mathbf{v} = \mathbf{v} + \mathbf{u}\)</span>. åŠ æ³•çš„äº¤æ¢æ€§ï¼š <span
class="math inline">\(\mathbf{u} + \mathbf{v} = \mathbf{v} +
\mathbf{u}\)</span> ã€‚</li>
<li>Associativity of addition: <span class="math inline">\((\mathbf{u} +
\mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} +
\mathbf{w})\)</span>. åŠ æ³•çš„ç»“åˆæ€§ï¼š <span
class="math inline">\((\mathbf{u} + \mathbf{v}) + \mathbf{w} =
\mathbf{u} + (\mathbf{v} + \mathbf{w})\)</span> ã€‚</li>
<li>Additive identity: There exists a zero vector <span
class="math inline">\(\mathbf{0} \in V\)</span> such that <span
class="math inline">\(\mathbf{v} + \mathbf{0} = \mathbf{v}\)</span>.
åŠ æ³•æ’ç­‰å¼ï¼šå­˜åœ¨é›¶å‘é‡ <span class="math inline">\(\mathbf{0} \in
V\)</span> ä½¿å¾— <span class="math inline">\(\mathbf{v} + \mathbf{0} =
\mathbf{v}\)</span> ã€‚</li>
<li>Additive inverses: For each <span class="math inline">\(\mathbf{v}
\in V\)</span>, there exists <span class="math inline">\((-\mathbf{v}
\in V\)</span> such that <span class="math inline">\(\mathbf{v} +
(-\mathbf{v}) = \mathbf{0}\)</span>. åŠ æ³•é€†å…ƒï¼šå¯¹äºæ¯ä¸ª <span
class="math inline">\(\mathbf{v} \in V\)</span> ï¼Œå­˜åœ¨ <span
class="math inline">\((-\mathbf{v} \in V\)</span> ä½¿å¾— <span
class="math inline">\(\mathbf{v} + (-\mathbf{v}) = \mathbf{0}\)</span>
ã€‚</li>
<li>Compatibility of scalar multiplication: <span
class="math inline">\(a(b\mathbf{v}) = (ab)\mathbf{v}\)</span>.
æ ‡é‡ä¹˜æ³•çš„å…¼å®¹æ€§ï¼š <span class="math inline">\(a(b\mathbf{v}) =
(ab)\mathbf{v}\)</span> ã€‚</li>
<li>Identity element of scalars: 1â‹…v=v. æ ‡é‡çš„æ ‡è¯†å…ƒï¼š 1â‹…v=v ã€‚</li>
<li>Distributivity over vector addition: <span
class="math inline">\(a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} +
a\mathbf{v}\)</span>. å‘é‡åŠ æ³•çš„åˆ†é…å¾‹ï¼š <span
class="math inline">\(a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} +
a\mathbf{v}\)</span> ã€‚</li>
<li>Distributivity over scalar addition: <span
class="math inline">\((a+b)\mathbf{v} = a\mathbf{v} +
b\mathbf{v}\)</span>. æ ‡é‡åŠ æ³•çš„åˆ†é…å¾‹ï¼š <span
class="math inline">\((a+b)\mathbf{v} = a\mathbf{v} +
b\mathbf{v}\)</span> ã€‚</li>
</ol>
<p>If a set <span class="math inline">\(V\)</span> with operations
satisfies all eight axioms, we call it a vector space. å¦‚æœä¸€ä¸ªé›†åˆ
<span class="math inline">\(V\)</span>
æ»¡è¶³æ‰€æœ‰å…«ä¸ªå…¬ç†ï¼Œæˆ‘ä»¬ç§°å®ƒä¸ºå‘é‡ç©ºé—´ã€‚</p>
<h3 id="examples-1">Examples</h3>
<p>ç¤ºä¾‹</p>
<p>Example 4.1.1. Standard Euclidean space <span
class="math inline">\(\mathbb{R}^n\)</span> with ordinary addition and
scalar multiplication is a vector space. This is the model case from
which the axioms are abstracted. ä¾‹ 4.1.1. æ ‡å‡†æ¬§å‡ é‡Œå¾—ç©ºé—´ <span
class="math inline">\(\mathbb{R}^n\)</span>
è¿›è¡Œæ™®é€šçš„åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•è¿ç®—åï¼Œæ˜¯ä¸€ä¸ªå‘é‡ç©ºé—´ã€‚è¿™æ˜¯æŠ½è±¡å‡ºå…¬ç†çš„å…¸å‹ä¾‹å­ã€‚</p>
<p>Example 4.1.2. Polynomials The set of all polynomials with real
coefficients, denoted <span
class="math inline">\(\mathbb{R}[x]\)</span>, forms a vector space.
Addition and scalar multiplication are defined term by term. ä¾‹ 4.1.2.
å¤šé¡¹å¼ æ‰€æœ‰å®ç³»æ•°å¤šé¡¹å¼çš„é›†åˆï¼Œè®°ä¸º <span
class="math inline">\(\mathbb{R}[x]\)</span>
ï¼Œæ„æˆä¸€ä¸ªå‘é‡ç©ºé—´ã€‚åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•æ˜¯é€é¡¹å®šä¹‰çš„ã€‚</p>
<p>Example 4.1.3. Functions The set of all real-valued functions on an
interval, e.g.Â <span class="math inline">\(f: [0,1] \to
\mathbb{R}\)</span>, forms a vector space, since functions can be added
and scaled pointwise. ä¾‹ 4.1.3. å‡½æ•° åŒºé—´ä¸Šçš„æ‰€æœ‰å®å€¼å‡½æ•°çš„é›†åˆï¼Œä¾‹å¦‚
<span class="math inline">\(f: [0,1] \to \mathbb{R}\)</span>
ï¼Œå½¢æˆä¸€ä¸ªå‘é‡ç©ºé—´ï¼Œå› ä¸ºå‡½æ•°å¯ä»¥é€ç‚¹æ·»åŠ å’Œç¼©æ”¾ã€‚</p>
<h3 id="non-examples">Non-Examples</h3>
<p>éç¤ºä¾‹</p>
<p>Not every set with operations qualifies. For instance, the set of
positive real numbers under usual addition is not a vector space,
because additive inverses (negative numbers) are missing. The axioms
must all hold.
å¹¶éæ‰€æœ‰åŒ…å«è¿ç®—çš„é›†åˆéƒ½ç¬¦åˆæ¡ä»¶ã€‚ä¾‹å¦‚ï¼Œé€šå¸¸åŠ æ³•è¿ç®—ä¸‹çš„æ­£å®æ•°é›†ä¸æ˜¯å‘é‡ç©ºé—´ï¼Œå› ä¸ºç¼ºå°‘åŠ æ³•é€†å…ƒï¼ˆè´Ÿæ•°ï¼‰ã€‚å…¬ç†å¿…é¡»å…¨éƒ¨æˆç«‹ã€‚</p>
<h3 id="geometric-interpretation-3">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>In familiar cases like <span
class="math inline">\(\mathbb{R}^2\)</span> or <span
class="math inline">\(\mathbb{R}^3\)</span>, vector spaces provide the
stage for geometry: vectors can be added, scaled, and combined to form
lines, planes, and higher-dimensional structures. In abstract settings
like function spaces, the same algebraic rules let us apply geometric
intuition to infinite-dimensional problems. åœ¨åƒ <span
class="math inline">\(\mathbb{R}^2\)</span> æˆ– <span
class="math inline">\(\mathbb{R}^3\)</span>
è¿™æ ·å¸¸è§çš„æƒ…å½¢ä¸‹ï¼Œå‘é‡ç©ºé—´ä¸ºå‡ ä½•å­¦æä¾›äº†èˆå°ï¼šå‘é‡å¯ä»¥ç›¸åŠ ã€ç¼©æ”¾å’Œç»„åˆï¼Œä»è€Œå½¢æˆçº¿ã€å¹³é¢å’Œæ›´é«˜ç»´åº¦çš„ç»“æ„ã€‚åœ¨åƒå‡½æ•°ç©ºé—´è¿™æ ·çš„æŠ½è±¡ç¯å¢ƒä¸­ï¼ŒåŒæ ·çš„ä»£æ•°è§„åˆ™è®©æˆ‘ä»¬èƒ½å¤Ÿå°†å‡ ä½•ç›´è§‰åº”ç”¨äºæ— é™ç»´é—®é¢˜ã€‚</p>
<h3 id="why-this-matters-11">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>The concept of vector space unifies seemingly different mathematical
objects under a single framework. Whether dealing with forces in
physics, signals in engineering, or data in machine learning, the common
language of vector spaces allows us to use the same techniques
everywhere.
å‘é‡ç©ºé—´çš„æ¦‚å¿µå°†çœ‹ä¼¼ä¸åŒçš„æ•°å­¦å¯¹è±¡ç»Ÿä¸€åœ¨ä¸€ä¸ªæ¡†æ¶ä¸‹ã€‚æ— è®ºæ˜¯å¤„ç†ç‰©ç†å­¦ä¸­çš„åŠ›ã€å·¥ç¨‹å­¦ä¸­çš„ä¿¡å·ï¼Œè¿˜æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®ï¼Œå‘é‡ç©ºé—´çš„é€šç”¨è¯­è¨€ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä»»ä½•åœ°æ–¹ä½¿ç”¨ç›¸åŒçš„æŠ€æœ¯ã€‚</p>
<h3 id="exercises-4.1">Exercises 4.1</h3>
<p>ç»ƒä¹ 4.1</p>
<ol type="1">
<li>Verify that <span class="math inline">\(\mathbb{R}^2\)</span> with
standard addition and scalar multiplication satisfies all eight vector
space axioms. éªŒè¯ <span class="math inline">\(\mathbb{R}^2\)</span>
é€šè¿‡æ ‡å‡†åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•æ»¡è¶³æ‰€æœ‰å…«ä¸ªå‘é‡ç©ºé—´å…¬ç†ã€‚</li>
<li>Show that the set of integers <span
class="math inline">\(\mathbb{Z}\)</span> with ordinary operations is
not a vector space over <span class="math inline">\(\mathbb{R}\)</span>.
Which axiom fails? è¯æ˜ï¼šå…·æœ‰æ™®é€šè¿ç®—çš„æ•´æ•°é›† <span
class="math inline">\(\mathbb{Z}\)</span> ä¸æ˜¯ <span
class="math inline">\(\mathbb{R}\)</span>
ä¸Šçš„å‘é‡ç©ºé—´ã€‚å“ªæ¡å…¬ç†ä¸æˆç«‹ï¼Ÿ</li>
<li>Consider the set of all polynomials of degree at most 3. Show it
forms a vector space over <span
class="math inline">\(\mathbb{R}\)</span>. What is its dimension?
è€ƒè™‘æ‰€æœ‰æ¬¡æ•°æœ€å¤šä¸º3çš„å¤šé¡¹å¼çš„é›†åˆã€‚è¯æ˜å®ƒæ„æˆä¸€ä¸ª <span
class="math inline">\(\mathbb{R}\)</span>
ä¸Šçš„å‘é‡ç©ºé—´ã€‚å®ƒçš„ç»´åº¦æ˜¯å¤šå°‘ï¼Ÿ</li>
<li>Give an example of a vector space where the vectors are not
geometric objects. ç»™å‡ºä¸€ä¸ªå‘é‡ç©ºé—´çš„ä¾‹å­ï¼Œå…¶ä¸­çš„å‘é‡ä¸æ˜¯å‡ ä½•å¯¹è±¡ã€‚</li>
<li>Prove that in any vector space, the zero vector is unique.
è¯æ˜åœ¨ä»»ä½•å‘é‡ç©ºé—´ä¸­ï¼Œé›¶å‘é‡éƒ½æ˜¯å”¯ä¸€çš„ã€‚</li>
</ol>
<h2 id="subspaces">4.2 Subspaces</h2>
<p>4.2 å­ç©ºé—´</p>
<p>A subspace is a smaller vector space living inside a larger one. Just
as lines and planes naturally sit inside three-dimensional space,
subspaces generalize these ideas to higher dimensions and more abstract
settings.
å­ç©ºé—´æ˜¯ä½äºè¾ƒå¤§å‘é‡ç©ºé—´ä¸­çš„è¾ƒå°å‘é‡ç©ºé—´ã€‚æ­£å¦‚çº¿å’Œå¹³é¢è‡ªç„¶åœ°å­˜åœ¨äºä¸‰ç»´ç©ºé—´ä¸­ä¸€æ ·ï¼Œå­ç©ºé—´å°†è¿™äº›æ¦‚å¿µæ¨å¹¿åˆ°æ›´é«˜ç»´åº¦å’Œæ›´æŠ½è±¡çš„åœºæ™¯ã€‚</p>
<h3 id="definition-1">Definition</h3>
<p>å®šä¹‰</p>
<p>Let <span class="math inline">\(V\)</span> be a vector space. A
subset <span class="math inline">\(W \subseteq V\)</span> is called a
subspace of <span class="math inline">\(V\)</span> if: ä»¤ <span
class="math inline">\(V\)</span> ä¸ºå‘é‡ç©ºé—´ã€‚è‹¥æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™å­é›†
<span class="math inline">\(W \subseteq V\)</span> ç§°ä¸º <span
class="math inline">\(V\)</span> çš„å­ç©ºé—´ï¼š</p>
<ol type="1">
<li><span class="math inline">\(\mathbf{0} \in W\)</span> (contains the
zero vector), <span class="math inline">\(\mathbf{0} \in W\)</span>
ï¼ˆåŒ…å«é›¶å‘é‡ï¼‰ï¼Œ</li>
<li>For all <span class="math inline">\(\mathbf{u}, \mathbf{v} \in
W\)</span>, the sum <span class="math inline">\(\mathbf{u} + \mathbf{v}
\in W\)</span> (closed under addition), å¯¹äºæ‰€æœ‰ <span
class="math inline">\(\mathbf{u}, \mathbf{v} \in W\)</span> ï¼Œæ€»å’Œä¸º
<span class="math inline">\(\mathbf{u} + \mathbf{v} \in W\)</span>
ï¼ˆåŠ æ³•é—­åŒ…ï¼‰ï¼Œ</li>
<li>For all scalars <span class="math inline">\(c \in
\mathbb{R}\)</span> and vectors <span class="math inline">\(\mathbf{v}
\in W\)</span>, the product <span class="math inline">\(c\mathbf{v} \in
W\)</span> (closed under scalar multiplication). å¯¹äºæ‰€æœ‰æ ‡é‡ <span
class="math inline">\(c \in \mathbb{R}\)</span> å’Œå‘é‡ <span
class="math inline">\(\mathbf{v} \in W\)</span> ï¼Œä¹˜ç§¯ <span
class="math inline">\(c\mathbf{v} \in W\)</span>
ï¼ˆåœ¨æ ‡é‡ä¹˜æ³•ä¸‹å°é—­ï¼‰ã€‚</li>
</ol>
<p>If these hold, then <span class="math inline">\(W\)</span> is itself
a vector space with the inherited operations. å¦‚æœè¿™äº›æˆç«‹ï¼Œé‚£ä¹ˆ <span
class="math inline">\(W\)</span> æœ¬èº«å°±æ˜¯å…·æœ‰ç»§æ‰¿æ“ä½œçš„å‘é‡ç©ºé—´ã€‚</p>
<h3 id="examples-2">Examples</h3>
<p>ç¤ºä¾‹</p>
<p>Example 4.2.1. Line through the origin in <span
class="math inline">\(\mathbb{R}^2\)</span> The set ä¾‹ 4.2.1. ç©¿è¿‡ <span
class="math inline">\(\mathbb{R}^2\)</span> ä¸­çš„åŸç‚¹çš„çº¿ è¯¥å¥—è£…</p>
<p><span class="math display">\[
W = \{ (t, 2t) \mid t \in \mathbb{R} \}
\]</span></p>
<p>is a subspace of <span class="math inline">\(\mathbb{R}^2\)</span>.
It contains the zero vector, is closed under addition, and is closed
under scalar multiplication. æ˜¯ <span
class="math inline">\(\mathbb{R}^2\)</span>
çš„ä¸€ä¸ªå­ç©ºé—´ã€‚å®ƒåŒ…å«é›¶å‘é‡ï¼Œåœ¨åŠ æ³•è¿ç®—ä¸‹å°é—­ï¼Œåœ¨æ ‡é‡ä¹˜æ³•è¿ç®—ä¸‹å°é—­ã€‚</p>
<p>Example 4.2.2. The xâ€“y plane in <span
class="math inline">\(\mathbb{R}^3\)</span> The set ä¾‹ 4.2.2. <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­çš„ x-y å¹³é¢ è¯¥å¥—è£…</p>
<p><span class="math display">\[
W = \{ (x, y, 0) \mid x,y \in \mathbb{R} \}
\]</span></p>
<p>is a subspace of <span class="math inline">\(\mathbb{R}^3\)</span>.
It is the collection of all vectors lying in the plane through the
origin parallel to the xâ€“y plane. æ˜¯ <span
class="math inline">\(\mathbb{R}^3\)</span>
çš„ä¸€ä¸ªå­ç©ºé—´ã€‚å®ƒæ˜¯ä½äºé€šè¿‡åŸç‚¹å¹¶å¹³è¡Œäº x-y
å¹³é¢çš„å¹³é¢å†…çš„æ‰€æœ‰å‘é‡çš„é›†åˆã€‚</p>
<p>Example 4.2.3. Null space of a matrix For a matrix <span
class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, the null
space ä¾‹ 4.2.3. çŸ©é˜µçš„é›¶ç©ºé—´ å¯¹äºçŸ©é˜µ <span class="math inline">\(A \in
\mathbb{R}^{m \times n}\)</span> ï¼Œé›¶ç©ºé—´</p>
<p><span class="math display">\[
\{ \mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = \mathbf{0} \}
\]</span></p>
<p>is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>.
This subspace represents all solutions to the homogeneous system. æ˜¯
<span class="math inline">\(\mathbb{R}^n\)</span>
çš„ä¸€ä¸ªå­ç©ºé—´ã€‚è¯¥å­ç©ºé—´è¡¨ç¤ºé½æ¬¡ç³»ç»Ÿçš„æ‰€æœ‰è§£ã€‚</p>
<h3 id="non-examples-1">Non-Examples</h3>
<p>éç¤ºä¾‹</p>
<p>Not every subset is a subspace. å¹¶éæ¯ä¸ªå­é›†éƒ½æ˜¯å­ç©ºé—´ã€‚</p>
<ul>
<li>The set <span class="math inline">\({ (x,y) \in \mathbb{R}^2 \mid x
\geq 0 }\)</span> is not a subspace: it is not closed under scalar
multiplication (a negative scalar breaks the condition). é›†åˆ <span
class="math inline">\({ (x,y) \in \mathbb{R}^2 \mid x \geq 0 }\)</span>
ä¸æ˜¯å­ç©ºé—´ï¼šå®ƒåœ¨æ ‡é‡ä¹˜æ³•ä¸‹ä¸å°é—­ï¼ˆè´Ÿæ ‡é‡ä¼šç ´åè¯¥æ¡ä»¶ï¼‰ã€‚</li>
<li>Any line in <span class="math inline">\(\mathbb{R}^2\)</span> that
does not pass through the origin is not a subspace, because it does not
contain <span class="math inline">\(\mathbf{0}\)</span>. <span
class="math inline">\(\mathbb{R}^2\)</span>
ä¸­ä»»ä½•ä¸ç»è¿‡åŸç‚¹çš„çº¿éƒ½ä¸æ˜¯å­ç©ºé—´ï¼Œå› ä¸ºå®ƒä¸åŒ…å« <span
class="math inline">\(\mathbf{0}\)</span> ã€‚</li>
</ul>
<h3 id="geometric-interpretation-4">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>Subspaces are the linear structures inside vector spaces.
å­ç©ºé—´æ˜¯å‘é‡ç©ºé—´å†…çš„çº¿æ€§ç»“æ„ã€‚</p>
<ul>
<li>In <span class="math inline">\(\mathbb{R}^2\)</span>, the subspaces
are: the zero vector, any line through the origin, or the entire plane.
åœ¨ <span class="math inline">\(\mathbb{R}^2\)</span>
ä¸­ï¼Œå­ç©ºé—´æ˜¯ï¼šé›¶å‘é‡ã€è¿‡åŸç‚¹çš„ä»»æ„ç›´çº¿æˆ–æ•´ä¸ªå¹³é¢ã€‚</li>
<li>In <span class="math inline">\(\mathbb{R}^3\)</span>, the subspaces
are: the zero vector, any line through the origin, any plane through the
origin, or the entire space. åœ¨ <span
class="math inline">\(\mathbb{R}^3\)</span>
ä¸­ï¼Œå­ç©ºé—´æ˜¯ï¼šé›¶å‘é‡ã€è¿‡åŸç‚¹çš„ä»»æ„ç›´çº¿ã€è¿‡åŸç‚¹çš„ä»»æ„å¹³é¢æˆ–æ•´ä¸ªç©ºé—´ã€‚</li>
<li>In higher dimensions, the same principle applies: subspaces are the
flat linear pieces through the origin.
åœ¨æ›´é«˜çš„ç»´åº¦ä¸­ï¼ŒåŒæ ·çš„åŸç†é€‚ç”¨ï¼šå­ç©ºé—´æ˜¯é€šè¿‡åŸç‚¹çš„å¹³å¦çº¿æ€§éƒ¨åˆ†ã€‚</li>
</ul>
<h3 id="why-this-matters-12">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Subspaces capture the essential structure of linear problems. Column
spaces, row spaces, and null spaces are all subspaces. Much of linear
algebra consists of understanding how these subspaces intersect, span,
and complement each other.
å­ç©ºé—´æ•æ‰äº†çº¿æ€§é—®é¢˜çš„æœ¬è´¨ç»“æ„ã€‚åˆ—ç©ºé—´ã€è¡Œç©ºé—´å’Œé›¶ç©ºé—´éƒ½æ˜¯å­ç©ºé—´ã€‚çº¿æ€§ä»£æ•°çš„å¤§éƒ¨åˆ†å†…å®¹éƒ½åœ¨äºç†è§£è¿™äº›å­ç©ºé—´å¦‚ä½•ç›¸äº’äº¤å‰ã€å»¶ä¼¸å’Œäº’è¡¥ã€‚</p>
<h3 id="exercises-4.2">Exercises 4.2</h3>
<p>ç»ƒä¹  4.2</p>
<ol type="1">
<li>Prove that the set <span class="math inline">\(W = { (x,0) \mid x
\in \mathbb{R} } \subseteq \mathbb{R}^2\)</span> is a subspace. è¯æ˜é›†åˆ
<span class="math inline">\(W = { (x,0) \mid x \in \mathbb{R} }
\subseteq \mathbb{R}^2\)</span> æ˜¯ä¸€ä¸ªå­ç©ºé—´ã€‚</li>
<li>Show that the line <span class="math inline">\({ (1+t, 2t) \mid t
\in \mathbb{R} }\)</span> is not a subspace of <span
class="math inline">\(\mathbb{R}^2\)</span>. Which condition fails?
è¯æ˜è¡Œ <span class="math inline">\({ (1+t, 2t) \mid t \in \mathbb{R}
}\)</span> ä¸æ˜¯ <span class="math inline">\(\mathbb{R}^2\)</span>
çš„å­ç©ºé—´ã€‚å“ªä¸ªæ¡ä»¶ä¸æˆç«‹ï¼Ÿ</li>
<li>Determine whether the set of all vectors <span
class="math inline">\((x,y,z) \in \mathbb{R}^3\)</span> satisfying <span
class="math inline">\(x+y+z=0\)</span> is a subspace. ç¡®å®šæ»¡è¶³ <span
class="math inline">\(x+y+z=0\)</span> çš„æ‰€æœ‰å‘é‡ <span
class="math inline">\((x,y,z) \in \mathbb{R}^3\)</span>
çš„é›†åˆæ˜¯å¦ä¸ºå­ç©ºé—´ã€‚</li>
<li>For the matrix å¯¹äºçŸ©é˜µ</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 &amp; 3 \\4 &amp; 5 &amp; 6\end{bmatrix}
\]</span></p>
<p>Describe the null space of <span class="math inline">\(A\)</span> as
a subspace of <span class="math inline">\(\mathbb{R}^3\)</span>. å°†
<span class="math inline">\(A\)</span> çš„é›¶ç©ºé—´æè¿°ä¸º <span
class="math inline">\(\mathbb{R}^3\)</span> çš„å­ç©ºé—´ã€‚</p>
<ol start="5" type="1">
<li>List all possible subspaces of <span
class="math inline">\(\mathbb{R}^2\)</span>. åˆ—å‡º <span
class="math inline">\(\mathbb{R}^2\)</span> æ‰€æœ‰å¯èƒ½çš„å­ç©ºé—´ã€‚</li>
</ol>
<h2 id="span-basis-dimension">4.3 Span, Basis, Dimension</h2>
<p>4.3 è·¨åº¦ã€åŸºã€ç»´åº¦</p>
<p>The ideas of span, basis, and dimension provide the language for
describing the size and structure of subspaces. Together, they tell us
how a vector space is generated, how many building blocks it requires,
and how those blocks can be chosen.
è·¨åº¦ã€åŸºå’Œç»´æ•°çš„æ¦‚å¿µæä¾›äº†æè¿°å­ç©ºé—´å¤§å°å’Œç»“æ„çš„è¯­è¨€ã€‚å®ƒä»¬å…±åŒå‘Šè¯‰æˆ‘ä»¬å‘é‡ç©ºé—´æ˜¯å¦‚ä½•ç”Ÿæˆçš„ï¼Œå®ƒéœ€è¦å¤šå°‘ä¸ªæ„å»ºå—ï¼Œä»¥åŠå¦‚ä½•é€‰æ‹©è¿™äº›æ„å»ºå—ã€‚</p>
<h3 id="span">Span</h3>
<p>è·¨åº¦</p>
<p>Given a set of vectors <span class="math inline">\({\mathbf{v}_1,
\mathbf{v}_2, \dots, \mathbf{v}_k} \subseteq V\)</span>, the span is the
collection of all linear combinations: ç»™å®šä¸€ç»„å‘é‡ <span
class="math inline">\({\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k}
\subseteq V\)</span> ï¼Œè·¨åº¦æ˜¯æ‰€æœ‰çº¿æ€§ç»„åˆçš„é›†åˆï¼š</p>
<p><span class="math display">\[
\text{span}\{\mathbf{v}_1, \dots, \mathbf{v}_k\} = \{ c_1\mathbf{v}_1 +
\cdots + c_k\mathbf{v}_k \mid c_i \in \mathbb{R} \}.
\]</span></p>
<p>The span is always a subspace of <span
class="math inline">\(V\)</span>, namely the smallest subspace
containing those vectors. è·¨åº¦å§‹ç»ˆæ˜¯ <span
class="math inline">\(V\)</span>
çš„å­ç©ºé—´ï¼Œå³åŒ…å«è¿™äº›å‘é‡çš„æœ€å°å­ç©ºé—´ã€‚</p>
<p>Example 4.3.1. In <span class="math inline">\(\mathbb{R}^2\)</span>,
$ = {(x,0) x },$ the x-axis. Similarly, <span
class="math inline">\(\text{span}\{(1,0),(0,1)\} =
\mathbb{R}^2.\)</span> ä¾‹ 4.3.1ã€‚ åœ¨ <span
class="math inline">\(\mathbb{R}^2\)</span> ä¸­ï¼Œ $ = {(x,0) x },$ x
è½´ã€‚åŒæ ·ï¼Œ <span class="math inline">\(\text{span}\{(1,0),(0,1)\} =
\mathbb{R}^2.\)</span></p>
<h3 id="basis">Basis</h3>
<p>åŸºç¡€</p>
<p>A basis of a vector space <span class="math inline">\(V\)</span> is a
set of vectors that: å‘é‡ç©ºé—´ <span class="math inline">\(V\)</span>
çš„åŸºæ˜¯ä¸€ç»„å‘é‡ï¼Œå…¶ï¼š</p>
<ol type="1">
<li>Span <span class="math inline">\(V\)</span>. è·¨åº¦ <span
class="math inline">\(V\)</span> ã€‚</li>
<li>Are linearly independent (no vector in the set is a linear
combination of the others).
æ˜¯çº¿æ€§ç‹¬ç«‹çš„ï¼ˆé›†åˆä¸­æ²¡æœ‰å‘é‡æ˜¯å…¶ä»–å‘é‡çš„çº¿æ€§ç»„åˆï¼‰ã€‚</li>
</ol>
<p>If either condition fails, the set is not a basis.
å¦‚æœä»»ä¸€æ¡ä»¶ä¸æˆç«‹ï¼Œåˆ™è¯¥é›†åˆä¸ä½œä¸ºåŸºç¡€ã€‚</p>
<p>Example 4.3.2. In <span class="math inline">\(\mathbb{R}^3\)</span>,
the standard unit vectors ä¾‹ 4.3.2ã€‚ åœ¨ <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­ï¼Œæ ‡å‡†å•ä½å‘é‡</p>
<p><span class="math display">\[
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3
= (0,0,1)
\]</span></p>
<p>form a basis. Every vector <span
class="math inline">\((x,y,z)\)</span> can be uniquely written as
æ„æˆåŸºç¡€ã€‚æ¯ä¸ªå‘é‡ <span class="math inline">\((x,y,z)\)</span>
éƒ½å¯ä»¥å”¯ä¸€åœ°å†™æˆ</p>
<p><span class="math display">\[
x\mathbf{e}_1 + y\mathbf{e}_2 + z\mathbf{e}_3.
\]</span></p>
<h3 id="dimension">Dimension</h3>
<p>æ–¹é¢</p>
<p>The dimension of a vector space <span
class="math inline">\(V\)</span>, written <span
class="math inline">\(\dim(V)\)</span>, is the number of vectors in any
basis of <span class="math inline">\(V\)</span>. This number is
well-defined: all bases of a vector space have the same cardinality.
å‘é‡ç©ºé—´ <span class="math inline">\(V\)</span> çš„ç»´æ•°ï¼Œè®°ä½œ <span
class="math inline">\(\dim(V)\)</span> ï¼Œæ˜¯ä»»æ„ <span
class="math inline">\(V\)</span>
çš„åŸºä¸­å‘é‡çš„æ•°é‡ã€‚è¿™ä¸ªç»´æ•°å®šä¹‰æ˜ç¡®ï¼šå‘é‡ç©ºé—´çš„æ‰€æœ‰åŸºéƒ½å…·æœ‰ç›¸åŒçš„åŸºæ•°ã€‚</p>
<p>Examples 4.3.3. ç¤ºä¾‹ 4.3.3ã€‚</p>
<ul>
<li><span class="math inline">\(\dim(\mathbb{R}^2) = 2\)</span>, with
basis <span class="math inline">\((1,0), (0,1)\)</span>. <span
class="math inline">\(\dim(\mathbb{R}^2) = 2\)</span> ï¼Œä¾æ®æ˜¯ <span
class="math inline">\((1,0), (0,1)\)</span> ã€‚</li>
<li><span class="math inline">\(\dim(\mathbb{R}^3) = 3\)</span>, with
basis <span class="math inline">\((1,0,0), (0,1,0), (0,0,1)\)</span>.
<span class="math inline">\(\dim(\mathbb{R}^3) = 3\)</span> ï¼Œä¾æ®æ˜¯
<span class="math inline">\((1,0,0), (0,1,0), (0,0,1)\)</span> ã€‚</li>
<li>The set of polynomials of degree at most 3 has dimension 4, with
basis <span class="math inline">\((1, x, x^2, x^3)\)</span>. æ¬¡æ•°æœ€å¤šä¸º
3 çš„å¤šé¡¹å¼é›†çš„ç»´åº¦ä¸º 4ï¼ŒåŸºä¸º <span class="math inline">\((1, x, x^2,
x^3)\)</span> ã€‚</li>
</ul>
<h3 id="geometric-interpretation-5">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<ul>
<li>The span is like the reach of a set of vectors.
è·¨åº¦å°±åƒä¸€ç»„å‘é‡çš„èŒƒå›´ã€‚</li>
<li>A basis is the minimal set of directions needed to reach everything
in the space. åŸºç¡€æ˜¯åˆ°è¾¾ç©ºé—´ä¸­æ‰€æœ‰äº‹ç‰©æ‰€éœ€çš„æœ€å°æ–¹å‘é›†ã€‚</li>
<li>The dimension is the count of those independent directions.
ç»´åº¦æ˜¯è¿™äº›ç‹¬ç«‹æ–¹å‘çš„æ•°é‡ã€‚</li>
</ul>
<p>Lines, planes, and higher-dimensional flats can all be described in
terms of span, basis, and dimension.
çº¿ã€å¹³é¢å’Œé«˜ç»´å¹³é¢éƒ½å¯ä»¥ç”¨è·¨åº¦ã€åŸºå’Œç»´åº¦æ¥æè¿°ã€‚</p>
<h3 id="why-this-matters-13">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>These concepts classify vector spaces and subspaces in terms of size
and structure. Many theorems in linear algebra-such as the Rankâ€“Nullity
Theorem-are consequences of understanding span, basis, and dimension. In
practical terms, bases are how we encode data in coordinates, and
dimension tells us how much freedom a system truly has.
è¿™äº›æ¦‚å¿µæ ¹æ®å¤§å°å’Œç»“æ„å¯¹å‘é‡ç©ºé—´å’Œå­ç©ºé—´è¿›è¡Œåˆ†ç±»ã€‚çº¿æ€§ä»£æ•°ä¸­çš„è®¸å¤šå®šç†ï¼Œä¾‹å¦‚ç§©é›¶å®šç†ï¼Œéƒ½æ˜¯ç†è§£è·¨åº¦ã€åŸºå’Œç»´æ•°çš„ç»“æœã€‚å®é™…ä¸Šï¼ŒåŸºæ˜¯æˆ‘ä»¬åœ¨åæ ‡ç³»ä¸­ç¼–ç æ•°æ®çš„æ–¹å¼ï¼Œè€Œç»´æ•°åˆ™å‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªç³»ç»ŸçœŸæ­£æ‹¥æœ‰å¤šå°‘è‡ªç”±åº¦ã€‚</p>
<h3 id="exercises-4.3">Exercises 4.3</h3>
<p>ç»ƒä¹  4.3</p>
<ol type="1">
<li>Show that <span class="math inline">\((1,0,0)\)</span>, <span
class="math inline">\((0,1,0)\)</span>, <span
class="math inline">\((1,1,0)\)</span> span the <span
class="math inline">\(xy\)</span>-plane in <span
class="math inline">\(\mathbb{R}^3\)</span>. Are they a basis? è¯æ˜
<span class="math inline">\((1,0,0)\)</span> , <span
class="math inline">\((0,1,0)\)</span> , <span
class="math inline">\((1,1,0)\)</span> åœ¨ <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­è·¨è¶Š <span
class="math inline">\(xy\)</span> -å¹³é¢ã€‚å®ƒä»¬æ˜¯åŸºå—ï¼Ÿ</li>
<li>Find a basis for the line <span class="math inline">\(\{(2t,-3t,t) :
t \in \mathbb{R}\}\)</span> in <span
class="math inline">\(\mathbb{R}^3\)</span>. æ‰¾å‡º <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­ç¬¬ <span
class="math inline">\(\{(2t,-3t,t) : t \in \mathbb{R}\}\)</span>
è¡Œçš„ä¾æ®ã€‚</li>
<li>Determine the dimension of the subspace of <span
class="math inline">\(\mathbb{R}^3\)</span> defined by <span
class="math inline">\(x+y+z=0\)</span>. ç¡®å®šç”± <span
class="math inline">\(x+y+z=0\)</span> å®šä¹‰çš„ <span
class="math inline">\(\mathbb{R}^3\)</span> å­ç©ºé—´çš„ç»´æ•°ã€‚</li>
<li>Prove that any two different bases of <span
class="math inline">\(\mathbb{R}^n\)</span> must contain exactly <span
class="math inline">\(n\)</span> vectors. è¯æ˜ <span
class="math inline">\(\mathbb{R}^n\)</span> çš„ä»»æ„ä¸¤ä¸ªä¸åŒåŸºå¿…å®šåŒ…å«æ°å¥½
<span class="math inline">\(n\)</span> ä¸ªå‘é‡ã€‚</li>
<li>Give a basis for the set of polynomials of degree <span
class="math inline">\(\leq 2\)</span>. What is its dimension? ç»™å‡ºæ¬¡æ•°ä¸º
<span class="math inline">\(\leq 2\)</span>
çš„å¤šé¡¹å¼é›†çš„åŸºã€‚å®ƒçš„ç»´æ•°æ˜¯å¤šå°‘ï¼Ÿ</li>
</ol>
<h2 id="coordinates">4.4 Coordinates</h2>
<p>4.4 åæ ‡</p>
<p>Once a basis for a vector space is chosen, every vector can be
expressed uniquely as a linear combination of the basis vectors. The
coefficients in this combination are called the coordinates of the
vector relative to that basis. Coordinates allow us to move between the
abstract world of vector spaces and the concrete world of numbers.
ä¸€æ—¦é€‰å®šäº†å‘é‡ç©ºé—´çš„åŸºï¼Œæ¯ä¸ªå‘é‡éƒ½å¯ä»¥å”¯ä¸€åœ°è¡¨ç¤ºä¸ºåŸºå‘é‡çš„çº¿æ€§ç»„åˆã€‚è¯¥ç»„åˆä¸­çš„ç³»æ•°ç§°ä¸ºå‘é‡ç›¸å¯¹äºè¯¥åŸºçš„åæ ‡ã€‚åæ ‡ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å‘é‡ç©ºé—´çš„æŠ½è±¡ä¸–ç•Œå’Œå…·ä½“çš„æ•°å­—ä¸–ç•Œä¹‹é—´ç§»åŠ¨ã€‚</p>
<h3 id="coordinates-relative-to-a-basis">Coordinates Relative to a
Basis</h3>
<p>ç›¸å¯¹äºåŸºåæ ‡</p>
<p>Let <span class="math inline">\(V\)</span> be a vector space, and let
ä»¤ <span class="math inline">\(V\)</span> ä¸ºå‘é‡ç©ºé—´ï¼Œ</p>
<p><span class="math display">\[
\mathcal{B} = \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}
\]</span></p>
<p>be an ordered basis for <span class="math inline">\(V\)</span>. Every
vector <span class="math inline">\(\mathbf{u} \in V\)</span> can be
written uniquely as æ˜¯ <span class="math inline">\(V\)</span>
çš„æœ‰åºåŸºã€‚æ¯ä¸ªå‘é‡ <span class="math inline">\(\mathbf{u} \in V\)</span>
éƒ½å¯ä»¥å”¯ä¸€åœ°å†™æˆ</p>
<p><span class="math display">\[
\mathbf{u} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n
\mathbf{v}_n.
\]</span></p>
<p>The scalars <span class="math inline">\((c_1, c_2, \dots,
c_n)\)</span> are the coordinates of <span
class="math inline">\(\mathbf{u}\)</span> relative to <span
class="math inline">\(\mathcal{B}\)</span>, written æ ‡é‡ <span
class="math inline">\((c_1, c_2, \dots, c_n)\)</span> æ˜¯ <span
class="math inline">\(\mathbf{u}\)</span> ç›¸å¯¹äº <span
class="math inline">\(\mathcal{B}\)</span> çš„åæ ‡ï¼Œå†™ä¸º</p>
<p><span class="math display">\[
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n
\end{bmatrix}.
\]</span></p>
<h3 id="example-in-mathbbr2">Example in <span
class="math inline">\(\mathbb{R}^2\)</span></h3>
<p><span class="math inline">\(\mathbb{R}^2\)</span> ä¸­çš„ç¤ºä¾‹</p>
<p>Example 4.4.1. Let the basis be ä¾‹ 4.4.1. è®¾åŸºç¡€ä¸º</p>
<p><span class="math display">\[
\mathcal{B} = \{ (1,1), (1,-1) \}.
\]</span></p>
<p>To find the coordinates of <span class="math inline">\(\mathbf{u} =
(3,1)\)</span> relative to <span
class="math inline">\(\mathcal{B}\)</span>, solve è¦æŸ¥æ‰¾ <span
class="math inline">\(\mathbf{u} = (3,1)\)</span> ç›¸å¯¹äº <span
class="math inline">\(\mathcal{B}\)</span> çš„åæ ‡ï¼Œè¯·æ±‚è§£</p>
<p><span class="math display">\[
(3,1) = c_1(1,1) + c_2(1,-1).
\]</span></p>
<p>This gives the system è¿™ä½¿å¾—ç³»ç»Ÿ</p>
<p><span class="math display">\[
\begin{cases}c_1 + c_2 = 3, \\c_1 - c_2 = 1.\end{cases}
\]</span></p>
<p>Adding: <span class="math inline">\(2c\_1 = 4 \\implies c\_1 =
2\)</span>. Then <span class="math inline">\(c\_2 = 1\)</span>.
æ·»åŠ ï¼š$2c_1 = 4 \implies c_1 = 2 $. Then $ c_2 = 1$ã€‚</p>
<p>So, æ‰€ä»¥ï¼Œ</p>
<p><span class="math display">\[
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}.
\]</span></p>
<h3 id="standard-coordinates">Standard Coordinates</h3>
<p>æ ‡å‡†åæ ‡</p>
<p>In <span class="math inline">\(\mathbb{R}^n\)</span>, the standard
basis is åœ¨ <span class="math inline">\(\mathbb{R}^n\)</span>
ä¸­ï¼Œæ ‡å‡†ä¾æ®æ˜¯</p>
<p><span class="math display">\[
\mathbf{e}_1 = (1,0,\dots,0), \quad \mathbf{e}_2 = (0,1,0,\dots,0),
\dots, \mathbf{e}_n = (0,\dots,0,1).
\]</span></p>
<p>Relative to this basis, the coordinates of a vector are simply its
entries. Thus, column vectors are coordinate representations by default.
ç›¸å¯¹äºæ­¤åŸºï¼Œå‘é‡çš„åæ ‡ä»…ä»…æ˜¯å®ƒçš„å…ƒç´ ã€‚å› æ­¤ï¼Œåˆ—å‘é‡é»˜è®¤ä¸ºåæ ‡è¡¨ç¤ºã€‚</p>
<h3 id="change-of-basis">Change of Basis</h3>
<p>åŸºç¡€å˜æ›´</p>
<p>If <span class="math inline">\(\mathcal{B} = {\mathbf{v}_1, \dots,
\mathbf{v}_n}\)</span> is a basis of <span
class="math inline">\(\mathbb{R}^n\)</span>, the change of basis matrix
is å¦‚æœğµ = ğ‘£ 1 , â€¦ , ğ‘£ ğ‘› B=v 1 â€‹ ï¼Œâ€¦ï¼Œv n â€‹ æ˜¯ <span
class="math inline">\(\mathbb{R}^n\)</span> çš„åŸºï¼ŒåŸºçŸ©é˜µçš„å˜åŒ–æ˜¯</p>
<p><span class="math display">\[
P = \begin{bmatrix} \mathbf{v}_1 &amp; \mathbf{v}_2 &amp; \cdots &amp;
\mathbf{v}_n \end{bmatrix},
\]</span></p>
<p>with basis vectors as columns. For any vector <span
class="math inline">\(\mathbf{u}\)</span>, ä»¥åŸºå‘é‡ä¸ºåˆ—ã€‚å¯¹äºä»»æ„å‘é‡
<span class="math inline">\(\mathbf{u}\)</span> ï¼Œ</p>
<p><span class="math display">\[
\mathbf{u} = P [\mathbf{u}]_{\mathcal{B}}, \qquad
[\mathbf{u}]_{\mathcal{B}} = P^{-1}\mathbf{u}.
\]</span></p>
<p>Thus, switching between bases reduces to matrix multiplication.
å› æ­¤ï¼ŒåŸºæ•°ä¹‹é—´çš„åˆ‡æ¢å°±ç®€åŒ–ä¸ºçŸ©é˜µä¹˜æ³•ã€‚</p>
<h3 id="geometric-interpretation-6">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>Coordinates are the address of a vector relative to a chosen set of
directions. Different bases are like different coordinate systems:
Cartesian, rotated, skewed, or scaled. The same vector may look very
different numerically depending on the basis, but its geometric identity
is unchanged.
åæ ‡æ˜¯å‘é‡ç›¸å¯¹äºä¸€ç»„é€‰å®šæ–¹å‘çš„åœ°å€ã€‚ä¸åŒçš„åŸºå°±åƒä¸åŒçš„åæ ‡ç³»ï¼šç¬›å¡å°”åæ ‡ç³»ã€æ—‹è½¬åæ ‡ç³»ã€å€¾æ–œåæ ‡ç³»æˆ–ç¼©æ”¾åæ ‡ç³»ã€‚åŒä¸€ä¸ªå‘é‡åœ¨ä¸åŒåŸºä¸Šå¯èƒ½å‘ˆç°å‡ºæˆªç„¶ä¸åŒçš„æ•°å€¼ï¼Œä½†å…¶å‡ ä½•æ’ç­‰å¼ä¿æŒä¸å˜ã€‚</p>
<h3 id="why-this-matters-14">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Coordinates turn abstract vectors into concrete numerical data.
Changing basis is the algebraic language for rotations of axes,
diagonalization of matrices, and principal component analysis in data
science. Mastery of coordinates is essential for moving fluidly between
geometry, algebra, and computation.
åæ ‡å°†æŠ½è±¡å‘é‡è½¬åŒ–ä¸ºå…·ä½“çš„æ•°å€¼æ•°æ®ã€‚å˜æ¢åŸºæ˜¯æ•°æ®ç§‘å­¦ä¸­è½´æ—‹è½¬ã€çŸ©é˜µå¯¹è§’åŒ–å’Œä¸»æˆåˆ†åˆ†æçš„ä»£æ•°è¯­è¨€ã€‚æŒæ¡åæ ‡ç³»å¯¹äºåœ¨å‡ ä½•ã€ä»£æ•°å’Œè®¡ç®—ä¹‹é—´æµç•…åˆ‡æ¢è‡³å…³é‡è¦ã€‚</p>
<h3 id="exercises-4.4">Exercises 4.4</h3>
<p>ç»ƒä¹  4.4</p>
<ol type="1">
<li>Express <span class="math inline">\((4,2)\)</span> in terms of the
basis <span class="math inline">\((1,1), (1,-1)\)</span>. æ ¹æ®åŸºç¡€ <span
class="math inline">\((1,1), (1,-1)\)</span> è¡¨è¾¾ <span
class="math inline">\((4,2)\)</span> ã€‚</li>
<li>Find the coordinates of <span class="math inline">\((1,2,3)\)</span>
relative to the standard basis of <span
class="math inline">\(\mathbb{R}^3\)</span>. æ‰¾å‡º <span
class="math inline">\((1,2,3)\)</span> ç›¸å¯¹äº <span
class="math inline">\(\mathbb{R}^3\)</span> æ ‡å‡†åŸºçš„åæ ‡ã€‚</li>
<li>If <span class="math inline">\(\mathcal{B} = \{(2,0),
(0,3)\}\)</span>, compute <span class="math inline">\([ (4,6)
]_{\mathcal{B}}\)</span>. å¦‚æœ <span class="math inline">\(\mathcal{B} =
\{(2,0), (0,3)\}\)</span> ï¼Œåˆ™è®¡ç®— [ ( 4 , 6 ) ] ğµ [(4,6)] B â€‹ .</li>
<li>Construct the change of basis matrix from the standard basis of
<span class="math inline">\(\mathbb{R}^2\)</span> to <span
class="math inline">\(\mathcal{B} = \{(1,1), (1,-1)\}\)</span>.
æ„å»ºä»æ ‡å‡†åŸº <span class="math inline">\(\mathbb{R}^2\)</span> åˆ° <span
class="math inline">\(\mathcal{B} = \{(1,1), (1,-1)\}\)</span>
çš„åŸºå˜æ¢çŸ©é˜µã€‚</li>
<li>Prove that coordinate representation with respect to a basis is
unique. è¯æ˜å…³äºåŸºçš„åæ ‡è¡¨ç¤ºæ˜¯å”¯ä¸€çš„ã€‚</li>
</ol>
<h1 id="chapter-5.-linear-transformations">Chapter 5. Linear
Transformations</h1>
<p>ç¬¬äº”ç« çº¿æ€§å˜æ¢</p>
<h2 id="functions-that-preserve-linearity">5.1 Functions that Preserve
Linearity</h2>
<p>5.1 ä¿æŒçº¿æ€§çš„å‡½æ•°</p>
<p>A central theme of linear algebra is understanding linear
transformations: functions between vector spaces that preserve their
algebraic structure. These transformations generalize the idea of matrix
multiplication and capture the essence of linear behavior.
çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ç†è§£çº¿æ€§å˜æ¢ï¼šå‘é‡ç©ºé—´ä¹‹é—´ä¿æŒå…¶ä»£æ•°ç»“æ„çš„å‡½æ•°ã€‚è¿™äº›å˜æ¢æ¨å¹¿äº†çŸ©é˜µä¹˜æ³•çš„æ¦‚å¿µï¼Œå¹¶æŠ“ä½äº†çº¿æ€§è¡Œä¸ºçš„æœ¬è´¨ã€‚</p>
<h3 id="definition-2">Definition</h3>
<p>å®šä¹‰</p>
<p>Let <span class="math inline">\(V\)</span> and <span
class="math inline">\(W\)</span> be vector spaces over <span
class="math inline">\(\mathbb{R}\)</span>. A function ä»¤ <span
class="math inline">\(V\)</span> å’Œ <span
class="math inline">\(W\)</span> ä¸º <span
class="math inline">\(\mathbb{R}\)</span> ä¸Šçš„å‘é‡ç©ºé—´ã€‚å‡½æ•°</p>
<p><span class="math display">\[
T : V \to W
\]</span></p>
<p>is called a linear transformation (or linear map) if for all vectors
<span class="math inline">\(\mathbf{u}, \mathbf{v} \in V\)</span> and
all scalars <span class="math inline">\(c \in \mathbb{R}\)</span>:
å¦‚æœå¯¹äºæ‰€æœ‰å‘é‡ <span class="math inline">\(\mathbf{u}, \mathbf{v} \in
V\)</span> å’Œæ‰€æœ‰æ ‡é‡ <span class="math inline">\(c \in
\mathbb{R}\)</span> ï¼Œåˆ™ç§°ä¸ºçº¿æ€§å˜æ¢ï¼ˆæˆ–çº¿æ€§æ˜ å°„ï¼‰ï¼š</p>
<ol type="1">
<li>Additivity: åŠ æ€§ï¼š</li>
</ol>
<p><span class="math display">\[
T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}),
\]</span></p>
<ol start="2" type="1">
<li>Homogeneity: åŒè´¨æ€§ï¼š</li>
</ol>
<p><span class="math display">\[
T(c\mathbf{u}) = cT(\mathbf{u}).
\]</span></p>
<p>If both conditions hold, then <span class="math inline">\(T\)</span>
automatically respects linear combinations: å¦‚æœä¸¤ä¸ªæ¡ä»¶éƒ½æˆç«‹ï¼Œåˆ™ <span
class="math inline">\(T\)</span> è‡ªåŠ¨éµå¾ªçº¿æ€§ç»„åˆï¼š</p>
<p><span class="math display">\[
T(c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k) = c_1 T(\mathbf{v}_1) +
\cdots + c_k T(\mathbf{v}_k).
\]</span></p>
<h3 id="examples-3">Examples</h3>
<p>ç¤ºä¾‹</p>
<p>Example 5.1.1. Scaling in <span
class="math inline">\(\mathbb{R}^2\)</span>. Let <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span> be
defined by ä¾‹ 5.1.1. ç¼©æ”¾ <span
class="math inline">\(\mathbb{R}^2\)</span> ã€‚ä»¤ <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span>
å®šä¹‰ä¸º</p>
<p><span class="math display">\[
T(x,y) = (2x, 2y).
\]</span></p>
<p>This doubles the length of every vector, preserving direction. It is
linear. è¿™ä¼šä½¿æ¯ä¸ªå‘é‡çš„é•¿åº¦åŠ å€ï¼ŒåŒæ—¶ä¿æŒæ–¹å‘ä¸å˜ã€‚å®ƒæ˜¯çº¿æ€§çš„ã€‚</p>
<p>Example 5.1.2. Rotation. ä¾‹ 5.1.2. æ—‹è½¬ã€‚</p>
<p>Let <span class="math inline">\(R_\theta: \mathbb{R}^2 \to
\mathbb{R}^2\)</span> be ä»¤ <span class="math inline">\(R_\theta:
\mathbb{R}^2 \to \mathbb{R}^2\)</span> ä¸º</p>
<p><span class="math display">\[
R_\theta(x,y) = (x\cos\theta - y\sin\theta, \; x\sin\theta +
y\cos\theta).
\]</span></p>
<p>This rotates vectors by angle <span
class="math inline">\(\theta\)</span>. It satisfies additivity and
homogeneity, hence is linear. è¿™å°†å‘é‡æ—‹è½¬è§’åº¦ <span
class="math inline">\(\theta\)</span>
ã€‚å®ƒæ»¡è¶³å¯åŠ æ€§å’Œé½æ¬¡æ€§ï¼Œå› æ­¤æ˜¯çº¿æ€§çš„ã€‚</p>
<p>Example 5.1.3. Differentiation. ä¾‹ 5.1.3. åŒºåˆ†ã€‚</p>
<p>Let <span class="math inline">\(D: \mathbb{R}[x] \to
\mathbb{R}[x]\)</span> be differentiation: <span
class="math inline">\(D(p(x)) = p&#39;(x)\)</span>. ä»¤ <span
class="math inline">\(D: \mathbb{R}[x] \to \mathbb{R}[x]\)</span>
ä¸ºå¾®åˆ†ï¼š <span class="math inline">\(D(p(x)) = p&#39;(x)\)</span> ã€‚</p>
<p>Since derivatives respect addition and scalar multiples,
differentiation is a linear transformation.
ç”±äºå¯¼æ•°å°Šé‡åŠ æ³•å’Œæ ‡é‡å€æ•°ï¼Œå› æ­¤å¾®åˆ†æ˜¯ä¸€ç§çº¿æ€§å˜æ¢ã€‚</p>
<h3 id="non-example">Non-Example</h3>
<p>éç¤ºä¾‹</p>
<p>The map <span class="math inline">\(S:\mathbb{R}^2 \to
\mathbb{R}^2\)</span> defined by åœ°å›¾ <span
class="math inline">\(S:\mathbb{R}^2 \to \mathbb{R}^2\)</span>
å®šä¹‰ä¸º</p>
<p><span class="math display">\[
S(x,y) = (x^2, y^2)
\]</span></p>
<p>is not linear, because <span class="math inline">\(S(\mathbf{u} +
\mathbf{v}) \neq S(\mathbf{u}) + S(\mathbf{v})\)</span> in general.
ä¸æ˜¯çº¿æ€§çš„ï¼Œå› ä¸ºä¸€èˆ¬æ¥è¯´ <span class="math inline">\(S(\mathbf{u} +
\mathbf{v}) \neq S(\mathbf{u}) + S(\mathbf{v})\)</span> ã€‚</p>
<h3 id="geometric-interpretation-7">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>Linear transformations are exactly those that preserve the origin,
lines through the origin, and proportions along those lines. They
include familiar operations: scaling, rotations, reflections, shears,
and projections. Nonlinear transformations bend or curve space, breaking
these properties.
çº¿æ€§å˜æ¢æ­£æ˜¯é‚£äº›ä¿ç•™åŸç‚¹ã€è¿‡åŸç‚¹çš„ç›´çº¿ä»¥åŠæ²¿è¿™äº›ç›´çº¿çš„æ¯”ä¾‹çš„å˜æ¢ã€‚å®ƒä»¬åŒ…æ‹¬æˆ‘ä»¬ç†Ÿæ‚‰çš„æ“ä½œï¼šç¼©æ”¾ã€æ—‹è½¬ã€åå°„ã€å‰ªåˆ‡å’ŒæŠ•å½±ã€‚éçº¿æ€§å˜æ¢ä¼šå¼¯æ›²ç©ºé—´ï¼Œä»è€Œç ´åè¿™äº›å±æ€§ã€‚</p>
<h3 id="why-this-matters-15">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Linear transformations unify geometry, algebra, and computation. They
explain how matrices act on vectors, how data can be rotated or
projected, and how systems evolve under linear rules. Much of linear
algebra is devoted to understanding these transformations, their
representations, and their invariants.
çº¿æ€§å˜æ¢ç»Ÿä¸€äº†å‡ ä½•ã€ä»£æ•°å’Œè®¡ç®—ã€‚å®ƒè§£é‡Šäº†çŸ©é˜µå¦‚ä½•ä½œç”¨äºå‘é‡ï¼Œæ•°æ®å¦‚ä½•æ—‹è½¬æˆ–æŠ•å½±ï¼Œä»¥åŠç³»ç»Ÿå¦‚ä½•åœ¨çº¿æ€§è§„åˆ™ä¸‹æ¼”åŒ–ã€‚çº¿æ€§ä»£æ•°çš„å¤§éƒ¨åˆ†å†…å®¹è‡´åŠ›äºç†è§£è¿™äº›å˜æ¢ã€å®ƒä»¬çš„è¡¨ç¤ºåŠå…¶ä¸å˜é‡ã€‚</p>
<h3 id="exercises-5.1">Exercises 5.1</h3>
<p>ç»ƒä¹  5.1</p>
<ol type="1">
<li>Verify that <span class="math inline">\(T(x,y) = (3x-y, 2y)\)</span>
is a linear transformation on <span
class="math inline">\(\mathbb{R}^2\)</span>. éªŒè¯ <span
class="math inline">\(T(x,y) = (3x-y, 2y)\)</span> æ˜¯å¦æ˜¯ <span
class="math inline">\(\mathbb{R}^2\)</span> çš„çº¿æ€§å˜æ¢ã€‚</li>
<li>Show that <span class="math inline">\(T(x,y) = (x+1, y)\)</span> is
not linear. Which axiom fails? è¯æ˜ <span class="math inline">\(T(x,y) =
(x+1, y)\)</span> ä¸æ˜¯çº¿æ€§çš„ã€‚å“ªæ¡å…¬ç†ä¸æˆç«‹ï¼Ÿ</li>
<li>Prove that if <span class="math inline">\(T\)</span> and <span
class="math inline">\(S\)</span> are linear transformations, then so is
<span class="math inline">\(T+S\)</span>. è¯æ˜å¦‚æœ <span
class="math inline">\(T\)</span> å’Œ <span
class="math inline">\(S\)</span> æ˜¯çº¿æ€§å˜æ¢ï¼Œé‚£ä¹ˆ <span
class="math inline">\(T+S\)</span> ä¹Ÿæ˜¯çº¿æ€§å˜æ¢ã€‚</li>
<li>Give an example of a linear transformation from <span
class="math inline">\(\mathbb{R}^3\)</span> to <span
class="math inline">\(\mathbb{R}^2\)</span>. ç»™å‡ºä¸€ä¸ªä» <span
class="math inline">\(\mathbb{R}^3\)</span> åˆ° <span
class="math inline">\(\mathbb{R}^2\)</span> çš„çº¿æ€§å˜æ¢çš„ä¾‹å­ã€‚</li>
<li>Let <span class="math inline">\(T:\mathbb{R}[x] \to
\mathbb{R}[x]\)</span> be integration: ä»¤ <span
class="math inline">\(T:\mathbb{R}[x] \to \mathbb{R}[x]\)</span>
ä¸ºç§¯åˆ†ï¼š</li>
</ol>
<p><span class="math display">\[
T(p(x)) = \int_0^x p(t)\\,dt.
\]</span></p>
<p>Prove that <span class="math inline">\(T\)</span> is a linear
transformation. è¯æ˜ <span class="math inline">\(T\)</span>
æ˜¯çº¿æ€§å˜æ¢ã€‚</p>
<h2 id="matrix-representation-of-linear-maps">5.2 Matrix Representation
of Linear Maps</h2>
<p>5.2 çº¿æ€§æ˜ å°„çš„çŸ©é˜µè¡¨ç¤º</p>
<p>Every linear transformation between finite-dimensional vector spaces
can be represented by a matrix. This correspondence is one of the
central insights of linear algebra: it lets us use the tools of matrix
arithmetic to study abstract transformations.
æœ‰é™ç»´å‘é‡ç©ºé—´ä¹‹é—´çš„æ‰€æœ‰çº¿æ€§å˜æ¢éƒ½å¯ä»¥ç”¨çŸ©é˜µè¡¨ç¤ºã€‚è¿™ç§å¯¹åº”å…³ç³»æ˜¯çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒæ´è§ä¹‹ä¸€ï¼šå®ƒè®©æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨çŸ©é˜µè¿ç®—å·¥å…·æ¥ç ”ç©¶æŠ½è±¡çš„å˜æ¢ã€‚</p>
<h3 id="from-linear-map-to-matrix">From Linear Map to Matrix</h3>
<p>ä»çº¿æ€§æ˜ å°„åˆ°çŸ©é˜µ</p>
<p>Let <span class="math inline">\(T: \mathbb{R}^n \to
\mathbb{R}^m\)</span> be a linear transformation. Choose the standard
basis <span class="math inline">\(\{ \mathbf{e}_1, \dots, \mathbf{e}_n
\}\)</span> of <span class="math inline">\(\mathbb{R}^n\)</span>, where
<span class="math inline">\(\mathbf{e}_i\)</span> has a 1 in the <span
class="math inline">\(i\)</span>-th position and 0 elsewhere. ä»¤ <span
class="math inline">\(T: \mathbb{R}^n \to \mathbb{R}^m\)</span>
ä¸ºçº¿æ€§å˜æ¢ã€‚é€‰å– <span class="math inline">\(\mathbb{R}^n\)</span>
çš„æ ‡å‡†åŸº <span class="math inline">\(\{ \mathbf{e}_1, \dots,
\mathbf{e}_n \}\)</span> ï¼Œå…¶ä¸­ ğ‘’ ğ‘– e i â€‹ ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªä½ç½®ä¸º 1ï¼Œå…¶ä»–åœ°æ–¹ä¸º 0ã€‚</p>
<p>The action of <span class="math inline">\(T\)</span> on each basis
vector determines the entire transformation: <span
class="math inline">\(T\)</span> å¯¹æ¯ä¸ªåŸºå‘é‡çš„ä½œç”¨å†³å®šäº†æ•´ä¸ªå˜æ¢ï¼š</p>
<p><span class="math display">\[
T(\mathbf{e}\_j) = \begin{bmatrix}a_{1j} \\a_{2j} \\\vdots \\a_{mj}
\end{bmatrix}.
\]</span></p>
<p>Placing these outputs as columns gives the matrix of <span
class="math inline">\(T\)</span>: å°†è¿™äº›è¾“å‡ºä½œä¸ºåˆ—æ”¾ç½®ï¼Œå¾—åˆ°çŸ©é˜µ <span
class="math inline">\(T\)</span> ï¼š</p>
<p><span class="math display">\[
[T] = A = \begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}
\\a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\\vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\a_{m1} &amp; a_{m2} &amp; \cdots &amp;
a_{mn}\end{bmatrix}.
\]</span></p>
<p>Then for any vector <span class="math inline">\(\mathbf{x} \in
\mathbb{R}^n\)</span>: ç„¶åå¯¹äºä»»æ„å‘é‡ <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> ï¼š</p>
<p><span class="math display">\[
T(\mathbf{x}) = A\mathbf{x}.
\]</span></p>
<h3 id="examples-4">Examples</h3>
<p>ç¤ºä¾‹</p>
<p>Example 5.2.1. Scaling in <span
class="math inline">\(\mathbb{R}^2\)</span>. Let <span
class="math inline">\(T(x,y) = (2x, 3y)\)</span>. Then ä¾‹ 5.2.1. ç¼©æ”¾
<span class="math inline">\(\mathbb{R}^2\)</span> ã€‚è®¾ <span
class="math inline">\(T(x,y) = (2x, 3y)\)</span> ã€‚ç„¶å</p>
<p><span class="math display">\[
T(\mathbf{e}_1) = (2,0), \quad T(\mathbf{e}_2) = (0,3).
\]</span></p>
<p>So the matrix is æ‰€ä»¥çŸ©é˜µæ˜¯</p>
<p><span class="math display">\[
[T] = \begin{bmatrix}2 &amp; 0 \\0 &amp; 3\end{bmatrix}.
\]</span></p>
<p>Example 5.2.2. Rotation in the plane. The rotation transformation
<span class="math inline">\(R_\theta(x,y) = (x\cos\theta - y\sin\theta,
\; x\sin\theta + y\cos\theta)\)</span> has matrix ä¾‹5.2.2. å¹³é¢æ—‹è½¬ã€‚
æ—‹è½¬å˜æ¢ <span class="math inline">\(R_\theta(x,y) = (x\cos\theta -
y\sin\theta, \; x\sin\theta + y\cos\theta)\)</span> å…·æœ‰çŸ©é˜µ</p>
<p><span class="math display">\[
[R_\theta] = \begin{bmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta
&amp; \cos\theta\end{bmatrix}.
\]</span></p>
<p>Example 5.2.3. Projection onto the x-axis. The map <span
class="math inline">\(P(x,y) = (x,0)\)</span> corresponds to ä¾‹ 5.2.3.
æŠ•å½±åˆ° x è½´ã€‚ åœ°å›¾ <span class="math inline">\(P(x,y) = (x,0)\)</span>
å¯¹åº”äº</p>
<p><span class="math display">\[
[P] = \begin{bmatrix}1 &amp; 0 \\0 &amp; 0\end{bmatrix}.
\]</span></p>
<h3 id="change-of-basis-1">Change of Basis</h3>
<p>åŸºç¡€å˜æ›´</p>
<p>Matrix representations depend on the chosen basis. If <span
class="math inline">\(\mathcal{B}\)</span> and <span
class="math inline">\(\mathcal{C}\)</span> are bases of <span
class="math inline">\(\mathbb{R}^n\)</span> and <span
class="math inline">\(\mathbb{R}^m\)</span>, then the matrix of <span
class="math inline">\(T: \mathbb{R}^n \to \mathbb{R}^m\)</span> with
respect to these bases is obtained by expressing <span
class="math inline">\(T(\mathbf{v}_j)\)</span> in terms of <span
class="math inline">\(\mathcal{C}\)</span> for each <span
class="math inline">\(\mathbf{v}_j \in \mathcal{B}\)</span>. Changing
bases corresponds to conjugating the matrix by the appropriate
change-of-basis matrices. çŸ©é˜µè¡¨ç¤ºå–å†³äºæ‰€é€‰çš„åŸºã€‚å¦‚æœ <span
class="math inline">\(\mathcal{B}\)</span> å’Œ <span
class="math inline">\(\mathcal{C}\)</span> æ˜¯ <span
class="math inline">\(\mathbb{R}^n\)</span> çš„åŸº å’Œ <span
class="math inline">\(\mathbb{R}^m\)</span> ï¼Œåˆ™ <span
class="math inline">\(T: \mathbb{R}^n \to \mathbb{R}^m\)</span>
å…³äºè¿™äº›åŸºçš„çŸ©é˜µï¼Œå¯ä»¥é€šè¿‡å°† <span
class="math inline">\(T(\mathbf{v}_j)\)</span> è¡¨ç¤ºä¸º <span
class="math inline">\(\mathcal{C}\)</span> æ¥è·å¾—ï¼Œå…¶ä¸­ <span
class="math inline">\(\mathbf{v}_j \in \mathcal{B}\)</span> è¡¨ç¤ºä¸º <span
class="math inline">\(T(\mathbf{v}_j)\)</span>ã€‚æ”¹å˜åŸºç›¸å½“äºå°†çŸ©é˜µä¸é€‚å½“çš„åŸºå˜æ¢çŸ©é˜µå…±è½­ã€‚</p>
<h3 id="geometric-interpretation-8">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>Matrices are not just convenient notation-they <em>are</em> linear
maps once a basis is fixed. Every rotation, reflection, projection,
shear, or scaling corresponds to multiplying by a specific matrix. Thus,
studying linear transformations reduces to studying their matrices.
çŸ©é˜µä¸ä»…ä»…æ˜¯æ–¹ä¾¿çš„ç¬¦å·â€”â€”ä¸€æ—¦åŸºç¡®å®šï¼Œå®ƒä»¬<em>å°±æ˜¯</em>çº¿æ€§æ˜ å°„ã€‚æ‰€æœ‰æ—‹è½¬ã€åå°„ã€æŠ•å½±ã€å‰ªåˆ‡æˆ–ç¼©æ”¾éƒ½å¯¹åº”äºä¹˜ä»¥ä¸€ä¸ªç‰¹å®šçš„çŸ©é˜µã€‚å› æ­¤ï¼Œç ”ç©¶çº¿æ€§å˜æ¢å¯ä»¥å½’ç»“ä¸ºç ”ç©¶å®ƒä»¬çš„çŸ©é˜µã€‚</p>
<h3 id="why-this-matters-16">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Matrix representations make linear transformations computable. They
connect abstract definitions to explicit calculations, enabling
algorithms for solving systems, finding eigenvalues, and performing
decompositions. Applications from graphics to machine learning depend on
this translation.
çŸ©é˜µè¡¨ç¤ºä½¿çº¿æ€§å˜æ¢å¯è®¡ç®—ã€‚å®ƒä»¬å°†æŠ½è±¡å®šä¹‰ä¸æ˜ç¡®çš„è®¡ç®—è”ç³»èµ·æ¥ï¼Œä»è€Œæ”¯æŒæ±‚è§£ç³»ç»Ÿã€æŸ¥æ‰¾ç‰¹å¾å€¼å’Œæ‰§è¡Œåˆ†è§£çš„ç®—æ³•ã€‚ä»å›¾å½¢åˆ°æœºå™¨å­¦ä¹ ç­‰å„ç§åº”ç”¨éƒ½ä¾èµ–äºè¿™ç§è½¬æ¢ã€‚</p>
<h3 id="exercises-5.2">Exercises 5.2</h3>
<p>ç»ƒä¹  5.2</p>
<ol type="1">
<li>Find the matrix representation of <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span>, <span
class="math inline">\(T(x,y) = (x+y, x-y)\)</span>. æ‰¾åˆ° <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span> , <span
class="math inline">\(T(x,y) = (x+y, x-y)\)</span> çš„çŸ©é˜µè¡¨ç¤ºã€‚</li>
<li>Determine the matrix of the linear transformation <span
class="math inline">\(T:\mathbb{R}^3 \to \mathbb{R}^2\)</span>, <span
class="math inline">\(T(x,y,z) = (x+z, y-2z)\)</span>. ç¡®å®šçº¿æ€§å˜æ¢çŸ©é˜µ
<span class="math inline">\(T:\mathbb{R}^3 \to \mathbb{R}^2\)</span> ï¼Œ
<span class="math inline">\(T(x,y,z) = (x+z, y-2z)\)</span> ã€‚</li>
<li>What matrix represents reflection across the line <span
class="math inline">\(y=x\)</span> in <span
class="math inline">\(\mathbb{R}^2\)</span>? å“ªä¸ªçŸ©é˜µè¡¨ç¤º <span
class="math inline">\(\mathbb{R}^2\)</span> ä¸­æ²¿çº¿ <span
class="math inline">\(y=x\)</span> çš„åå°„ï¼Ÿ</li>
<li>Show that the matrix of the identity transformation on <span
class="math inline">\(\mathbb{R}^n\)</span> is <span
class="math inline">\(I_n\)</span>. è¯æ˜ <span
class="math inline">\(\mathbb{R}^n\)</span> ä¸Šçš„æ’ç­‰å˜æ¢çŸ©é˜µæ˜¯ ğ¼ ğ‘› I n â€‹
.</li>
<li>For the differentiation map <span
class="math inline">\(D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]\)</span>,
where <span class="math inline">\(\mathbb{R}_k[x]\)</span> is the space
of polynomials of degree at most <span class="math inline">\(k\)</span>,
find the matrix of <span class="math inline">\(D\)</span> relative to
the bases <span class="math inline">\(\{1,x,x^2\}\)</span> and <span
class="math inline">\(\{1,x\}\)</span>. å¯¹äºå¾®åˆ†æ˜ å°„ <span
class="math inline">\(D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]\)</span>
ï¼Œå…¶ä¸­ <span class="math inline">\(\mathbb{R}_k[x]\)</span> æ˜¯æ¬¡æ•°æœ€å¤šä¸º
<span class="math inline">\(k\)</span> çš„å¤šé¡¹å¼ç©ºé—´ï¼Œæ±‚å‡º <span
class="math inline">\(D\)</span> ç›¸å¯¹äºåŸºæ•° <span
class="math inline">\(\{1,x,x^2\}\)</span> å’Œ <span
class="math inline">\(\{1,x\}\)</span> çš„çŸ©é˜µã€‚</li>
</ol>
<h2 id="kernel-and-image">5.3 Kernel and Image</h2>
<p>5.3 å†…æ ¸å’Œé•œåƒ</p>
<p>To understand a linear transformation deeply, we must examine what it
kills and what it produces. These ideas are captured by the kernel and
the image, two fundamental subspaces associated with any linear map.
è¦æ·±å…¥ç†è§£çº¿æ€§å˜æ¢ï¼Œæˆ‘ä»¬å¿…é¡»è€ƒå¯Ÿå®ƒæ¶ˆé™¤äº†ä»€ä¹ˆï¼Œåˆäº§ç”Ÿäº†ä»€ä¹ˆã€‚è¿™äº›æ¦‚å¿µå¯ä»¥é€šè¿‡æ ¸å’Œåƒæ¥ç†è§£ï¼Œå®ƒä»¬æ˜¯ä»»ä½•çº¿æ€§æ˜ å°„éƒ½ç›¸å…³çš„ä¸¤ä¸ªåŸºæœ¬å­ç©ºé—´ã€‚</p>
<h3 id="the-kernel">The Kernel</h3>
<p>å†…æ ¸</p>
<p>The kernel (or null space) of a linear transformation <span
class="math inline">\(T: V \to W\)</span> is the set of all vectors in
<span class="math inline">\(V\)</span> that map to the zero vector in
<span class="math inline">\(W\)</span>: çº¿æ€§å˜æ¢ <span
class="math inline">\(T: V \to W\)</span> çš„æ ¸ï¼ˆæˆ–é›¶ç©ºé—´ï¼‰æ˜¯ <span
class="math inline">\(V\)</span> ä¸­æ˜ å°„åˆ° <span
class="math inline">\(W\)</span> ä¸­çš„é›¶å‘é‡çš„æ‰€æœ‰å‘é‡çš„é›†åˆï¼š</p>
<p><span class="math display">\[
\ker(T) = \{ \mathbf{v} \in V \mid T(\mathbf{v}) = \mathbf{0} \}.
\]</span></p>
<p>The kernel is always a subspace of <span
class="math inline">\(V\)</span>. It measures the degeneracy of the
transformation-directions that collapse to nothing. æ ¸å§‹ç»ˆæ˜¯ <span
class="math inline">\(V\)</span>
çš„å­ç©ºé—´ã€‚å®ƒè¡¡é‡çš„æ˜¯åç¼©ä¸ºé›¶çš„å˜æ¢æ–¹å‘çš„é€€åŒ–ç¨‹åº¦ã€‚</p>
<p>Example 5.3.1. Let <span class="math inline">\(T:\mathbb{R}^3 \to
\mathbb{R}^2\)</span> be defined by ä¾‹ 5.3.1ã€‚ è®© <span
class="math inline">\(T:\mathbb{R}^3 \to \mathbb{R}^2\)</span>
å®šä¹‰ä¸º</p>
<p><span class="math display">\[
T(x,y,z) = (x+y, y+z).
\]</span></p>
<p>In matrix form, ä»¥çŸ©é˜µå½¢å¼ï¼Œ</p>
<p><span class="math display">\[
[T] = \begin{bmatrix}1 &amp; 1 &amp; 0 \\0 &amp; 1 &amp; 1\end{bmatrix}.
\]</span></p>
<p>To find the kernel, solve è¦æ‰¾åˆ°å†…æ ¸ï¼Œè¯·è§£å†³</p>
<p><span class="math display">\[
\begin{bmatrix}1 &amp; 1 &amp; 0 \\0 &amp; 1 &amp;
1\end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix}= \begin{bmatrix}
0 \\ 0 \end{bmatrix}.
\]</span></p>
<p>This gives the equations <span class="math inline">\(x + y =
0\)</span>, <span class="math inline">\(y + z = 0\)</span>. Hence <span
class="math inline">\(x = -y, z = -y\)</span>. The kernel is
ç”±æ­¤å¾—åˆ°æ–¹ç¨‹ <span class="math inline">\(x + y = 0\)</span> ï¼Œ <span
class="math inline">\(y + z = 0\)</span> ã€‚å› æ­¤ <span
class="math inline">\(x = -y, z = -y\)</span> ã€‚æ ¸å‡½æ•°ä¸º</p>
<p><span class="math display">\[
\ker(T) = \{ (-t, t, -t) \mid t \in \mathbb{R} \},
\]</span></p>
<p>a line in <span class="math inline">\(\mathbb{R}^3\)</span>. <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­çš„ä¸€è¡Œã€‚</p>
<h3 id="the-image">The Image</h3>
<p>å›¾åƒ</p>
<p>The image (or range) of a linear transformation <span
class="math inline">\(T: V \to W\)</span> is the set of all outputs:
çº¿æ€§å˜æ¢ <span class="math inline">\(T: V \to W\)</span>
çš„å›¾åƒï¼ˆæˆ–èŒƒå›´ï¼‰æ˜¯æ‰€æœ‰è¾“å‡ºçš„é›†åˆï¼š</p>
<p><span class="math display">\[
\text{im}(T) = \{ T(\mathbf{v}) \mid \mathbf{v} \in V \} \subseteq W.
\]</span></p>
<p>Equivalently, it is the span of the columns of the representing
matrix. The image is always a subspace of <span
class="math inline">\(W\)</span>.
ç­‰æ•ˆåœ°ï¼Œå®ƒæ˜¯è¡¨ç¤ºçŸ©é˜µçš„åˆ—çš„è·¨åº¦ã€‚å›¾åƒå§‹ç»ˆæ˜¯ <span
class="math inline">\(W\)</span> çš„å­ç©ºé—´ã€‚</p>
<p>Example 5.3.2. For the same transformation as above, ä¾‹ 5.3.2.
å¯¹äºä¸ä¸Šè¿°ç›¸åŒçš„å˜æ¢ï¼Œ</p>
<p><span class="math display">\[
[T] = \begin{bmatrix}1 &amp; 1 &amp; 0 \\0 &amp; 1 &amp; 1\end{bmatrix},
\]</span></p>
<p>the columns are <span class="math inline">\((1,0)\)</span>, <span
class="math inline">\((1,1)\)</span>, and <span
class="math inline">\((0,1)\)</span>. Since <span
class="math inline">\((1,1) = (1,0) + (0,1)\)</span>, the image is åˆ—ä¸º
<span class="math inline">\((1,0)\)</span> ã€ <span
class="math inline">\((1,1)\)</span> å’Œ <span
class="math inline">\((0,1)\)</span> ã€‚ç”±äº <span
class="math inline">\((1,1) = (1,0) + (0,1)\)</span> ï¼Œå› æ­¤å›¾åƒä¸º</p>
<p><span class="math display">\[
\text{im}(T) = \text{span}\{ (1,0), (0,1) \} = \mathbb{R}^2.
\]</span></p>
<h3 id="dimension-formula-ranknullity-theorem">Dimension Formula
(Rankâ€“Nullity Theorem)</h3>
<p>ç»´åº¦å…¬å¼ï¼ˆç§©-é›¶åº¦å®šç†ï¼‰</p>
<p>For a linear transformation <span class="math inline">\(T: V \to
W\)</span> with <span class="math inline">\(V\)</span>
finite-dimensional, å¯¹äºçº¿æ€§å˜æ¢ <span class="math inline">\(T: V \to
W\)</span> ä¸” <span class="math inline">\(V\)</span> ä¸ºæœ‰é™ç»´ï¼Œ</p>
<p><span class="math display">\[
\dim(\ker(T)) + \dim(\text{im}(T)) = \dim(V).
\]</span></p>
<p>This fundamental result connects the lost directions (kernel) with
the achieved directions (image).
è¿™ä¸ªåŸºæœ¬ç»“æœå°†ä¸¢å¤±çš„æ–¹å‘ï¼ˆå†…æ ¸ï¼‰ä¸å®ç°çš„æ–¹å‘ï¼ˆå›¾åƒï¼‰è”ç³»èµ·æ¥ã€‚</p>
<h3 id="geometric-interpretation-9">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<ul>
<li>The kernel describes how the transformation flattens space (e.g.,
projecting a 3D object onto a plane).
å†…æ ¸æè¿°äº†å˜æ¢å¦‚ä½•ä½¿ç©ºé—´å˜å¹³å¦ï¼ˆä¾‹å¦‚ï¼Œå°† 3D å¯¹è±¡æŠ•å½±åˆ°å¹³é¢ä¸Šï¼‰ã€‚</li>
<li>The image describes the target subspace reached by the
transformation. è¯¥å›¾åƒæè¿°äº†å˜æ¢æ‰€è¾¾åˆ°çš„ç›®æ ‡å­ç©ºé—´ã€‚</li>
<li>The rankâ€“nullity theorem quantifies the tradeoff: the more
dimensions collapse, the fewer remain in the image.
ç§©é›¶å®šç†é‡åŒ–äº†è¿™ç§æƒè¡¡ï¼šç»´åº¦å´©æºƒå¾—è¶Šå¤šï¼Œå›¾åƒä¸­å‰©ä½™çš„ç»´åº¦å°±è¶Šå°‘ã€‚</li>
</ul>
<h3 id="why-this-matters-17">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Kernel and image capture the essence of a linear map. They classify
transformations, explain when systems have unique or infinite solutions,
and form the backbone of important results like the Rankâ€“Nullity
Theorem, diagonalization, and spectral theory.
æ ¸å’Œå›¾åƒæ•æ‰äº†çº¿æ€§æ˜ å°„çš„æœ¬è´¨ã€‚å®ƒä»¬å¯¹å˜æ¢è¿›è¡Œåˆ†ç±»ï¼Œè§£é‡Šç³»ç»Ÿä½•æ—¶å…·æœ‰å”¯ä¸€æˆ–æ— é™è§£ï¼Œå¹¶æ„æˆç§©é›¶å®šç†ã€å¯¹è§’åŒ–å’Œè°±ç†è®ºç­‰é‡è¦ç»“æœçš„æ”¯æŸ±ã€‚</p>
<h3 id="exercises-5.3">Exercises 5.3</h3>
<p>ç»ƒä¹  5.3</p>
<ol type="1">
<li>Find the kernel and image of <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span>, <span
class="math inline">\(T(x,y) = (x-y, x+y)\)</span>. æŸ¥æ‰¾ <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span> ã€ <span
class="math inline">\(T(x,y) = (x-y, x+y)\)</span> çš„æ ¸å’Œå›¾åƒã€‚</li>
<li>Let è®©</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; 1 &amp; 4 \end{bmatrix}
\]</span></p>
<p>Find bases for <span class="math inline">\(\ker(A)\)</span> and <span
class="math inline">\(\text{im}(A)\)</span>. 3. For the projection map
<span class="math inline">\(P(x,y,z) = (x,y,0)\)</span>, describe the
kernel and image. 4. Prove that <span
class="math inline">\(\ker(T)\)</span> and <span
class="math inline">\(\text{im}(T)\)</span> are always subspaces. 5.
Verify the Rankâ€“Nullity Theorem for the transformation in Example 5.3.1.
æ‰¾åˆ° <span class="math inline">\(\ker(A)\)</span> å’Œ <span
class="math inline">\(\text{im}(A)\)</span> çš„åŸºã€‚3. å¯¹äºæŠ•å½±å›¾ <span
class="math inline">\(P(x,y,z) = (x,y,0)\)</span> ï¼Œæè¿°å…¶æ ¸å’Œå›¾åƒã€‚4.
è¯æ˜ <span class="math inline">\(\ker(T)\)</span> å’Œ <span
class="math inline">\(\text{im}(T)\)</span> å§‹ç»ˆæ˜¯å­ç©ºé—´ã€‚5.
éªŒè¯ç¤ºä¾‹5.3.1ä¸­å˜æ¢çš„ç§©é›¶æ€§å®šç†ã€‚</p>
<h2 id="change-of-basis-2">5.4 Change of Basis</h2>
<p>5.4 åŸºç¡€å˜æ›´</p>
<p>Linear transformations can look very different depending on the
coordinate system we use. The process of rewriting vectors and
transformations relative to a new basis is called a change of basis.
This concept lies at the heart of diagonalization, orthogonalization,
and many computational techniques.
æ ¹æ®æˆ‘ä»¬ä½¿ç”¨çš„åæ ‡ç³»ï¼Œçº¿æ€§å˜æ¢çœ‹èµ·æ¥å¯èƒ½éå¸¸ä¸åŒã€‚ç›¸å¯¹äºæ–°çš„åŸºé‡å†™å‘é‡å’Œå˜æ¢çš„è¿‡ç¨‹ç§°ä¸ºåŸºå˜æ¢ã€‚è¿™ä¸ªæ¦‚å¿µæ˜¯å¯¹è§’åŒ–ã€æ­£äº¤åŒ–ä»¥åŠè®¸å¤šè®¡ç®—æŠ€æœ¯çš„æ ¸å¿ƒã€‚</p>
<h3 id="coordinate-change">Coordinate Change</h3>
<p>åæ ‡å˜æ¢</p>
<p>Suppose <span class="math inline">\(V\)</span> is an <span
class="math inline">\(n\)</span>-dimensional vector space, and let <span
class="math inline">\(\mathcal{B} = \{\mathbf{v}_1, \dots,
\mathbf{v}_n\}\)</span> be a basis. Every vector <span
class="math inline">\(\mathbf{x} \in V\)</span> has a coordinate vector
<span class="math inline">\([\mathbf{x}]_{\mathcal{B}} \in
\mathbb{R}^n\)</span>. å‡è®¾ <span class="math inline">\(V\)</span>
æ˜¯ä¸€ä¸ª <span class="math inline">\(n\)</span> ç»´å‘é‡ç©ºé—´ï¼Œè®¾ <span
class="math inline">\(\mathcal{B} = \{\mathbf{v}_1, \dots,
\mathbf{v}_n\}\)</span> ä¸ºåŸºã€‚æ¯ä¸ªå‘é‡ <span
class="math inline">\(\mathbf{x} \in V\)</span> éƒ½æœ‰ä¸€ä¸ªåæ ‡å‘é‡ <span
class="math inline">\([\mathbf{x}]_{\mathcal{B}} \in
\mathbb{R}^n\)</span> ã€‚</p>
<p>If <span class="math inline">\(P\)</span> is the change-of-basis
matrix from <span class="math inline">\(\mathcal{B}\)</span> to the
standard basis, then å¦‚æœ <span class="math inline">\(P\)</span> æ˜¯ä»
<span class="math inline">\(\mathcal{B}\)</span>
åˆ°æ ‡å‡†åŸºçš„åŸºå˜æ¢çŸ©é˜µï¼Œåˆ™</p>
<p><span class="math display">\[
\mathbf{x} = P [\mathbf{x}]_{\mathcal{B}}.
\]</span></p>
<p>Equivalently, ç­‰ä»·åœ°ï¼Œ</p>
<p><span class="math display">\[
[\mathbf{x}]_{\mathcal{B}} = P^{-1} \mathbf{x}.
\]</span></p>
<p>Here, <span class="math inline">\(P\)</span> has the basis vectors of
<span class="math inline">\(\mathcal{B}\)</span> as its columns: è¿™é‡Œï¼Œ
<span class="math inline">\(P\)</span> ä»¥ <span
class="math inline">\(\mathcal{B}\)</span> çš„åŸºå‘é‡ä½œä¸ºå…¶åˆ—ï¼š</p>
<p><span class="math display">\[
P = \begin{bmatrix}\mathbf{v}_1 &amp; \mathbf{v}_2 &amp; \cdots &amp;
\mathbf{v}_n\end{bmatrix}.
\]</span></p>
<h3 id="transformation-of-matrices">Transformation of Matrices</h3>
<p>çŸ©é˜µå˜æ¢</p>
<p>Let <span class="math inline">\(T: V \to V\)</span> be a linear
transformation. Suppose its matrix in the standard basis is <span
class="math inline">\(A\)</span>. In the basis <span
class="math inline">\(\mathcal{B}\)</span>, the representing matrix
becomes ä»¤ <span class="math inline">\(T: V \to V\)</span>
ä¸ºçº¿æ€§å˜æ¢ã€‚å‡è®¾å…¶åœ¨æ ‡å‡†åŸºä¸­çš„çŸ©é˜µä¸º <span
class="math inline">\(A\)</span> ã€‚åœ¨åŸº <span
class="math inline">\(\mathcal{B}\)</span> ä¸­ï¼Œè¡¨ç¤ºçŸ©é˜µå˜ä¸º</p>
<p><span class="math display">\[
[T]_{\mathcal{B}} = P^{-1} A P.
\]</span></p>
<p>Thus, changing basis corresponds to a similarity transformation of
the matrix. å› æ­¤ï¼Œæ”¹å˜åŸºç¡€å¯¹åº”äºçŸ©é˜µçš„ç›¸ä¼¼å˜æ¢ã€‚</p>
<h3 id="example-3">Example</h3>
<p>ä¾‹å­</p>
<p>Example 5.4.1. Let <span class="math inline">\(T:\mathbb{R}^2 \to
\mathbb{R}^2\)</span> be given by ä¾‹ 5.4.1ã€‚ ä»¤ <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span> ä¸º</p>
<p><span class="math display">\[
T(x,y) = (3x + y, x + y).
\]</span></p>
<p>In the standard basis, its matrix is åœ¨æ ‡å‡†åŸºç¡€ä¸Šï¼Œå…¶çŸ©é˜µä¸º</p>
<p><span class="math display">\[
A = \begin{bmatrix}3 &amp; 1 \\1 &amp; 1\end{bmatrix}.
\]</span></p>
<p>Now consider the basis <span class="math inline">\(\mathcal{B} = \{
(1,1), (1,-1) \}\)</span>. The change-of-basis matrix is ç°åœ¨è€ƒè™‘åŸº
<span class="math inline">\(\mathcal{B} = \{ (1,1), (1,-1) \}\)</span>
ã€‚åŸºå˜æ¢çŸ©é˜µä¸º</p>
<p><span class="math display">\[
P = \begin{bmatrix}1 &amp; 1 \\1 &amp; -1\end{bmatrix}.
\]</span></p>
<p>Then ç„¶å</p>
<p><span class="math display">\[
[T]_{\mathcal{B}} = P^{-1} A P.
\]</span></p>
<p>Computing gives è®¡ç®—å¾—å‡º</p>
<p><span class="math display">\[
[T]_{\mathcal{B}} =\begin{bmatrix}4 &amp; 0 \\0 &amp; 0\end{bmatrix}.
\]</span></p>
<p>In this new basis, the transformation is diagonal: one direction is
scaled by 4, the other collapsed to 0.
åœ¨è¿™ä¸ªæ–°çš„åŸºç¡€ä¸Šï¼Œå˜æ¢æ˜¯å¯¹è§’çš„ï¼šä¸€ä¸ªæ–¹å‘ç¼©æ”¾ 4ï¼Œå¦ä¸€ä¸ªæ–¹å‘æŠ˜å ä¸º 0ã€‚</p>
<h3 id="geometric-interpretation-10">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>Change of basis is like rotating or skewing your coordinate grid. The
underlying transformation does not change, but its description in
numbers becomes simpler or more complicated depending on the basis.
Finding a basis that simplifies a transformation (often a diagonal
basis) is a key theme in linear algebra.
åŸºå˜æ¢å°±åƒæ—‹è½¬æˆ–å€¾æ–œåæ ‡ç½‘æ ¼ã€‚åº•å±‚å˜æ¢æœ¬èº«ä¸ä¼šæ”¹å˜ï¼Œä½†å…¶æ•°å€¼æè¿°ä¼šæ ¹æ®åŸºçš„å˜åŒ–è€Œå˜å¾—æ›´ç®€å•æˆ–æ›´å¤æ‚ã€‚å¯»æ‰¾èƒ½å¤Ÿç®€åŒ–å˜æ¢çš„åŸºï¼ˆé€šå¸¸æ˜¯å¯¹è§’åŸºï¼‰æ˜¯çº¿æ€§ä»£æ•°çš„ä¸€ä¸ªå…³é”®ä¸»é¢˜ã€‚</p>
<h3 id="why-this-matters-18">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Change of basis connects the abstract notion of similarity to
practical computation. It is the tool that allows us to diagonalize
matrices, compute eigenvalues, and simplify complex transformations. In
applications, it corresponds to choosing a more natural coordinate
system-whether in geometry, physics, or machine learning.
åŸºå˜æ¢å°†ç›¸ä¼¼æ€§çš„æŠ½è±¡æ¦‚å¿µä¸å®é™…è®¡ç®—è”ç³»èµ·æ¥ã€‚å®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿå¯¹çŸ©é˜µè¿›è¡Œå¯¹è§’åŒ–ã€è®¡ç®—ç‰¹å¾å€¼å¹¶ç®€åŒ–å¤æ‚çš„å˜æ¢ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå®ƒç›¸å½“äºé€‰æ‹©ä¸€ä¸ªæ›´è‡ªç„¶çš„åæ ‡ç³»â€”â€”æ— è®ºæ˜¯åœ¨å‡ ä½•ã€ç‰©ç†è¿˜æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸã€‚</p>
<h3 id="exercises-5.4">Exercises 5.4</h3>
<p>ç»ƒä¹  5.4</p>
<ol type="1">
<li>Let è®©</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 1 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<p>Compute its representation in the basis <span
class="math inline">\(\{(1,0),(1,1)\}\)</span>. 2. Find the
change-of-basis matrix from the standard basis of <span
class="math inline">\(\mathbb{R}^2\)</span> to <span
class="math inline">\(\{(2,1),(1,1)\}\)</span>. 3. Prove that similar
matrices (related by <span class="math inline">\(P^{-1}AP\)</span>)
represent the same linear transformation under different bases. 4.
Diagonalize the matrix è®¡ç®—å…¶åœ¨åŸº <span
class="math inline">\(\{(1,0),(1,1)\}\)</span> ä¸­çš„è¡¨ç¤ºã€‚2. æ±‚å‡ºä» <span
class="math inline">\(\mathbb{R}^2\)</span> åˆ° <span
class="math inline">\(\{(2,1),(1,1)\}\)</span> çš„æ ‡å‡†åŸºå˜æ¢çŸ©é˜µã€‚3.
è¯æ˜ç›¸ä¼¼çš„çŸ©é˜µï¼ˆç”± <span class="math inline">\(P^{-1}AP\)</span>
å…³è”ï¼‰åœ¨ä¸åŒåŸºä¸‹è¡¨ç¤ºç›¸åŒçš„çº¿æ€§å˜æ¢ã€‚4. å¯¹è§’åŒ–çŸ©é˜µ</p>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}
\]</span></p>
<p>in the basis <span class="math inline">\(\{(1,1),(1,-1)\}\)</span>.
5. In <span class="math inline">\(\mathbb{R}^3\)</span>, let <span
class="math inline">\(\mathcal{B} =
\{(1,0,0),(1,1,0),(1,1,1)\}\)</span>. Construct the change-of-basis
matrix <span class="math inline">\(P\)</span> and compute <span
class="math inline">\(P^{-1}\)</span>. åœ¨åŸº <span
class="math inline">\(\{(1,1),(1,-1)\}\)</span> ä¸­ã€‚5. åœ¨ <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­ï¼Œä»¤ <span
class="math inline">\(\mathcal{B} = \{(1,0,0),(1,1,0),(1,1,1)\}\)</span>
ã€‚æ„é€ åŸºå˜æ¢çŸ©é˜µ <span class="math inline">\(P\)</span> å¹¶è®¡ç®— <span
class="math inline">\(P^{-1}\)</span> ã€‚</p>
<h1 id="chapter-6.-determinants">Chapter 6. Determinants</h1>
<p>ç¬¬å…­ç«  å†³å®šå› ç´ </p>
<h2 id="motivation-and-geometric-meaning">6.1 Motivation and Geometric
Meaning</h2>
<p>6.1 åŠ¨æœºå’Œå‡ ä½•æ„ä¹‰</p>
<p>Determinants are numerical values associated with square matrices. At
first they may appear as a complicated formula, but their importance
comes from what they measure: determinants encode scaling, orientation,
and invertibility of linear transformations. They bridge algebra and
geometry.
è¡Œåˆ—å¼æ˜¯ä¸æ–¹é˜µç›¸å…³çš„æ•°å€¼ã€‚ä¹ä¸€çœ‹ï¼Œå®ƒä»¬å¯èƒ½çœ‹èµ·æ¥åƒä¸€ä¸ªå¤æ‚çš„å…¬å¼ï¼Œä½†å®ƒä»¬çš„é‡è¦æ€§åœ¨äºå®ƒä»¬æ‰€æµ‹é‡çš„å†…å®¹ï¼šè¡Œåˆ—å¼ç¼–ç äº†çº¿æ€§å˜æ¢çš„ç¼©æ”¾ã€æ–¹å‘å’Œå¯é€†æ€§ã€‚å®ƒä»¬è¿æ¥äº†ä»£æ•°å’Œå‡ ä½•ã€‚</p>
<h3 id="determinants-of-22-matrices">Determinants of 2Ã—2 Matrices</h3>
<p>2Ã—2 çŸ©é˜µçš„è¡Œåˆ—å¼</p>
<p>For a 2Ã—2 matrix å¯¹äº 2Ã—2 çŸ©é˜µ</p>
<p><span class="math display">\[
A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix},
\]</span></p>
<p>the determinant is defined as è¡Œåˆ—å¼å®šä¹‰ä¸º</p>
<p><span class="math display">\[
\det(A) = ad - bc.
\]</span></p>
<p>Geometric meaning: If <span class="math inline">\(A\)</span>
represents a linear transformation of the plane, then <span
class="math inline">\(|\det(A)|\)</span> is the area scaling factor. For
example, if <span class="math inline">\(\det(A) = 2\)</span>, areas of
shapes are doubled. If <span class="math inline">\(\det(A) = 0\)</span>,
the transformation collapses the plane to a line: all area is lost.
å‡ ä½•å«ä¹‰ï¼šå¦‚æœ <span class="math inline">\(A\)</span>
è¡¨ç¤ºå¹³é¢çš„çº¿æ€§å˜æ¢ï¼Œåˆ™ <span class="math inline">\(|\det(A)|\)</span>
æ˜¯é¢ç§¯ç¼©æ”¾å› å­ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ <span class="math inline">\(\det(A) =
2\)</span> ï¼Œå½¢çŠ¶çš„é¢ç§¯å°†åŠ å€ã€‚å¦‚æœ <span class="math inline">\(\det(A)
= 0\)</span> ï¼Œå˜æ¢å°†å¹³é¢æŠ˜å æˆä¸€æ¡çº¿ï¼šæ‰€æœ‰é¢ç§¯éƒ½å°†ä¸¢å¤±ã€‚</p>
<h3 id="determinants-of-33-matrices">Determinants of 3Ã—3 Matrices</h3>
<p>3Ã—3 çŸ©é˜µçš„è¡Œåˆ—å¼</p>
<p>For ä¸ºäº†</p>
<p><span class="math display">\[
A = \begin{bmatrix}a &amp; b &amp; c \\d &amp; e &amp; f \\g &amp; h
&amp; i\end{bmatrix},
\]</span></p>
<p>the determinant can be computed as è¡Œåˆ—å¼å¯ä»¥è®¡ç®—ä¸º</p>
<p><span class="math display">\[
\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).
\]</span></p>
<p>Geometric meaning: In <span
class="math inline">\(\mathbb{R}^3\)</span>, <span
class="math inline">\(|\det(A)|\)</span> is the volume scaling factor.
If <span class="math inline">\(\det(A) &lt; 0\)</span>, orientation is
reversed (a handedness flip), such as turning a right-handed coordinate
system into a left-handed one. å‡ ä½•å«ä¹‰ï¼šåœ¨ <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­ï¼Œ <span
class="math inline">\(|\det(A)|\)</span> æ˜¯ä½“ç§¯ç¼©æ”¾å› å­ã€‚å¦‚æœä¸º <span
class="math inline">\(\det(A) &lt; 0\)</span>
ï¼Œåˆ™æ–¹å‘åè½¬ï¼ˆå³æ‰‹æ€§ç¿»è½¬ï¼‰ï¼Œä¾‹å¦‚å°†å³æ‰‹åæ ‡ç³»è½¬æ¢ä¸ºå·¦æ‰‹åæ ‡ç³»ã€‚</p>
<h3 id="general-case">General Case</h3>
<p>ä¸€èˆ¬æƒ…å†µ</p>
<p>For <span class="math inline">\(A \in \mathbb{R}^{n \times
n}\)</span>, the determinant is a scalar that measures how the linear
transformation given by <span class="math inline">\(A\)</span> scales
n-dimensional volume. å¯¹äº <span class="math inline">\(A \in
\mathbb{R}^{n \times n}\)</span> ï¼Œè¡Œåˆ—å¼æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œå®ƒè¡¡é‡ <span
class="math inline">\(A\)</span> ç»™å‡ºçš„çº¿æ€§å˜æ¢å¦‚ä½•ç¼©æ”¾ n ç»´ä½“ç§¯ã€‚</p>
<ul>
<li>If <span class="math inline">\(\det(A) = 0\)</span>: the
transformation squashes space into a lower dimension, so <span
class="math inline">\(A\)</span> is not invertible. å¦‚æœ <span
class="math inline">\(\det(A) = 0\)</span>
ï¼šå˜æ¢å°†ç©ºé—´å‹ç¼©åˆ°è¾ƒä½ç»´åº¦ï¼Œå› æ­¤ <span class="math inline">\(A\)</span>
ä¸å¯é€†ã€‚</li>
<li>If <span class="math inline">\(\det(A) &gt; 0\)</span>: volume is
scaled by <span class="math inline">\(\det(A)\)</span>, orientation
preserved. å¦‚æœæ˜¯ <span class="math inline">\(\det(A) &gt; 0\)</span>
ï¼šä½“ç§¯æŒ‰ <span class="math inline">\(\det(A)\)</span>
ç¼©æ”¾ï¼Œæ–¹å‘ä¿æŒä¸å˜ã€‚</li>
<li>If <span class="math inline">\(\det(A) &lt; 0\)</span>: volume is
scaled by <span class="math inline">\(|\det(A)|\)</span>, orientation
reversed. å¦‚æœæ˜¯ <span class="math inline">\(\det(A) &lt; 0\)</span>
ï¼šä½“ç§¯æŒ‰ <span class="math inline">\(|\det(A)|\)</span>
ç¼©æ”¾ï¼Œæ–¹å‘åè½¬ã€‚</li>
</ul>
<h3 id="visual-examples">Visual Examples</h3>
<p>è§†è§‰ç¤ºä¾‹</p>
<ol type="1">
<li><p>Shear in <span class="math inline">\(\mathbb{R}^2\)</span>: <span
class="math inline">\(A = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1
\end{bmatrix}\)</span>. Then <span class="math inline">\(\det(A) =
1\)</span>. The transformation slants the unit square into a
parallelogram but preserves area. <span
class="math inline">\(\mathbb{R}^2\)</span> å¤„çš„å‰ªåˆ‡ï¼š <span
class="math inline">\(A = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1
\end{bmatrix}\)</span> ã€‚ç„¶åæ˜¯ <span class="math inline">\(\det(A) =
1\)</span> ã€‚å˜æ¢å°†å•ä½æ­£æ–¹å½¢å€¾æ–œä¸ºå¹³è¡Œå››è¾¹å½¢ï¼Œä½†ä¿ç•™é¢ç§¯ã€‚</p></li>
<li><p>Projection in <span class="math inline">\(\mathbb{R}^2\)</span>:
<span class="math inline">\(A = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0
\end{bmatrix}\)</span>. Then <span class="math inline">\(\det(A) =
0\)</span>. The unit square collapses into a line segment: area
vanishes. <span class="math inline">\(\mathbb{R}^2\)</span> ä¸­çš„æŠ•å½±ï¼š
<span class="math inline">\(A = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0
\end{bmatrix}\)</span> ã€‚ç„¶å <span class="math inline">\(\det(A) =
0\)</span> ã€‚å•ä½æ­£æ–¹å½¢åç¼©æˆä¸€æ¡çº¿æ®µï¼šé¢ç§¯æ¶ˆå¤±ã€‚</p></li>
<li><p>Rotation in <span class="math inline">\(\mathbb{R}^2\)</span>:
<span class="math inline">\(R_\theta = \begin{bmatrix} \cos\theta &amp;
-\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}\)</span>. Then
<span class="math inline">\(\det(R_\theta) = 1\)</span>. Rotations
preserve area and orientation. <span
class="math inline">\(\mathbb{R}^2\)</span> ä¸­çš„æ—‹è½¬ï¼š <span
class="math inline">\(R_\theta = \begin{bmatrix} \cos\theta &amp;
-\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}\)</span> ã€‚ç„¶å
<span class="math inline">\(\det(R_\theta) = 1\)</span>
ã€‚æ—‹è½¬ä¿ç•™é¢ç§¯å’Œæ–¹å‘ã€‚</p></li>
</ol>
<h3 id="why-this-matters-19">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>The determinant is not just a formula-it is a measure of
transformation. It tells us whether a matrix is invertible, how it
distorts space, and whether it flips orientation. This geometric insight
makes the determinant indispensable in analysis, geometry, and applied
mathematics.
è¡Œåˆ—å¼ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå…¬å¼ï¼Œå®ƒè¿˜æ˜¯ä¸€ç§å˜æ¢çš„åº¦é‡ã€‚å®ƒå‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªçŸ©é˜µæ˜¯å¦å¯é€†ï¼Œå®ƒå¦‚ä½•æ‰­æ›²ç©ºé—´ï¼Œä»¥åŠå®ƒæ˜¯å¦ä¼šç¿»è½¬æ–¹å‘ã€‚è¿™ç§å‡ ä½•å­¦ä¸Šçš„æ´å¯ŸåŠ›ä½¿å¾—è¡Œåˆ—å¼åœ¨åˆ†æã€å‡ ä½•å’Œåº”ç”¨æ•°å­¦ä¸­ä¸å¯æˆ–ç¼ºã€‚</p>
<h3 id="exercises-6.1">Exercises 6.1</h3>
<p>ç»ƒä¹  6.1</p>
<ol type="1">
<li>Compute the determinant of è®¡ç®—è¡Œåˆ—å¼</li>
</ol>
<p><span class="math display">\[
\begin{bmatrix} 2 &amp; 3 \\ 1 &amp; 4 \end{bmatrix}
\]</span></p>
<p>What area scaling factor does it represent? 2. Find the determinant
of the shear matrix å®ƒä»£è¡¨ä»€ä¹ˆé¢ç§¯æ¯”ä¾‹å› å­ï¼Ÿ2. æ±‚å‰ªåˆ‡çŸ©é˜µçš„è¡Œåˆ—å¼</p>
<p><span class="math display">\[
\begin{bmatrix} 1 &amp; 2 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<blockquote>
<p>ä¸‹é¢è¿™æ®µå…¬å¼è§£æé”™è¯¯ï¼Œæˆ‘æ”¹æˆæºç æ¨¡å¼äº†</p>
</blockquote>
<pre><code>What happens to the area of the unit square? 3. For the $3 \\times 3matrix \[100020003\] Compute the determinant. How does it scale volume in\\mathbb{R}^3$?4. Show that any rotation matrix in $\\mathbb{R}^2 has determinant \\1. 5. Give an example of a \\2 \\times 2$matrix with determinant$\-1$. What geometric action does it represent?
å•ä½æ­£æ–¹å½¢çš„é¢ç§¯ä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ï¼Ÿ 3. å¯¹äº $3 \\times 3 çŸ©é˜µ \[100020003\] è®¡ç®—è¡Œåˆ—å¼ã€‚\\mathbb{R}^3 $?4. Show that any rotation matrix in $ \\mathbb{R}^2 çš„è¡Œåˆ—å¼ä¸º \\ 1 ï¼Œå®ƒå¦‚ä½•ç¼©æ”¾ä½“ç§¯ ï¼Ÿ5. ä¸¾ä¸€ä¸ª \\ 2 \\times 2 $matrix with determinant$ -1$ çš„ä¾‹å­ ã€‚å®ƒä»£è¡¨ä»€ä¹ˆå‡ ä½•ä½œç”¨ï¼Ÿ</code></pre>
<h2 id="properties-of-determinants">6.2 Properties of Determinants</h2>
<p>6.2 è¡Œåˆ—å¼çš„æ€§è´¨</p>
<p>Beyond their geometric meaning, determinants satisfy a collection of
algebraic rules that make them powerful tools in linear algebra. These
properties allow us to compute efficiently, test invertibility, and
understand how determinants behave under matrix operations.
é™¤äº†å‡ ä½•æ„ä¹‰ä¹‹å¤–ï¼Œè¡Œåˆ—å¼è¿˜æ»¡è¶³ä¸€ç³»åˆ—ä»£æ•°è§„åˆ™ï¼Œä½¿å…¶æˆä¸ºçº¿æ€§ä»£æ•°ä¸­å¼ºå¤§çš„å·¥å…·ã€‚è¿™äº›æ€§è´¨ä½¿æˆ‘ä»¬èƒ½å¤Ÿé«˜æ•ˆè®¡ç®—ã€æµ‹è¯•å¯é€†æ€§ï¼Œå¹¶ç†è§£è¡Œåˆ—å¼åœ¨çŸ©â€‹â€‹é˜µè¿ç®—ä¸‹çš„è¡Œä¸ºã€‚</p>
<h3 id="basic-properties">Basic Properties</h3>
<p>åŸºæœ¬å±æ€§</p>
<p>Let <span class="math inline">\(A, B \in \mathbb{R}^{n \times
n}\)</span>, and let <span class="math inline">\(c \in
\mathbb{R}\)</span>. Then: ä»¤ <span class="math inline">\(A, B \in
\mathbb{R}^{n \times n}\)</span> ï¼Œä»¤ <span class="math inline">\(c \in
\mathbb{R}\)</span> ã€‚ç„¶åï¼š</p>
<ol type="1">
<li>Identity: èº«ä»½ï¼š</li>
</ol>
<p><span class="math display">\[
\det(I_n) = 1.
\]</span></p>
<ol start="2" type="1">
<li>Triangular matrices: If <span class="math inline">\(A\)</span> is
upper or lower triangular, then ä¸‰è§’çŸ©é˜µï¼š å¦‚æœ <span
class="math inline">\(A\)</span> æ˜¯ä¸Šä¸‰è§’æˆ–ä¸‹ä¸‰è§’ï¼Œåˆ™</li>
</ol>
<p><span class="math display">\[
\det(A) = a_{11} a_{22} \cdots a_{nn}.
\]</span></p>
<ol start="3" type="1">
<li><p>Row/column swap: Interchanging two rows (or columns) multiplies
the determinant by <span class="math inline">\(-1\)</span>. è¡Œ/åˆ—äº¤æ¢ï¼š
äº¤æ¢ä¸¤è¡Œï¼ˆæˆ–åˆ—ï¼‰å°†è¡Œåˆ—å¼ä¹˜ä»¥ <span class="math inline">\(-1\)</span>
ã€‚</p></li>
<li><p>Row/column scaling: Multiplying a row (or column) by a scalar
<span class="math inline">\(c\)</span> multiplies the determinant by
<span class="math inline">\(c\)</span>. è¡Œ/åˆ—ç¼©æ”¾ï¼š å°†è¡Œï¼ˆæˆ–åˆ—ï¼‰ä¹˜ä»¥æ ‡é‡
<span class="math inline">\(c\)</span> ä¼šå°†è¡Œåˆ—å¼ä¹˜ä»¥ <span
class="math inline">\(c\)</span> ã€‚</p></li>
<li><p>Row/column addition: Adding a multiple of one row to another does
not change the determinant.
è¡Œ/åˆ—åŠ æ³•ï¼šå°†ä¸€è¡Œçš„å€æ•°æ·»åŠ åˆ°å¦ä¸€è¡Œä¸ä¼šæ”¹å˜è¡Œåˆ—å¼ã€‚</p></li>
<li><p>Transpose: è½¬ç½®ï¼š</p></li>
</ol>
<p><span class="math display">\[
\det(A^T) = \det(A).
\]</span></p>
<ol start="7" type="1">
<li>Multiplicativity: ä¹˜æ³•æ€§ï¼š</li>
</ol>
<p><span class="math display">\[
\det(AB) = \det(A)\det(B).
\]</span></p>
<ol start="8" type="1">
<li>Invertibility: <span class="math inline">\(A\)</span> is invertible
if and only if <span class="math inline">\(\det(A) \neq 0\)</span>.
å¯é€†æ€§ï¼š å½“ä¸”ä»…å½“ <span class="math inline">\(\det(A) \neq 0\)</span>
æ—¶ï¼Œ <span class="math inline">\(A\)</span> æ‰æ˜¯å¯é€†çš„ã€‚</li>
</ol>
<h3 id="example-computations">Example Computations</h3>
<p>è®¡ç®—ç¤ºä¾‹</p>
<p>Example 6.2.1. For ä¾‹ 6.2.1. å¯¹äº</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 0 &amp; 0 \\1 &amp; 3 &amp; 0 \\-1 &amp; 4
&amp; 5\end{bmatrix},
\]</span></p>
<p><span class="math inline">\(A\)</span> is lower triangular, so <span
class="math inline">\(A\)</span> æ˜¯ä¸‹ä¸‰è§’ï¼Œæ‰€ä»¥</p>
<p><span class="math display">\[
\det(A) = 2 \cdot 3 \cdot 5 = 30.
\]</span></p>
<p>Example 6.2.2. Let ä¾‹ 6.2.2. è®¾</p>
<p><span class="math display">\[
B = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}, \quad C =
\begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}.
\]</span></p>
<p>Then ç„¶å</p>
<p><span class="math display">\[
\det(B) = 1\cdot 4 - 2\cdot 3 = -2, \quad \det(C) = -1.
\]</span></p>
<p>Since <span class="math inline">\(CB\)</span> is obtained by swapping
rows of <span class="math inline">\(B\)</span>, ç”±äº <span
class="math inline">\(CB\)</span> æ˜¯é€šè¿‡äº¤æ¢ <span
class="math inline">\(B\)</span> çš„è¡Œè·å¾—çš„ï¼Œ</p>
<p><span class="math display">\[
\det(CB) = -\det(B) = 2.
\]</span></p>
<p>This matches the multiplicativity rule: <span
class="math inline">\(\det(CB) = \det(C)\det(B) = (-1)(-2) = 2.\)</span>
è¿™ç¬¦åˆä¹˜æ³•è§„åˆ™ï¼š <span class="math inline">\(\det(CB) = \det(C)\det(B) =
(-1)(-2) = 2.\)</span></p>
<h3 id="geometric-insights">Geometric Insights</h3>
<p>å‡ ä½•æ´å¯Ÿ</p>
<ul>
<li>Row swaps: flipping orientation of space.
è¡Œäº¤æ¢ï¼šç¿»è½¬ç©ºé—´çš„æ–¹å‘ã€‚</li>
<li>Scaling a row: stretching space in one direction.
ç¼©æ”¾ä¸€è¡Œï¼šæœä¸€ä¸ªæ–¹å‘æ‹‰ä¼¸ç©ºé—´ã€‚</li>
<li>Row replacement: sliding hyperplanes without altering volume.
è¡Œæ›¿æ¢ï¼šæ»‘åŠ¨è¶…å¹³é¢è€Œä¸æ”¹å˜ä½“ç§¯ã€‚</li>
<li>Multiplicativity: performing two transformations multiplies their
scaling factors. ä¹˜æ³•æ€§ï¼šæ‰§è¡Œä¸¤ä¸ªå˜æ¢ä¼šå°†å®ƒä»¬çš„æ¯”ä¾‹å› å­ç›¸ä¹˜ã€‚</li>
</ul>
<p>These properties make determinants both computationally manageable
and geometrically interpretable.
è¿™äº›æ€§è´¨ä½¿å¾—è¡Œåˆ—å¼æ—¢æ˜“äºè®¡ç®—ç®¡ç†ï¼Œåˆæ˜“äºå‡ ä½•è§£é‡Šã€‚</p>
<h3 id="why-this-matters-20">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Determinant properties connect computation with geometry and theory.
They explain why Gaussian elimination works, why invertibility is
equivalent to nonzero determinant, and why determinants naturally arise
in areas like volume computation, eigenvalue theory, and differential
equations.
è¡Œåˆ—å¼çš„æ€§è´¨å°†è®¡ç®—ä¸å‡ ä½•å’Œç†è®ºè”ç³»èµ·æ¥ã€‚å®ƒä»¬è§£é‡Šäº†é«˜æ–¯æ¶ˆå…ƒæ³•ä¸ºä½•æœ‰æ•ˆï¼Œå¯é€†æ€§ä¸ºä½•ç­‰ä»·äºéé›¶è¡Œåˆ—å¼ï¼Œä»¥åŠè¡Œåˆ—å¼ä¸ºä½•è‡ªç„¶åœ°å‡ºç°åœ¨ä½“ç§¯è®¡ç®—ã€ç‰¹å¾å€¼ç†è®ºå’Œå¾®åˆ†æ–¹ç¨‹ç­‰é¢†åŸŸã€‚</p>
<h3 id="exercises-6.2">Exercises 6.2</h3>
<p>ç»ƒä¹  6.2</p>
<ol type="1">
<li>Compute the determinant of è®¡ç®—è¡Œåˆ—å¼</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; 1 &amp; 4 \\ 0 &amp; 0
&amp; 2 \end{bmatrix}.
\]</span></p>
<ol start="2" type="1">
<li><p>Show that if two rows of a square matrix are identical, then its
determinant is zero. è¯æ˜å¦‚æœæ–¹é˜µçš„ä¸¤è¡Œç›¸åŒï¼Œåˆ™å…¶è¡Œåˆ—å¼ä¸ºé›¶ã€‚</p></li>
<li><p>Verify <span class="math inline">\(\det(A^T) = \det(A)\)</span>
for éªŒè¯ <span class="math inline">\(\det(A^T) =
\det(A)\)</span></p></li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; -1 \\ 3 &amp; 4 \end{bmatrix}.
\]</span></p>
<ol start="4" type="1">
<li>If <span class="math inline">\(A\)</span> is invertible, prove that
å¦‚æœ <span class="math inline">\(A\)</span> å¯é€†ï¼Œåˆ™è¯æ˜</li>
</ol>
<p><span class="math display">\[
\det(A^{-1}) = \frac{1}{\det(A)}.
\]</span></p>
<ol start="5" type="1">
<li>Suppose <span class="math inline">\(A\)</span> is a <span
class="math inline">\(3\\times 3\)</span>matrix with<span
class="math inline">\(\\det(A) = 5\)</span>. What is <span
class="math inline">\(\\det(2A)\)</span>? å‡è®¾ <span
class="math inline">\(A\)</span> æ˜¯ $3\times 3 <span
class="math inline">\(matrix with\)</span> \det(A) = 5 $. What is $
\det(2A)$ï¼Ÿ</li>
</ol>
<h2 id="cofactor-expansion">6.3 Cofactor Expansion</h2>
<p>6.3 è¾…å› å­å±•å¼€</p>
<p>While determinants of small matrices can be computed directly from
formulas, larger matrices require a systematic method. The cofactor
expansion (also known as Laplace expansion) provides a recursive way to
compute determinants by breaking them into smaller ones.
è™½ç„¶å°çŸ©é˜µçš„è¡Œåˆ—å¼å¯ä»¥ç›´æ¥é€šè¿‡å…¬å¼è®¡ç®—ï¼Œä½†è¾ƒå¤§çš„çŸ©é˜µåˆ™éœ€è¦ç³»ç»Ÿçš„æ–¹æ³•ã€‚ä½™å› å­å±•å¼€å¼ï¼ˆä¹Ÿç§°ä¸ºæ‹‰æ™®æ‹‰æ–¯å±•å¼€å¼ï¼‰é€šè¿‡å°†è¡Œåˆ—å¼åˆ†è§£ä¸ºæ›´å°çš„çŸ©é˜µï¼Œæä¾›äº†ä¸€ç§é€’å½’è®¡ç®—è¡Œåˆ—å¼çš„æ–¹æ³•ã€‚</p>
<h3 id="minors-and-cofactors">Minors and Cofactors</h3>
<p>å°å¼å’Œè¾…å› å­</p>
<p>For an <span class="math inline">\(n \times n\)</span> matrix <span
class="math inline">\(A = [a_{ij}]\)</span>: å¯¹äº <span
class="math inline">\(n \times n\)</span> çŸ©é˜µ <span
class="math inline">\(A = [a_{ij}]\)</span> ï¼š</p>
<ul>
<li>The minor <span class="math inline">\(M_{ij}\)</span> is the
determinant of the <span class="math inline">\((n-1) \times
(n-1)\)</span> matrix obtained by deleting the <span
class="math inline">\(i\)</span>-th row and <span
class="math inline">\(j\)</span> -th column of <span
class="math inline">\(A\)</span>. å°è°ƒğ‘€ ğ‘– ğ‘— M ä¼Šå¥‡ â€‹ æ˜¯åˆ é™¤ç¬¬ <span
class="math inline">\(i\)</span> è¡Œå’Œ <span
class="math inline">\(j\)</span> åå¾—åˆ°çš„ <span
class="math inline">\((n-1) \times (n-1)\)</span> çŸ©é˜µçš„è¡Œåˆ—å¼ <span
class="math inline">\(A\)</span> çš„ç¬¬åˆ—ã€‚</li>
<li>The cofactor <span class="math inline">\(C_{ij}\)</span> is defined
by è¾…å› å­ğ¶ ğ‘– ğ‘— C ä¼Šå¥‡ â€‹ å®šä¹‰ä¸º</li>
</ul>
<p><span class="math display">\[
C_{ij} = (-1)^{i+j} M_{ij}.
\]</span></p>
<p>The sign factor <span class="math inline">\((-1)^{i+j}\)</span>
alternates in a checkerboard pattern: ç¬¦å·å› å­ <span
class="math inline">\((-1)^{i+j}\)</span> ä»¥æ£‹ç›˜æ ¼å›¾æ¡ˆäº¤æ›¿å‡ºç°ï¼š</p>
<p><span class="math display">\[
\begin{bmatrix}+ &amp; - &amp; + &amp; - &amp; \cdots \\- &amp; + &amp;
- &amp; + &amp; \cdots \\+ &amp; - &amp; + &amp; - &amp; \cdots \\\vdots
&amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots\end{bmatrix}.
\]</span></p>
<h3 id="cofactor-expansion-formula">Cofactor Expansion Formula</h3>
<p>è¾…å› å¼å±•å¼€å…¬å¼</p>
<p>The determinant of <span class="math inline">\(A\)</span> can be
computed by expanding along any row or any column: <span
class="math inline">\(A\)</span>
çš„è¡Œåˆ—å¼å¯ä»¥é€šè¿‡æ²¿ä»»æ„è¡Œæˆ–ä»»æ„åˆ—å±•å¼€æ¥è®¡ç®—ï¼š</p>
<p><span class="math display">\[
\det(A) = \sum_{j=1}^n a_{ij} C_{ij} \quad \text{(expansion along row
\(i\))},
\]</span></p>
<p><span class="math display">\[
\det(A) = \sum_{i=1}^n a_{ij} C_{ij} \quad \text{(expansion along column
\(j\))}.
\]</span></p>
<h3 id="example-4">Example</h3>
<p>ä¾‹å­</p>
<p>Example 6.3.1. Compute ä¾‹ 6.3.1. è®¡ç®—</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 &amp; 3 \\0 &amp; 4 &amp; 5 \\1 &amp; 0
&amp; 6\end{bmatrix}.
\]</span></p>
<p>Expand along the first row: æ²¿ç¬¬ä¸€è¡Œå±•å¼€ï¼š</p>
<p><span class="math display">\[
\det(A) = 1 \cdot C_{11} + 2 \cdot C_{12} + 3 \cdot C_{13}.
\]</span></p>
<ul>
<li>For <span class="math inline">\(C_{11}\)</span>: å¯¹äºğ¶ 11 C 11 â€‹
:</li>
</ul>
<p><span class="math display">\[
M_{11} = \det \begin{bmatrix} 4 &amp; 5 \\ 0 &amp; 6 \end{bmatrix} = 24
\]</span></p>
<p>so <span class="math inline">\(C_{11} = (+1)(24) = 24\)</span>. æ‰€ä»¥
<span class="math inline">\(C_{11} = (+1)(24) = 24\)</span> ã€‚</p>
<ul>
<li>For <span class="math inline">\(C_{12}\)</span>: å¯¹äºğ¶ 12 C 12 â€‹
:</li>
</ul>
<p><span class="math display">\[
M_{12} = \det \begin{bmatrix} 0 &amp; 5 \\ 1 &amp; 6 \end{bmatrix} = 0 -
5 = -5
\]</span></p>
<p>so <span class="math inline">\(C_{12} = (-1)(-5) = 5\)</span>. æ‰€ä»¥
<span class="math inline">\(C_{12} = (-1)(-5) = 5\)</span> ã€‚</p>
<ul>
<li>For <span class="math inline">\(C_{13}\)</span>: å¯¹äºğ¶ 13 C 13 â€‹
:</li>
</ul>
<p><span class="math display">\[
M_{13} = \det \begin{bmatrix} 0 &amp; 4 \\ 1 &amp; 0 \end{bmatrix} = 0 -
4 = -4
\]</span></p>
<p>so <span class="math inline">\(C_{13} = (+1)(-4) = -4\)</span>. æ‰€ä»¥
<span class="math inline">\(C_{13} = (+1)(-4) = -4\)</span> ã€‚</p>
<p>Thus, å› æ­¤ï¼Œ</p>
<p><span class="math display">\[
\det(A) = 1(24) + 2(5) + 3(-4) = 24 + 10 - 12 = 22.
\]</span></p>
<h3 id="properties-of-cofactor-expansion">Properties of Cofactor
Expansion</h3>
<p>è¾…å› å­å±•å¼€çš„æ€§è´¨</p>
<ol type="1">
<li>Expansion along any row or column yields the same result.
æ²¿ä»»æ„è¡Œæˆ–åˆ—æ‰©å±•éƒ½ä¼šäº§ç”Ÿç›¸åŒçš„ç»“æœã€‚</li>
<li>The cofactor expansion provides a recursive definition of
determinant: a determinant of size <span
class="math inline">\(n\)</span> is expressed in terms of determinants
of size <span class="math inline">\(n-1\)</span>.
ä½™å› å­å±•å¼€æä¾›äº†è¡Œåˆ—å¼çš„é€’å½’å®šä¹‰ï¼šå¤§å°ä¸º <span
class="math inline">\(n\)</span> çš„è¡Œåˆ—å¼å¯ä»¥ç”¨å¤§å°ä¸º <span
class="math inline">\(n-1\)</span> çš„è¡Œåˆ—å¼æ¥è¡¨ç¤ºã€‚</li>
<li>Cofactors are fundamental in constructing the adjugate matrix, which
gives a formula for inverses:
ä½™å› å­æ˜¯æ„é€ ä¼´éšçŸ©é˜µçš„åŸºç¡€ï¼Œå®ƒç»™å‡ºäº†é€†çš„å…¬å¼ï¼š</li>
</ol>
<p><span class="math display">\[
A^{-1} = \frac{1}{\det(A)} \, \text{adj}(A), \quad \text{where adj}(A) =
[C_{ji}].
\]</span></p>
<h3 id="geometric-interpretation-11">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>Cofactor expansion breaks down the determinant into contributions
from sub-volumes defined by fixing one row or column at a time. Each
cofactor measures how that row/column influences the overall volume
scaling.
ä½™å› å­å±•å¼€å°†è¡Œåˆ—å¼åˆ†è§£ä¸ºç”±æ¯æ¬¡å›ºå®šä¸€è¡Œæˆ–ä¸€åˆ—å®šä¹‰çš„å­ä½“ç§¯çš„è´¡çŒ®ã€‚æ¯ä¸ªä½™å› å­è¡¡é‡è¯¥è¡Œ/åˆ—å¯¹æ•´ä½“ä½“ç§¯ç¼©æ”¾çš„å½±å“ã€‚</p>
<h3 id="why-this-matters-21">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Cofactor expansion generalizes the small-matrix formulas and provides
a conceptual definition of determinants. While not the most efficient
way to compute determinants for large matrices, it is essential for
theory, proofs, and connections to adjugates, Cramerâ€™s rule, and
classical geometry.
ä½™å› å­å±•å¼€å¼æ¨å¹¿äº†å°çŸ©é˜µå…¬å¼ï¼Œå¹¶æä¾›äº†è¡Œåˆ—å¼çš„æ¦‚å¿µå®šä¹‰ã€‚è™½ç„¶å®ƒå¹¶éè®¡ç®—å¤§çŸ©é˜µè¡Œåˆ—å¼çš„æœ€æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å®ƒå¯¹äºç†è®ºã€è¯æ˜ä»¥åŠä¸ä¼´éšé¡¹ã€å…‹è±å§†è§„åˆ™å’Œå¤å…¸å‡ ä½•çš„è”ç³»è‡³å…³é‡è¦ã€‚</p>
<h3 id="exercises-6.3">Exercises 6.3</h3>
<p>ç»ƒä¹  6.3</p>
<ol type="1">
<li>Compute the determinant of è®¡ç®—è¡Œåˆ—å¼</li>
</ol>
<p><span class="math display">\[
\begin{bmatrix}2 &amp; 0 &amp; 1 \\3 &amp; -1 &amp; 4 \\1 &amp; 2 &amp;
0\end{bmatrix}
\]</span></p>
<p>by cofactor expansion along the first column.
é€šè¿‡æ²¿ç¬¬ä¸€åˆ—çš„ä½™å› å­å±•å¼€ã€‚</p>
<ol start="2" type="1">
<li><p>Verify that expanding along the second row of Example 6.3.1 gives
the same determinant. éªŒè¯æ²¿ç¤ºä¾‹ 6.3.1
çš„ç¬¬äºŒè¡Œå±•å¼€æ˜¯å¦ç»™å‡ºç›¸åŒçš„è¡Œåˆ—å¼ã€‚</p></li>
<li><p>Prove that expansion along any row gives the same value.
è¯æ˜æ²¿ä»»ä½•è¡Œå±•å¼€éƒ½ä¼šç»™å‡ºç›¸åŒçš„å€¼ã€‚</p></li>
<li><p>Show that if a row of a matrix is zero, then its determinant is
zero. è¯æ˜å¦‚æœçŸ©é˜µçš„æŸä¸€è¡Œæ˜¯é›¶ï¼Œé‚£ä¹ˆå®ƒçš„è¡Œåˆ—å¼ä¹Ÿæ˜¯é›¶ã€‚</p></li>
<li><p>Use cofactor expansion to prove that <span
class="math inline">\(\det(A) = \det(A^T)\)</span>. ä½¿ç”¨ä½™å› å­å±•å¼€æ¥è¯æ˜
<span class="math inline">\(\det(A) = \det(A^T)\)</span> ã€‚</p></li>
</ol>
<h2 id="applications-volume-invertibility-test">6.4 Applications
(Volume, Invertibility Test)</h2>
<p>6.4 åº”ç”¨ï¼ˆä½“ç§¯ã€å¯é€†æ€§æµ‹è¯•ï¼‰</p>
<p>Determinants are not merely algebraic curiosities; they have concrete
geometric and computational uses. Two of the most important applications
are measuring volumes and testing invertibility of matrices.
è¡Œåˆ—å¼ä¸ä»…ä»…æ˜¯ä»£æ•°ä¸Šçš„å¥‡é—»ï¼›å®ƒä»¬æœ‰ç€å…·ä½“çš„å‡ ä½•å’Œè®¡ç®—ç”¨é€”ã€‚å…¶ä¸­æœ€é‡è¦çš„ä¸¤ä¸ªåº”ç”¨æ˜¯æµ‹é‡ä½“ç§¯å’Œæ£€éªŒçŸ©é˜µçš„å¯é€†æ€§ã€‚</p>
<h3 id="determinants-as-volume-scalers">Determinants as Volume
Scalers</h3>
<p>å†³å®šå› ç´ ä½œä¸ºä½“ç§¯æ ‡é‡</p>
<p>Given vectors <span class="math inline">\(\mathbf{v}_1, \mathbf{v}_2,
\dots, \mathbf{v}_n \in \mathbb{R}^n\)</span>, arrange them as columns
of a matrix: ç»™å®šå‘é‡ <span class="math inline">\(\mathbf{v}_1,
\mathbf{v}_2, \dots, \mathbf{v}_n \in \mathbb{R}^n\)</span>
ï¼Œå°†å®ƒä»¬æ’åˆ—ä¸ºçŸ©é˜µçš„åˆ—ï¼š</p>
<p><span class="math display">\[
A = \begin{bmatrix}| &amp; | &amp; &amp; | \\\mathbf{v}_1 &amp;
\mathbf{v}_2 &amp; \cdots &amp; \mathbf{v}_n \\| &amp; | &amp; &amp;
|\end{bmatrix}.
\]</span></p>
<p>Then <span class="math inline">\(|\det(A)|\)</span> equals the volume
of the parallelepiped spanned by these vectors. é‚£ä¹ˆ <span
class="math inline">\(|\det(A)|\)</span>
ç­‰äºè¿™äº›å‘é‡æ‰€è·¨è¶Šçš„å¹³è¡Œå…­é¢ä½“çš„ä½“ç§¯ã€‚</p>
<ul>
<li>In <span class="math inline">\(\mathbb{R}^2\)</span>, <span
class="math inline">\(|\det(A)|\)</span> gives the area of the
parallelogram spanned by <span class="math inline">\(\mathbf{v}_1,
\mathbf{v}_2\)</span>. åœ¨ <span
class="math inline">\(\mathbb{R}^2\)</span> ä¸­ï¼Œ <span
class="math inline">\(|\det(A)|\)</span> ç»™å‡ºç”± ğ‘£ æ„æˆçš„å¹³è¡Œå››è¾¹å½¢çš„é¢ç§¯
1 , ğ‘£ 2 v 1 â€‹ ï¼Œv 2 â€‹ .</li>
<li>In <span class="math inline">\(\mathbb{R}^3\)</span>, <span
class="math inline">\(|\det(A)|\)</span> gives the volume of the
parallelepiped spanned by <span class="math inline">\(\mathbf{v}_1,
\mathbf{v}_2, \mathbf{v}_3\)</span>. åœ¨ <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­ï¼Œ <span
class="math inline">\(|\det(A)|\)</span> ç»™å‡ºå¹³è¡Œå…­é¢ä½“çš„ä½“ç§¯ï¼Œè·¨åº¦ä¸º ğ‘£
1 , ğ‘£ 2 , ğ‘£ 3 v 1 â€‹ ï¼Œv 2 â€‹ ï¼Œv 3 â€‹ .</li>
<li>In higher dimensions, it generalizes to <span
class="math inline">\(n\)</span>-dimensional volume (hypervolume).
åœ¨æ›´é«˜ç»´åº¦ä¸­ï¼Œå®ƒå¯ä»¥æ¨å¹¿åˆ° <span class="math inline">\(n\)</span>
ç»´ä½“ç§¯ï¼ˆè¶…ä½“ç§¯ï¼‰ã€‚</li>
</ul>
<p>Example 6.4.1. Let ä¾‹ 6.4.1. è®¾</p>
<p><span class="math display">\[
\mathbf{v}_1 = (1,0,0), \quad \mathbf{v}_2 = (1,1,0), \quad \mathbf{v}_3
= (1,1,1).
\]</span></p>
<p>Then ç„¶å</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 1 &amp; 1 \\0 &amp; 1 &amp; 1 \\0 &amp; 0
&amp; 1\end{bmatrix}, \quad \det(A) = 1.
\]</span></p>
<p>So the parallelepiped has volume 1, even though the vectors are not
orthogonal. å› æ­¤ï¼Œå³ä½¿å‘é‡ä¸æ­£äº¤ï¼Œå¹³è¡Œå…­é¢ä½“çš„ä½“ç§¯ä¹Ÿæ˜¯ 1 ã€‚</p>
<h3 id="invertibility-test">Invertibility Test</h3>
<p>å¯é€†æ€§æµ‹è¯•</p>
<p>A square matrix <span class="math inline">\(A\)</span> is invertible
if and only if <span class="math inline">\(\det(A) \neq 0\)</span>. æ–¹é˜µ
<span class="math inline">\(A\)</span> å¯é€†å½“ä¸”ä»…å½“ <span
class="math inline">\(\det(A) \neq 0\)</span> ã€‚</p>
<ul>
<li>If <span class="math inline">\(\det(A) = 0\)</span>: the
transformation collapses space into a lower dimension (area/volume is
zero). No inverse exists. å¦‚æœ <span class="math inline">\(\det(A) =
0\)</span>
ï¼šå˜æ¢å°†ç©ºé—´å¡Œç¼©è‡³è¾ƒä½ç»´åº¦ï¼ˆé¢ç§¯/ä½“ç§¯ä¸ºé›¶ï¼‰ã€‚ä¸å­˜åœ¨é€†å˜æ¢ã€‚</li>
<li>If <span class="math inline">\(\det(A) \neq 0\)</span>: the
transformation scales volume by <span
class="math inline">\(|\det(A)|\)</span>, and is reversible. å¦‚æœ <span
class="math inline">\(\det(A) \neq 0\)</span> ï¼šå˜æ¢å°†ä½“ç§¯ç¼©æ”¾ <span
class="math inline">\(|\det(A)|\)</span> ï¼Œå¹¶ä¸”æ˜¯å¯é€†çš„ã€‚</li>
</ul>
<p>Example 6.4.2. The matrix ä¾‹ 6.4.2. çŸ©é˜µ</p>
<p><span class="math display">\[
B = \begin{bmatrix} 2 &amp; 4 \\ 1 &amp; 2 \end{bmatrix}
\]</span></p>
<p>has determinant <span class="math inline">\(\det(B) = 2 \cdot 2 - 4
\cdot 1 = 0\)</span>. Thus, <span class="math inline">\(B\)</span> is
not invertible. Geometrically, the two column vectors are collinear,
spanning only a line in <span
class="math inline">\(\mathbb{R}^2\)</span>. è¡Œåˆ—å¼ä¸º <span
class="math inline">\(\det(B) = 2 \cdot 2 - 4 \cdot 1 = 0\)</span>
ã€‚å› æ­¤ï¼Œ <span class="math inline">\(B\)</span>
ä¸å¯é€†ã€‚å‡ ä½•ä¸Šï¼Œè¿™ä¸¤ä¸ªåˆ—å‘é‡å…±çº¿ï¼Œåœ¨ <span
class="math inline">\(\mathbb{R}^2\)</span> ä¸­ä»…å»¶ä¼¸ä¸€æ¡çº¿ã€‚</p>
<h3 id="cramers-rule">Cramerâ€™s Rule</h3>
<p>å…‹è±é»˜è§„åˆ™</p>
<p>Determinants also provide an explicit formula for solving systems of
linear equations when the matrix is invertible. For <span
class="math inline">\(A\mathbf{x} = \mathbf{b}\)</span> with <span
class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>:
å½“çŸ©é˜µå¯é€†æ—¶ï¼Œè¡Œåˆ—å¼è¿˜æä¾›äº†æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„çš„æ˜ç¡®å…¬å¼ã€‚ å¯¹äºå¸¦æœ‰ <span
class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> çš„ <span
class="math inline">\(A\mathbf{x} = \mathbf{b}\)</span> ï¼š</p>
<p><span class="math display">\[
x_i = \frac{\det(A_i)}{\det(A)},
\]</span></p>
<p>where <span class="math inline">\(A_i\)</span> is obtained by
replacing the <span class="math inline">\(i\)</span>-th column of <span
class="math inline">\(A\)</span> with <span
class="math inline">\(\mathbf{b}\)</span>. While inefficient
computationally, Cramerâ€™s rule highlights the determinantâ€™s role in
solutions and uniqueness. å…¶ä¸­ğ´ ğ‘– A i â€‹ é€šè¿‡å°† <span
class="math inline">\(A\)</span> çš„ç¬¬ <span
class="math inline">\(i\)</span> åˆ—æ›¿æ¢ä¸º <span
class="math inline">\(\mathbf{b}\)</span>
å¾—åˆ°ã€‚å…‹è±å§†è§„åˆ™è™½ç„¶è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œä½†å®ƒå‡¸æ˜¾äº†è¡Œåˆ—å¼åœ¨è§£å’Œå”¯ä¸€æ€§æ–¹é¢çš„ä½œç”¨ã€‚</p>
<h3 id="orientation">Orientation</h3>
<p>æ–¹å‘</p>
<p>The sign of <span class="math inline">\(\det(A)\)</span> indicates
whether a transformation preserves or reverses orientation. For example,
a reflection in the plane has determinant <span
class="math inline">\(-1\)</span>, flipping handedness. <span
class="math inline">\(\det(A)\)</span>
çš„ç¬¦å·è¡¨ç¤ºå˜æ¢æ˜¯ä¿æŒæ–¹å‘è¿˜æ˜¯åè½¬æ–¹å‘ã€‚ä¾‹å¦‚ï¼Œå¹³é¢ä¸Šçš„åå°„å…·æœ‰è¡Œåˆ—å¼ <span
class="math inline">\(-1\)</span> ï¼Œå³ç¿»è½¬æ—‹å‘æ€§ã€‚</p>
<h3 id="why-this-matters-22">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Determinants condense key information: they measure scaling, test
invertibility, and track orientation. These insights are indispensable
in geometry (areas and volumes), analysis (Jacobian determinants in
calculus), and computation ( solving systems and checking singularity).
è¡Œåˆ—å¼æµ“ç¼©äº†å…³é”®ä¿¡æ¯ï¼šå®ƒä»¬æµ‹é‡ç¼©æ”¾æ¯”ä¾‹ã€æ£€éªŒå¯é€†æ€§å¹¶è¿½è¸ªæ–¹å‘ã€‚è¿™äº›æ´è§åœ¨å‡ ä½•å­¦ï¼ˆé¢ç§¯å’Œä½“ç§¯ï¼‰ã€åˆ†æå­¦ï¼ˆå¾®ç§¯åˆ†ä¸­çš„é›…å¯æ¯”è¡Œåˆ—å¼ï¼‰å’Œè®¡ç®—å­¦ï¼ˆæ±‚è§£ç³»ç»Ÿå’Œæ£€æŸ¥å¥‡ç‚¹ï¼‰ä¸­éƒ½ä¸å¯æˆ–ç¼ºã€‚</p>
<h3 id="exercises-6.4">Exercises 6.4</h3>
<p>ç»ƒä¹  6.4</p>
<ol type="1">
<li><p>Compute the area of the parallelogram spanned by <span
class="math inline">\((2,1)\)</span> and <span
class="math inline">\((1,3)\)</span>. è®¡ç®— <span
class="math inline">\((2,1)\)</span> å’Œ <span
class="math inline">\((1,3)\)</span> æ‰€æ„æˆçš„å¹³è¡Œå››è¾¹å½¢çš„é¢ç§¯ã€‚</p></li>
<li><p>Find the volume of the parallelepiped spanned by <span
class="math inline">\((1,0,0), (1,1,0), (1,1,1)\)</span>. æ±‚å‡º <span
class="math inline">\((1,0,0), (1,1,0), (1,1,1)\)</span>
æ‰€è·¨åº¦çš„å¹³è¡Œå…­é¢ä½“çš„ä½“ç§¯ã€‚</p></li>
<li><p>Determine whether the matrix ç¡®å®šçŸ©é˜µ</p></li>
</ol>
<p><span class="math display">\[
\begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 6 \end{bmatrix}
\]</span></p>
<p>is invertible. Justify using determinants. 4. Use Cramerâ€™s rule to
solve æ˜¯å¯é€†çš„ã€‚ç”¨è¡Œåˆ—å¼è¯æ˜ã€‚4. ä½¿ç”¨å…‹è±å§†è§„åˆ™æ±‚è§£</p>
<p><span class="math display">\[
\begin{cases}x + y = 3, \\2x - y = 0.\end{cases}
\]</span></p>
<ol start="5" type="1">
<li>Explain geometrically why a determinant of zero implies no inverse
exists. ä»å‡ ä½•è§’åº¦è§£é‡Šä¸ºä»€ä¹ˆè¡Œåˆ—å¼ä¸ºé›¶æ„å‘³ç€ä¸å­˜åœ¨é€†å…ƒã€‚</li>
</ol>
<h1 id="chapter-7.-inner-product-spaces">Chapter 7. Inner Product
Spaces</h1>
<p>ç¬¬ä¸ƒç« å†…ç§¯ç©ºé—´</p>
<h2 id="inner-products-and-norms">7.1 Inner Products and Norms</h2>
<p>7.1 å†…ç§¯å’ŒèŒƒæ•°</p>
<p>To extend the geometric ideas of length, distance, and angle beyond
<span class="math inline">\(\mathbb{R}^2\)</span> and <span
class="math inline">\(\mathbb{R}^3\)</span>, we introduce inner
products. Inner products provide a way of measuring similarity between
vectors, while norms derived from them measure length. These concepts
are the foundation of geometry inside vector spaces.
ä¸ºäº†å°†é•¿åº¦ã€è·ç¦»å’Œè§’åº¦çš„å‡ ä½•æ¦‚å¿µæ‰©å±•åˆ° <span
class="math inline">\(\mathbb{R}^2\)</span> å’Œ <span
class="math inline">\(\mathbb{R}^3\)</span>
ä¹‹å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å†…ç§¯ã€‚å†…ç§¯æä¾›äº†ä¸€ç§åº¦é‡å‘é‡ä¹‹é—´ç›¸ä¼¼æ€§çš„æ–¹æ³•ï¼Œè€Œç”±å†…ç§¯å¯¼å‡ºçš„èŒƒæ•°åˆ™ç”¨äºåº¦é‡é•¿åº¦ã€‚è¿™äº›æ¦‚å¿µæ˜¯å‘é‡ç©ºé—´å‡ ä½•çš„åŸºç¡€ã€‚</p>
<h3 id="inner-product">Inner Product</h3>
<p>å†…ç§¯</p>
<p>An inner product on a real vector space <span
class="math inline">\(V\)</span> is a function å®å‘é‡ç©ºé—´ <span
class="math inline">\(V\)</span> ä¸Šçš„å†…ç§¯æ˜¯ä¸€ä¸ªå‡½æ•°</p>
<p><span class="math display">\[
\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}
\]</span></p>
<p>that assigns to each pair of vectors <span
class="math inline">\((\mathbf{u}, \mathbf{v})\)</span> a real number,
subject to the following properties: ä¸ºæ¯å¯¹å‘é‡ <span
class="math inline">\((\mathbf{u}, \mathbf{v})\)</span>
åˆ†é…ä¸€ä¸ªå®æ•°ï¼Œå¹¶éµå¾ªä»¥ä¸‹å±æ€§ï¼š</p>
<ol type="1">
<li><p>Symmetry: <span class="math inline">\(\langle \mathbf{u},
\mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.\)</span>
å¯¹ç§°ï¼š <span class="math inline">\(\langle \mathbf{u}, \mathbf{v}
\rangle = \langle \mathbf{v}, \mathbf{u} \rangle.\)</span></p></li>
<li><p>Linearity in the first argument: <span
class="math inline">\(\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v}
\rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle
\mathbf{w}, \mathbf{v} \rangle.\)</span> ç¬¬ä¸€ä¸ªå‚æ•°çš„çº¿æ€§ï¼š <span
class="math inline">\(\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v}
\rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle
\mathbf{w}, \mathbf{v} \rangle.\)</span></p></li>
<li><p>Positive-definiteness: <span class="math inline">\(\langle
\mathbf{v}, \mathbf{v} \rangle \geq 0\)</span>, and equality holds if
and only if <span class="math inline">\(\mathbf{v} =
\mathbf{0}\)</span>. æ­£å®šæ€§ï¼š <span class="math inline">\(\langle
\mathbf{v}, \mathbf{v} \rangle \geq 0\)</span> ï¼Œä¸”ä»…å½“ <span
class="math inline">\(\mathbf{v} = \mathbf{0}\)</span>
æ—¶ç­‰å¼æˆç«‹ã€‚</p></li>
</ol>
<p>The standard inner product on <span
class="math inline">\(\mathbb{R}^n\)</span> is the dot product: <span
class="math inline">\(\mathbb{R}^n\)</span> ä¸Šçš„æ ‡å‡†å†…ç§¯æ˜¯ç‚¹ç§¯ï¼š</p>
<p><span class="math display">\[
\langle \mathbf{u}, \mathbf{v} \rangle = u_1 v_1 + u_2 v_2 + \cdots +
u_n v_n.
\]</span></p>
<h3 id="norms">Norms</h3>
<p>è§„èŒƒ</p>
<p>The norm of a vector is its length, defined in terms of the inner
product: å‘é‡çš„èŒƒæ•°æ˜¯å…¶é•¿åº¦ï¼Œæ ¹æ®å†…ç§¯å®šä¹‰ï¼š</p>
<p><span class="math display">\[
\|\mathbf{v}\| = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}.
\]</span></p>
<p>For the dot product in <span
class="math inline">\(\mathbb{R}^n\)</span>: å¯¹äº <span
class="math inline">\(\mathbb{R}^n\)</span> ä¸­çš„ç‚¹ç§¯ï¼š</p>
<p><span class="math display">\[
\|(x_1, x_2, \dots, x_n)\| = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}.
\]</span></p>
<h3 id="angles-between-vectors-1">Angles Between Vectors</h3>
<p>å‘é‡ä¹‹é—´çš„è§’åº¦</p>
<p>The inner product allows us to define the angle <span
class="math inline">\(\theta\)</span> between two nonzero vectors <span
class="math inline">\(\mathbf{u}, \mathbf{v}\)</span> by
é€šè¿‡å†…ç§¯ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸¤ä¸ªéé›¶å‘é‡ <span
class="math inline">\(\mathbf{u}, \mathbf{v}\)</span> ä¹‹é—´çš„è§’åº¦ <span
class="math inline">\(\theta\)</span> ï¼Œå³</p>
<p><span class="math display">\[
\cos \theta = \frac{\langle \mathbf{u}, \mathbf{v}
\rangle}{\|\mathbf{u}\| \, \|\mathbf{v}\|}.
\]</span></p>
<p>Thus, two vectors are orthogonal if <span
class="math inline">\(\langle \mathbf{u}, \mathbf{v} \rangle =
0\)</span>. å› æ­¤ï¼Œè‹¥ <span class="math inline">\(\langle \mathbf{u},
\mathbf{v} \rangle = 0\)</span> ï¼Œåˆ™ä¸¤ä¸ªå‘é‡æ­£äº¤ã€‚</p>
<h3 id="examples-5">Examples</h3>
<p>ç¤ºä¾‹</p>
<p>Example 7.1.1. In <span class="math inline">\(\mathbb{R}^2\)</span>,
with <span class="math inline">\(\mathbf{u} = (1,2)\)</span>, <span
class="math inline">\(\mathbf{v} = (3,4)\)</span>: ä¾‹ 7.1.1ã€‚ åœ¨ <span
class="math inline">\(\mathbb{R}^2\)</span> ä¸­ï¼Œä½¿ç”¨ <span
class="math inline">\(\mathbf{u} = (1,2)\)</span> ã€ <span
class="math inline">\(\mathbf{v} = (3,4)\)</span> ï¼š</p>
<p><span class="math display">\[
\langle \mathbf{u}, \mathbf{v} \rangle = 1\cdot 3 + 2\cdot 4 = 11.
\]</span></p>
<p><span class="math display">\[
\|\mathbf{u}\| = \sqrt{1^2 + 2^2} = \sqrt{5}, \quad \|\mathbf{v}\| =
\sqrt{3^2 + 4^2} = 5.
\]</span></p>
<p>So, æ‰€ä»¥ï¼Œ</p>
<p><span class="math display">\[
\cos \theta = \frac{11}{\sqrt{5}\cdot 5}.
\]</span></p>
<p>Example 7.1.2. In the function space <span
class="math inline">\(C[0,1]\)</span>, the inner product ä¾‹ 7.1.2ã€‚
åœ¨å‡½æ•°ç©ºé—´ <span class="math inline">\(C[0,1]\)</span> ä¸­ï¼Œå†…ç§¯</p>
<p><span class="math display">\[
\langle f, g \rangle = \int_0^1 f(x) g(x)\, dx
\]</span></p>
<p>defines a length å®šä¹‰é•¿åº¦</p>
<p><span class="math display">\[
\|f\| = \sqrt{\int_0^1 f(x)^2 dx}.
\]</span></p>
<p>This generalizes geometry to infinite-dimensional spaces.
è¿™å°†å‡ ä½•å­¦æ¨å¹¿åˆ°æ— é™ç»´ç©ºé—´ã€‚</p>
<h3 id="geometric-interpretation-12">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<ul>
<li>Inner product: measures similarity between vectors.
å†…ç§¯ï¼šæµ‹é‡å‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>Norm: length of a vector. èŒƒæ•°ï¼šå‘é‡çš„é•¿åº¦ã€‚</li>
<li>Angle: measure of alignment between two directions.
è§’åº¦ï¼šä¸¤ä¸ªæ–¹å‘ä¹‹é—´çš„å¯¹é½åº¦é‡ã€‚</li>
</ul>
<p>These concepts unify algebraic operations with geometric intuition.
è¿™äº›æ¦‚å¿µå°†ä»£æ•°è¿ç®—ä¸å‡ ä½•ç›´è§‰ç»Ÿä¸€èµ·æ¥ã€‚</p>
<h3 id="why-this-matters-23">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Inner products and norms allow us to extend geometry into abstract
vector spaces. They form the basis of orthogonality, projections,
Fourier series, least squares approximation, and many applications in
physics and machine learning.
å†…ç§¯å’ŒèŒƒæ•°ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†å‡ ä½•æ‰©å±•åˆ°æŠ½è±¡å‘é‡ç©ºé—´ã€‚å®ƒä»¬æ„æˆäº†æ­£äº¤æ€§ã€æŠ•å½±ã€å‚…é‡Œå¶çº§æ•°ã€æœ€å°äºŒä¹˜è¿‘ä¼¼ä»¥åŠç‰©ç†å­¦å’Œæœºå™¨å­¦ä¹ ä¸­è®¸å¤šåº”ç”¨çš„åŸºç¡€ã€‚</p>
<h3 id="exercises-7.1">Exercises 7.1</h3>
<p>ç»ƒä¹  7.1</p>
<ol type="1">
<li><p>Compute <span class="math inline">\(\langle (2,-1,3), (1,4,0)
\rangle\)</span>. Then find the angle between them. è®¡ç®— <span
class="math inline">\(\langle (2,-1,3), (1,4,0) \rangle\)</span>
ã€‚ç„¶åæ±‚å‡ºå®ƒä»¬ä¹‹é—´çš„è§’åº¦ã€‚</p></li>
<li><p>Show that <span class="math inline">\(\|(x,y)\| =
\sqrt{x^2+y^2}\)</span> satisfies the properties of a norm. è¯æ˜âˆ¥ ( ğ‘¥ ,
ğ‘¦ ) âˆ¥ = ğ‘¥ 2 + ğ‘¦ 2 âˆ¥(x,y)âˆ¥= x 2 +y 2 â€‹ æ»¡è¶³èŒƒæ•°çš„æ€§è´¨ã€‚</p></li>
<li><p>In <span class="math inline">\(\mathbb{R}^3\)</span>, verify that
<span class="math inline">\((1,1,0)\)</span> and <span
class="math inline">\((1,-1,0)\)</span> are orthogonal. åœ¨ <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­ï¼ŒéªŒè¯ <span
class="math inline">\((1,1,0)\)</span> å’Œ <span
class="math inline">\((1,-1,0)\)</span> æ˜¯å¦æ­£äº¤ã€‚</p></li>
<li><p>In <span class="math inline">\(C[0,1]\)</span>, compute <span
class="math inline">\(\langle f,g \rangle\)</span> for <span
class="math inline">\(f(x)=x\)</span>, <span
class="math inline">\(g(x)=1\)</span>. åœ¨ <span
class="math inline">\(C[0,1]\)</span> ä¸­ï¼Œè®¡ç®— <span
class="math inline">\(f(x)=x\)</span> ã€ <span
class="math inline">\(g(x)=1\)</span> çš„ <span
class="math inline">\(\langle f,g \rangle\)</span> ã€‚</p></li>
<li><p>Prove the Cauchyâ€“Schwarz inequality: è¯æ˜æŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼ï¼š</p>
<p><span class="math display">\[
|\langle \mathbf{u}, \mathbf{v} \rangle| \leq \|\mathbf{u}\| \,
\|\mathbf{v}\|.
\]</span></p></li>
</ol>
<h2 id="orthogonal-projections">7.2 Orthogonal Projections</h2>
<p>7.2 æ­£äº¤æŠ•å½±</p>
<p>One of the most useful applications of inner products is the notion
of orthogonal projection. Projection allows us to approximate a vector
by another lying in a subspace, minimizing error in the sense of
distance. This idea underpins geometry, statistics, and numerical
analysis.
å†…ç§¯æœ€æœ‰ç”¨çš„åº”ç”¨ä¹‹ä¸€æ˜¯æ­£äº¤æŠ•å½±çš„æ¦‚å¿µã€‚æŠ•å½±ä½¿æˆ‘ä»¬èƒ½å¤Ÿç”¨å­ç©ºé—´ä¸­çš„å¦ä¸€ä¸ªå‘é‡æ¥è¿‘ä¼¼ä¸€ä¸ªå‘é‡ï¼Œä»è€Œæœ€å°åŒ–è·ç¦»æ–¹å‘ä¸Šçš„è¯¯å·®ã€‚è¿™ä¸€æ€æƒ³æ˜¯å‡ ä½•ã€ç»Ÿè®¡å­¦å’Œæ•°å€¼åˆ†æçš„åŸºç¡€ã€‚</p>
<h3 id="projection-onto-a-line">Projection onto a Line</h3>
<p>æŠ•å½±åˆ°çº¿ä¸Š</p>
<p>Let <span class="math inline">\(\mathbf{u} \in \mathbb{R}^n\)</span>
be a nonzero vector. The line spanned by <span
class="math inline">\(\mathbf{u}\)</span> is ä»¤ <span
class="math inline">\(\mathbf{u} \in \mathbb{R}^n\)</span> ä¸ºéé›¶å‘é‡ã€‚
<span class="math inline">\(\mathbf{u}\)</span> æ‰€æ„æˆçš„çº¿æ®µä¸º</p>
<p><span class="math display">\[
L = \{ c\mathbf{u} \mid c \in \mathbb{R} \}.
\]</span></p>
<p>Given a vector <span class="math inline">\(\mathbf{v}\)</span>, the
projection of <span class="math inline">\(\mathbf{v}\)</span> onto <span
class="math inline">\(\mathbf{u}\)</span> is the vector in <span
class="math inline">\(L\)</span> closest to <span
class="math inline">\(\mathbf{v}\)</span>. Geometrically, it is the
shadow of <span class="math inline">\(\mathbf{v}\)</span> on the line.
ç»™å®šå‘é‡ <span class="math inline">\(\mathbf{v}\)</span> ï¼Œ <span
class="math inline">\(\mathbf{v}\)</span> åœ¨ <span
class="math inline">\(\mathbf{u}\)</span> ä¸Šçš„æŠ•å½±æ˜¯ <span
class="math inline">\(L\)</span> ä¸­è·ç¦» <span
class="math inline">\(\mathbf{v}\)</span> æœ€è¿‘çš„å‘é‡ã€‚ä»å‡ ä½•å­¦ä¸Šè®²ï¼Œå®ƒæ˜¯
<span class="math inline">\(\mathbf{v}\)</span> åœ¨çº¿ä¸Šçš„é˜´å½±ã€‚</p>
<p>The formula is å…¬å¼æ˜¯</p>
<p><span class="math display">\[
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\langle \mathbf{v},
\mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle} \,
\mathbf{u}.
\]</span></p>
<p>The error vector <span class="math inline">\(\mathbf{v} -
\text{proj}_{\mathbf{u}}(\mathbf{v})\)</span> is orthogonal to <span
class="math inline">\(\mathbf{u}\)</span>. è¯¯å·®å‘é‡ <span
class="math inline">\(\mathbf{v} -
\text{proj}_{\mathbf{u}}(\mathbf{v})\)</span> ä¸ <span
class="math inline">\(\mathbf{u}\)</span> æ­£äº¤ã€‚</p>
<h3 id="example-7.2.1">Example 7.2.1</h3>
<p>ä¾‹ 7.2.1</p>
<p>Let <span class="math inline">\(\mathbf{u} = (1,2)\)</span>, <span
class="math inline">\(\mathbf{v} = (3,1)\)</span>. ä»¤ <span
class="math inline">\(\mathbf{u} = (1,2)\)</span> ï¼Œ <span
class="math inline">\(\mathbf{v} = (3,1)\)</span> ã€‚</p>
<p><span class="math display">\[
\langle \mathbf{v}, \mathbf{u} \rangle = 3\cdot 1 + 1\cdot 2 = 5,
\quad\langle \mathbf{u}, \mathbf{u} \rangle = 1^2 + 2^2 = 5.
\]</span></p>
<p>So æ‰€ä»¥</p>
<p><span class="math display">\[
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{5}{5}(1,2) = (1,2).
\]</span></p>
<p>The error vector is <span class="math inline">\((3,1) - (1,2) =
(2,-1)\)</span>, which is orthogonal to <span
class="math inline">\((1,2)\)</span>. è¯¯å·®å‘é‡ä¸º <span
class="math inline">\((3,1) - (1,2) = (2,-1)\)</span> ï¼Œä¸ <span
class="math inline">\((1,2)\)</span> æ­£äº¤ã€‚</p>
<h3 id="projection-onto-a-subspace">Projection onto a Subspace</h3>
<p>æŠ•å½±åˆ°å­ç©ºé—´</p>
<p>Suppose <span class="math inline">\(W \subseteq \mathbb{R}^n\)</span>
is a subspace with orthonormal basis <span class="math inline">\(\{
\mathbf{w}_1, \dots, \mathbf{w}_k \}\)</span>. The projection of a
vector <span class="math inline">\(\mathbf{v}\)</span> onto <span
class="math inline">\(W\)</span> is å‡è®¾ <span class="math inline">\(W
\subseteq \mathbb{R}^n\)</span> æ˜¯ä¸€ä¸ªå…·æœ‰æ­£äº¤åŸº <span
class="math inline">\(\{ \mathbf{w}_1, \dots, \mathbf{w}_k \}\)</span>
çš„å­ç©ºé—´ã€‚å‘é‡ <span class="math inline">\(\mathbf{v}\)</span> åœ¨ <span
class="math inline">\(W\)</span> ä¸Šçš„æŠ•å½±ä¸º</p>
<p><span class="math display">\[
\text{proj}_{W}(\mathbf{v}) = \langle \mathbf{v}, \mathbf{w}_1 \rangle
\mathbf{w}_1 + \cdots + \langle \mathbf{v}, \mathbf{w}_k \rangle
\mathbf{w}_k.
\]</span></p>
<p>This is the unique vector in <span class="math inline">\(W\)</span>
closest to <span class="math inline">\(\mathbf{v}\)</span>. The
difference <span class="math inline">\(\mathbf{v} -
\text{proj}_{W}(\mathbf{v})\)</span> is orthogonal to all of <span
class="math inline">\(W\)</span>. è¿™æ˜¯ <span
class="math inline">\(W\)</span> ä¸­ä¸ <span
class="math inline">\(\mathbf{v}\)</span> æœ€æ¥è¿‘çš„å”¯ä¸€å‘é‡ã€‚å·®å€¼ <span
class="math inline">\(\mathbf{v} - \text{proj}_{W}(\mathbf{v})\)</span>
ä¸æ‰€æœ‰ <span class="math inline">\(W\)</span> æ­£äº¤ã€‚</p>
<h3 id="least-squares-approximation">Least Squares Approximation</h3>
<p>æœ€å°äºŒä¹˜è¿‘ä¼¼</p>
<p>Orthogonal projection explains the method of least squares. To solve
an overdetermined system <span class="math inline">\(A\mathbf{x} \approx
\mathbf{b}\)</span>, we seek the <span
class="math inline">\(\mathbf{x}\)</span> that makes <span
class="math inline">\(A\mathbf{x}\)</span> the projection of <span
class="math inline">\(\mathbf{b}\)</span> onto the column space of <span
class="math inline">\(A\)</span>. This gives the normal equations
æ­£äº¤æŠ•å½±è§£é‡Šäº†æœ€å°äºŒä¹˜æ³•ã€‚ä¸ºäº†è§£å†³è¶…å®šé—®é¢˜ ç³»ç»Ÿ <span
class="math inline">\(A\mathbf{x} \approx \mathbf{b}\)</span> ï¼Œæˆ‘ä»¬å¯»æ‰¾
<span class="math inline">\(\mathbf{x}\)</span> ï¼Œä½¿å¾— <span
class="math inline">\(A\mathbf{x}\)</span> æˆä¸º <span
class="math inline">\(\mathbf{b}\)</span> åœ¨ <span
class="math inline">\(A\)</span> çš„åˆ—ç©ºé—´ä¸Šçš„æŠ•å½±ã€‚è¿™ç»™å‡ºäº†æ­£åˆ™æ–¹ç¨‹</p>
<p><span class="math display">\[
A^T A \mathbf{x} = A^T \mathbf{b}.
\]</span></p>
<p>Thus, least squares is just projection in disguise.
å› æ­¤ï¼Œæœ€å°äºŒä¹˜æ³•åªæ˜¯ä¼ªè£…çš„æŠ•å½±ã€‚</p>
<h3 id="geometric-interpretation-13">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<ul>
<li>Projection finds the closest point in a subspace to a given vector.
æŠ•å½±æ‰¾åˆ°å­ç©ºé—´ä¸­è·ç¦»ç»™å®šå‘é‡æœ€è¿‘çš„ç‚¹ã€‚</li>
<li>It minimizes distance (error) in the sense of Euclidean norm.
å®ƒæŒ‰ç…§æ¬§å‡ é‡Œå¾—èŒƒæ•°çš„æ„ä¹‰æœ€å°åŒ–è·ç¦»ï¼ˆè¯¯å·®ï¼‰ã€‚</li>
<li>Orthogonality ensures the error vector points directly away from the
subspace. æ­£äº¤æ€§ç¡®ä¿è¯¯å·®å‘é‡ç›´æ¥æŒ‡å‘è¿œç¦»å­ç©ºé—´çš„æ–¹å‘ã€‚</li>
</ul>
<h3 id="why-this-matters-24">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Orthogonal projection is central in both pure and applied
mathematics. It underlies the geometry of subspaces, the theory of
Fourier series, regression in statistics, and approximation methods in
numerical linear algebra. Whenever we fit data with a simpler model,
projection is at work.
æ­£äº¤æŠ•å½±åœ¨çº¯æ•°å­¦å’Œåº”ç”¨æ•°å­¦ä¸­éƒ½è‡³å…³é‡è¦ã€‚å®ƒæ˜¯å­ç©ºé—´å‡ ä½•ã€å‚…é‡Œå¶çº§æ•°ç†è®ºã€ç»Ÿè®¡å­¦ä¸­çš„å›å½’ä»¥åŠæ•°å€¼çº¿æ€§ä»£æ•°ä¸­çš„è¿‘ä¼¼æ–¹æ³•çš„åŸºç¡€ã€‚æ¯å½“æˆ‘ä»¬ç”¨æ›´ç®€å•çš„æ¨¡å‹æ‹Ÿåˆæ•°æ®æ—¶ï¼ŒæŠ•å½±å°±ä¼šå‘æŒ¥ä½œç”¨ã€‚</p>
<h3 id="exercises-7.2">Exercises 7.2</h3>
<p>ç»ƒä¹  7.2</p>
<ol type="1">
<li>Compute the projection of <span class="math inline">\((2,3)\)</span>
onto the vector <span class="math inline">\((1,1)\)</span>. è®¡ç®— <span
class="math inline">\((2,3)\)</span> åˆ°å‘é‡ <span
class="math inline">\((1,1)\)</span> çš„æŠ•å½±ã€‚</li>
<li>Show that <span class="math inline">\(\mathbf{v} -
\text{proj}_{\mathbf{u}}(\mathbf{v})\)</span> is orthogonal to <span
class="math inline">\(\mathbf{u}\)</span>. è¯æ˜ <span
class="math inline">\(\mathbf{v} -
\text{proj}_{\mathbf{u}}(\mathbf{v})\)</span> ä¸ <span
class="math inline">\(\mathbf{u}\)</span> æ­£äº¤ã€‚</li>
<li>Let <span class="math inline">\(W = \text{span}\{(1,0,0), (0,1,0)\}
\subseteq \mathbb{R}^3\)</span>. Find the projection of <span
class="math inline">\((1,2,3)\)</span> onto <span
class="math inline">\(W\)</span>. ä»¤ <span class="math inline">\(W =
\text{span}\{(1,0,0), (0,1,0)\} \subseteq \mathbb{R}^3\)</span> ã€‚æ±‚
<span class="math inline">\((1,2,3)\)</span> åˆ° <span
class="math inline">\(W\)</span> çš„æŠ•å½±ã€‚</li>
<li>Explain why least squares fitting corresponds to projection onto the
column space of <span class="math inline">\(A\)</span>.
è§£é‡Šä¸ºä»€ä¹ˆæœ€å°äºŒä¹˜æ‹Ÿåˆå¯¹åº”äº <span class="math inline">\(A\)</span>
çš„åˆ—ç©ºé—´ä¸Šçš„æŠ•å½±ã€‚</li>
<li>Prove that projection onto a subspace <span
class="math inline">\(W\)</span> is unique: there is exactly one closest
vector in <span class="math inline">\(W\)</span> to a given <span
class="math inline">\(\mathbf{v}\)</span>. è¯æ˜æŠ•å½±åˆ°å­ç©ºé—´ <span
class="math inline">\(W\)</span> æ˜¯å”¯ä¸€çš„ï¼šåœ¨ <span
class="math inline">\(W\)</span> ä¸­ï¼Œæœ‰ä¸”ä»…æœ‰ä¸€ä¸ªä¸ç»™å®š <span
class="math inline">\(\mathbf{v}\)</span> æœ€æ¥è¿‘çš„å‘é‡ã€‚</li>
</ol>
<h2 id="gramschmidt-process">7.3 Gramâ€“Schmidt Process</h2>
<p>7.3 æ ¼æ‹‰å§†-æ–½å¯†ç‰¹è¿‡ç¨‹</p>
<p>The Gramâ€“Schmidt process is a systematic way to turn any linearly
independent set of vectors into an orthonormal basis. This is especially
useful because orthonormal bases simplify computations: inner products
become simple coordinate comparisons, and projections take clean forms.
æ ¼æ‹‰å§†-æ–½å¯†ç‰¹è¿‡ç¨‹æ˜¯ä¸€ç§å°†ä»»æ„çº¿æ€§æ— å…³çš„å‘é‡é›†è½¬åŒ–ä¸ºæ­£äº¤åŸºçš„ç³»ç»Ÿæ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å°¤å…¶æœ‰ç”¨ï¼Œå› ä¸ºæ­£äº¤åŸºå¯ä»¥ç®€åŒ–è®¡ç®—ï¼šå†…ç§¯å˜æˆäº†ç®€å•çš„åæ ‡æ¯”è¾ƒï¼Œå¹¶ä¸”æŠ•å½±å‘ˆç°å‡ºæ¸…æ™°çš„å½¢å¼ã€‚</p>
<h3 id="the-idea">The Idea</h3>
<p>ç†å¿µ</p>
<p>Given a linearly independent set of vectors <span
class="math inline">\(\{\mathbf{v}_1, \mathbf{v}_2, \dots,
\mathbf{v}_n\}\)</span> in an inner product space, we want to construct
an orthonormal set <span class="math inline">\(\{\mathbf{u}_1,
\mathbf{u}_2, \dots, \mathbf{u}_n\}\)</span> that spans the same
subspace. ç»™å®šå†…ç§¯ç©ºé—´ä¸­ä¸€ç»„çº¿æ€§æ— å…³çš„å‘é‡ <span
class="math inline">\(\{\mathbf{v}_1, \mathbf{v}_2, \dots,
\mathbf{v}_n\}\)</span> ï¼Œæˆ‘ä»¬æƒ³è¦æ„å»ºä¸€ä¸ªè·¨è¶ŠåŒä¸€å­ç©ºé—´çš„æ­£äº¤é›† <span
class="math inline">\(\{\mathbf{u}_1, \mathbf{u}_2, \dots,
\mathbf{u}_n\}\)</span> ã€‚</p>
<p>We proceed step by step: æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥ï¼š</p>
<ol type="1">
<li>Start with <span class="math inline">\(\mathbf{v}_1\)</span>,
normalize it to get <span class="math inline">\(\mathbf{u}_1\)</span>.
ä»ğ‘£å¼€å§‹ 1 v 1 â€‹ ï¼Œå°†å…¶æ ‡å‡†åŒ–å¾—åˆ°ğ‘¢ 1 u 1 â€‹ .</li>
<li>Subtract from <span class="math inline">\(\mathbf{v}_2\)</span> its
projection onto <span class="math inline">\(\mathbf{u}_1\)</span>,
leaving a vector orthogonal to <span
class="math inline">\(\mathbf{u}_1\)</span>. Normalize to get <span
class="math inline">\(\mathbf{u}_2\)</span>. ä»ğ‘£ä¸­å‡å» 2 v 2 â€‹
å®ƒåœ¨ğ‘¢ä¸Šçš„æŠ•å½± 1 u 1 â€‹ ï¼Œç•™ä¸‹ä¸€ä¸ªä¸ğ‘¢æ­£äº¤çš„å‘é‡ 1 u 1 â€‹ . æ ‡å‡†åŒ–å¾—åˆ°ğ‘¢ 2 u 2 â€‹
.</li>
<li>For each <span class="math inline">\(\mathbf{v}_k\)</span>, subtract
projections onto all previously constructed <span
class="math inline">\(\mathbf{u}_1, \dots, \mathbf{u}_{k-1}\)</span>,
then normalize. å¯¹äºæ¯ä¸ªğ‘£ ğ‘˜ v k â€‹ ï¼Œå‡å»æ‰€æœ‰å…ˆå‰æ„å»ºçš„ğ‘¢ä¸Šçš„æŠ•å½± 1 , â€¦ , ğ‘¢
ğ‘˜ âˆ’ 1 u 1 â€‹ ï¼Œâ€¦ï¼Œä½  kâˆ’1 â€‹ ï¼Œç„¶åæ ‡å‡†åŒ–ã€‚</li>
</ol>
<h3 id="the-algorithm">The Algorithm</h3>
<p>ç®—æ³•</p>
<p>For <span class="math inline">\(k = 1, 2, \dots, n\)</span>: å¯¹äº
<span class="math inline">\(k = 1, 2, \dots, n\)</span> ï¼š</p>
<p><span class="math display">\[
\mathbf{w}_k = \mathbf{v}_k - \sum_{j=1}^{k-1} \langle \mathbf{v}_k,
\mathbf{u}_j \rangle \mathbf{u}_j,
\]</span></p>
<p><span class="math display">\[
\mathbf{u}_k = \frac{\mathbf{w}_k}{\|\mathbf{w}_k\|}.
\]</span></p>
<p>The result <span class="math inline">\(\{\mathbf{u}_1, \dots,
\mathbf{u}_n\}\)</span> is an orthonormal basis of the span of the
original vectors. ç»“æœ <span class="math inline">\(\{\mathbf{u}_1,
\dots, \mathbf{u}_n\}\)</span> æ˜¯åŸå§‹å‘é‡è·¨åº¦çš„æ­£äº¤åŸºã€‚</p>
<h3 id="example-7.3.1">Example 7.3.1</h3>
<p>ä¾‹ 7.3.1</p>
<p>Take <span class="math inline">\(\mathbf{v}_1 = (1,1,0), \
\mathbf{v}_2 = (1,0,1), \ \mathbf{v}_3 = (0,1,1)\)</span> in <span
class="math inline">\(\mathbb{R}^3\)</span>. åœ¨ <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­ä¹˜å <span
class="math inline">\(\mathbf{v}_1 = (1,1,0), \ \mathbf{v}_2 = (1,0,1),
\ \mathbf{v}_3 = (0,1,1)\)</span> ã€‚</p>
<ol type="1">
<li>Normalize <span class="math inline">\(\mathbf{v}_1\)</span>: æ ‡å‡†åŒ–ğ‘£
1 v 1 â€‹ :</li>
</ol>
<p><span class="math display">\[
\mathbf{u}_1 = \frac{1}{\sqrt{2}}(1,1,0).
\]</span></p>
<ol start="2" type="1">
<li>Subtract projection of <span
class="math inline">\(\mathbf{v}_2\)</span> on <span
class="math inline">\(\mathbf{u}_1\)</span>: å‡å»ğ‘£çš„æŠ•å½± 2 v 2 â€‹ åœ¨ğ‘¢ 1 u
1 â€‹ :</li>
</ol>
<p><span class="math display">\[
\mathbf{w}_2 = \mathbf{v}_2 - \langle \mathbf{v}_2,\mathbf{u}_1 \rangle
\mathbf{u}_1.
\]</span></p>
<p><span class="math display">\[
\langle \mathbf{v}_2,\mathbf{u}_1 \rangle = \frac{1}{\sqrt{2}}(1\cdot 1
+ 0\cdot 1 + 1\cdot 0) = \tfrac{1}{\sqrt{2}}.
\]</span></p>
<p>So æ‰€ä»¥</p>
<p><span class="math display">\[
\mathbf{w}_2 = (1,0,1) - \tfrac{1}{\sqrt{2}}\cdot
\tfrac{1}{\sqrt{2}}(1,1,0)= (1,0,1) - \tfrac{1}{2}(1,1,0)=
\left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
\]</span></p>
<p>Normalize: è§„èŒƒåŒ–ï¼š</p>
<p><span class="math display">\[
\mathbf{u}_2 = \frac{1}{\sqrt{\tfrac{1}{4}+\tfrac{1}{4}+1}}
\left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)=
\frac{1}{\sqrt{\tfrac{3}{2}}}\left(\tfrac{1}{2}, -\tfrac{1}{2},
1\right).
\]</span></p>
<ol start="3" type="1">
<li>Subtract projections from <span
class="math inline">\(\mathbf{v}_3\)</span>: ä»ğ‘£ä¸­å‡å»æŠ•å½± 3 v 3 â€‹ :</li>
</ol>
<p><span class="math display">\[
\mathbf{w}_3 = \mathbf{v}_3 - \langle \mathbf{v}_3,\mathbf{u}_1 \rangle
\mathbf{u}_1 - \langle \mathbf{v}_3,\mathbf{u}_2 \rangle \mathbf{u}_2.
\]</span></p>
<p>After computing, normalize to obtain <span
class="math inline">\(\mathbf{u}_3\)</span>. è®¡ç®—åï¼Œå½’ä¸€åŒ–å¾—åˆ°ğ‘¢ 3 u 3 â€‹
.</p>
<p>The result is an orthonormal basis of the span of <span
class="math inline">\(\{\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3\}\)</span>.
ç»“æœæ˜¯ <span
class="math inline">\(\{\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3\}\)</span>
è·¨åº¦çš„æ­£äº¤åŸºã€‚</p>
<h3 id="geometric-interpretation-14">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>Gramâ€“Schmidt is like straightening out a set of vectors: you start
with the original directions and adjust each new vector to be
perpendicular to all previous ones. Then you scale to unit length. The
process ensures orthogonality while preserving the span.
æ ¼æ‹‰å§†-æ–½å¯†ç‰¹å˜æ¢å°±åƒæ‹‰ç›´ä¸€ç»„å‘é‡ï¼šä»åŸå§‹æ–¹å‘å¼€å§‹ï¼Œè°ƒæ•´æ¯ä¸ªæ–°å‘é‡ä½¿å…¶ä¸æ‰€æœ‰å…ˆå‰çš„å‘é‡å‚ç›´ã€‚ç„¶åç¼©æ”¾åˆ°å•ä½é•¿åº¦ã€‚è¿™ä¸ªè¿‡ç¨‹ç¡®ä¿äº†æ­£äº¤æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†è·¨åº¦ã€‚</p>
<h3 id="why-this-matters-25">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Orthonormal bases simplify inner products, projections, and
computations in general. They make coordinate systems easier to work
with and are crucial in numerical methods, QR decomposition, Fourier
analysis, and statistics (orthogonal polynomials, principal component
analysis).
æ­£äº¤åŸºå¯ä»¥ç®€åŒ–å†…ç§¯ã€æŠ•å½±å’Œä¸€èˆ¬è®¡ç®—ã€‚å®ƒä»¬ä½¿åæ ‡ç³»æ›´æ˜“äºä½¿ç”¨ï¼Œå¹¶ä¸”åœ¨æ•°å€¼æ–¹æ³•ã€QR
åˆ†è§£ã€å‚…é‡Œå¶åˆ†æå’Œç»Ÿè®¡å­¦ï¼ˆæ­£äº¤å¤šé¡¹å¼ã€ä¸»æˆåˆ†åˆ†æï¼‰ä¸­è‡³å…³é‡è¦ã€‚</p>
<h3 id="exercises-7.3">Exercises 7.3</h3>
<p>ç»ƒä¹  7.3</p>
<ol type="1">
<li>Apply Gramâ€“Schmidt to <span class="math inline">\((1,0),
(1,1)\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>. å¯¹
<span class="math inline">\(\mathbb{R}^2\)</span> ä¸­çš„ <span
class="math inline">\((1,0), (1,1)\)</span> åº”ç”¨ Gramâ€“Schmidt
å…¬å¼ã€‚</li>
<li>Orthogonalize <span class="math inline">\((1,1,1), (1,0,1)\)</span>
in <span class="math inline">\(\mathbb{R}^3\)</span>. åœ¨ <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­å¯¹ <span
class="math inline">\((1,1,1), (1,0,1)\)</span> è¿›è¡Œæ­£äº¤åŒ–ã€‚</li>
<li>Prove that each step of Gramâ€“Schmidt yields a vector orthogonal to
all previous ones. è¯æ˜ Gram-Schmidt
çš„æ¯ä¸€æ­¥éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªä¸æ‰€æœ‰å‰é¢çš„å‘é‡æ­£äº¤çš„å‘é‡ã€‚</li>
<li>Show that Gramâ€“Schmidt preserves the span of the original vectors.
è¯æ˜ Gramâ€“Schmidt ä¿ç•™äº†åŸå§‹å‘é‡çš„è·¨åº¦ã€‚</li>
<li>Explain how Gramâ€“Schmidt leads to the QR decomposition of a matrix.
è§£é‡Š Gram-Schmidt å¦‚ä½•å¯¼è‡´çŸ©é˜µçš„ QR åˆ†è§£ã€‚</li>
</ol>
<h2 id="orthonormal-bases">7.4 Orthonormal Bases</h2>
<p>7.4 æ­£äº¤åŸº</p>
<p>An orthonormal basis is a basis of a vector space in which all
vectors are both orthogonal to each other and have unit length. Such
bases are the most convenient possible coordinate systems: computations
involving inner products, projections, and norms become exceptionally
simple.
æ­£äº¤åŸºæ˜¯å‘é‡ç©ºé—´ä¸­çš„ä¸€ç§åŸºï¼Œå…¶ä¸­æ‰€æœ‰å‘é‡å½¼æ­¤æ­£äº¤ä¸”å…·æœ‰å•ä½é•¿åº¦ã€‚è¿™æ ·çš„åŸºæ˜¯æœ€æ–¹ä¾¿çš„åæ ‡ç³»ï¼šæ¶‰åŠå†…ç§¯ã€æŠ•å½±å’ŒèŒƒæ•°çš„è®¡ç®—å˜å¾—å¼‚å¸¸ç®€å•ã€‚</p>
<h3 id="definition-3">Definition</h3>
<p>å®šä¹‰</p>
<p>A set of vectors <span class="math inline">\(\{\mathbf{u}_1,
\mathbf{u}_2, \dots, \mathbf{u}_n\}\)</span> in an inner product space
<span class="math inline">\(V\)</span> is called an orthonormal basis if
å†…ç§¯ç©ºé—´ <span class="math inline">\(V\)</span> ä¸­çš„ä¸€ç»„å‘é‡ <span
class="math inline">\(\{\mathbf{u}_1, \mathbf{u}_2, \dots,
\mathbf{u}_n\}\)</span> ç§°ä¸ºæ­£äº¤åŸºï¼Œè‹¥</p>
<ol type="1">
<li><span class="math inline">\(\langle \mathbf{u}_i, \mathbf{u}_j
\rangle = 0\)</span> whenever <span class="math inline">\(i \neq
j\)</span> (orthogonality), <span class="math inline">\(\langle
\mathbf{u}_i, \mathbf{u}_j \rangle = 0\)</span> æ¯å½“ <span
class="math inline">\(i \neq j\)</span> ï¼ˆæ­£äº¤æ€§ï¼‰</li>
<li><span class="math inline">\(\|\mathbf{u}_i\| = 1\)</span> for all
<span class="math inline">\(i\)</span> (normalization), å¯¹æ‰€æœ‰ <span
class="math inline">\(i\)</span> è¿›è¡Œ <span
class="math inline">\(\|\mathbf{u}_i\| = 1\)</span> ï¼ˆè§„èŒƒåŒ–ï¼‰ï¼Œ</li>
<li>The set spans <span class="math inline">\(V\)</span>. è¯¥é›†åˆè·¨è¶Š
<span class="math inline">\(V\)</span> ã€‚</li>
</ol>
<h3 id="examples-6">Examples</h3>
<p>ç¤ºä¾‹</p>
<p>Example 7.4.1. In <span class="math inline">\(\mathbb{R}^2\)</span>,
the standard basis ä¾‹ 7.4.1. åœ¨ <span
class="math inline">\(\mathbb{R}^2\)</span> ä¸­ï¼Œæ ‡å‡†åŸºç¡€</p>
<p><span class="math display">\[
\mathbf{e}_1 = (1,0), \quad \mathbf{e}_2 = (0,1)
\]</span></p>
<p>is orthonormal under the dot product. åœ¨ç‚¹ç§¯ä¸‹æ˜¯æ­£äº¤çš„ã€‚</p>
<p>Example 7.4.2. In <span class="math inline">\(\mathbb{R}^3\)</span>,
the standard basis ä¾‹ 7.4.2. åœ¨ <span
class="math inline">\(\mathbb{R}^3\)</span> ä¸­ï¼Œæ ‡å‡†åŸºç¡€</p>
<p><span class="math display">\[
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3
= (0,0,1)
\]</span></p>
<p>is orthonormal. æ˜¯æ­£äº¤çš„ã€‚</p>
<p>Example 7.4.3. Fourier basis on functions: ä¾‹ 7.4.3.
å‡½æ•°çš„å‚…é‡Œå¶åŸºï¼š</p>
<p><span class="math display">\[
\{1, \cos x, \sin x, \cos 2x, \sin 2x, \dots\}
\]</span></p>
<p>is an orthogonal set in the space of square-integrable functions on
<span class="math inline">\([-\pi,\pi]\)</span> with inner product æ˜¯
<span class="math inline">\([-\pi,\pi]\)</span>
ä¸Šå¹³æ–¹å¯ç§¯å‡½æ•°ç©ºé—´ä¸­çš„æ­£äº¤é›†ï¼Œå…·æœ‰å†…ç§¯</p>
<p><span class="math display">\[
\langle f,g \rangle = \int_{-\pi}^{\pi} f(x) g(x)\, dx.
\]</span></p>
<p>After normalization, it becomes an orthonormal basis.
ç»è¿‡å½’ä¸€åŒ–ä¹‹åï¼Œå®ƒå°±å˜æˆäº†æ­£äº¤åŸºã€‚</p>
<h3 id="properties">Properties</h3>
<p>ç‰¹æ€§</p>
<ol type="1">
<li><p>Coordinate simplicity: If <span
class="math inline">\(\{\mathbf{u}_1,\dots,\mathbf{u}_n\}\)</span> is an
orthonormal basis of <span class="math inline">\(V\)</span>, then any
vector <span class="math inline">\(\mathbf{v}\in V\)</span> has
coordinates åæ ‡ç®€å•æ€§ï¼šå¦‚æœ <span
class="math inline">\(\{\mathbf{u}_1,\dots,\mathbf{u}_n\}\)</span> æ˜¯
<span class="math inline">\(V\)</span> çš„æ­£äº¤åŸºï¼Œåˆ™ä»»ä½•å‘é‡ <span
class="math inline">\(\mathbf{v}\in V\)</span> éƒ½æœ‰åæ ‡</p>
<p><span class="math display">\[
[\mathbf{v}] = \begin{bmatrix} \langle \mathbf{v}, \mathbf{u}_1 \rangle
\\ \vdots \\ \langle \mathbf{v}, \mathbf{u}_n \rangle \end{bmatrix}.
\]</span></p>
<p>That is, coordinates are just inner products.
ä¹Ÿå°±æ˜¯è¯´ï¼Œåæ ‡åªæ˜¯å†…ç§¯ã€‚</p></li>
<li><p>Parsevalâ€™s identity: For any <span
class="math inline">\(\mathbf{v} \in V\)</span>, å¸•å¡ç“¦å°”çš„èº«ä»½ï¼š
å¯¹äºä»»æ„çš„ <span class="math inline">\(\mathbf{v} \in V\)</span> ï¼Œ</p>
<p><span class="math display">\[
\|\mathbf{v}\|^2 = \sum_{i=1}^n |\langle \mathbf{v}, \mathbf{u}_i
\rangle|^2.
\]</span></p></li>
<li><p>Projections: The orthogonal projection onto the span of <span
class="math inline">\(\\{\mathbf{u}_1,\dots,\mathbf{u}_k\\}\)</span> is
é¢„æµ‹ï¼š ğ‘¢ è·¨åº¦ä¸Šçš„æ­£äº¤æŠ•å½± 1 , â€¦ , ğ‘¢ ğ‘˜ u 1 â€‹ ï¼Œâ€¦ï¼Œä½  k â€‹ æ˜¯</p>
<p><span class="math display">\[
\text{proj}(\mathbf{v}) = \sum_{i=1}^k \langle \mathbf{v}, \mathbf{u}_i
\rangle \mathbf{u}_i.
\]</span></p></li>
</ol>
<h3 id="constructing-orthonormal-bases">Constructing Orthonormal
Bases</h3>
<p>æ„é€ æ­£äº¤åŸº</p>
<ul>
<li>Start with any linearly independent set, then apply the Gramâ€“Schmidt
process to obtain an orthonormal set spanning the same subspace.
ä»ä»»æ„çº¿æ€§æ— å…³é›†å¼€å§‹ï¼Œç„¶ååº”ç”¨ Gram-Schmidt
è¿‡ç¨‹æ¥è·å–è·¨è¶Šç›¸åŒå­ç©ºé—´çš„æ­£äº¤é›†ã€‚</li>
<li>In practice, orthonormal bases are often chosen for numerical
stability and simplicity of computation.
åœ¨å®è·µä¸­ï¼Œé€šå¸¸é€‰æ‹©æ­£äº¤åŸºæ¥å®ç°æ•°å€¼ç¨³å®šæ€§å’Œè®¡ç®—ç®€å•æ€§ã€‚</li>
</ul>
<h3 id="geometric-interpretation-15">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>An orthonormal basis is like a perfectly aligned and equally scaled
coordinate system. Distances and angles are computed directly using
coordinates without correction factors. They are the ideal rulers of
linear algebra.
æ­£äº¤åŸºå°±åƒä¸€ä¸ªå®Œç¾å¯¹é½ä¸”ç­‰æ¯”ä¾‹ç¼©æ”¾çš„åæ ‡ç³»ã€‚è·ç¦»å’Œè§’åº¦ç›´æ¥ä½¿ç”¨åæ ‡è®¡ç®—ï¼Œæ— éœ€æ ¡æ­£å› å­ã€‚å®ƒä»¬æ˜¯çº¿æ€§ä»£æ•°çš„ç†æƒ³æ ‡å°ºã€‚</p>
<h3 id="why-this-matters-26">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Orthonormal bases simplify every aspect of linear algebra: solving
systems, computing projections, expanding functions, diagonalizing
symmetric matrices, and working with Fourier series. In data science,
principal component analysis produces orthonormal directions capturing
maximum variance.
æ­£äº¤åŸºç®€åŒ–äº†çº¿æ€§ä»£æ•°çš„å„ä¸ªæ–¹é¢ï¼šæ±‚è§£ç³»ç»Ÿã€è®¡ç®—æŠ•å½±ã€å±•å¼€å‡½æ•°ã€å¯¹è§’åŒ–å¯¹ç§°çŸ©é˜µä»¥åŠå¤„ç†å‚…é‡Œå¶çº§æ•°ã€‚åœ¨æ•°æ®ç§‘å­¦ä¸­ï¼Œä¸»æˆåˆ†åˆ†æå¯ä»¥ç”Ÿæˆæ­£äº¤æ–¹å‘ï¼Œä»è€Œæ•æ‰æœ€å¤§æ–¹å·®ã€‚</p>
<h3 id="exercises-7.4">Exercises 7.4</h3>
<p>ç»ƒä¹  7.4</p>
<ol type="1">
<li>Verify that <span class="math inline">\((1/\\sqrt{2})(1,1)\)</span>
and <span class="math inline">\((1/\\sqrt{2})(1,-1)\)</span> form an
orthonormal basis of <span class="math inline">\(\mathbb{R}^2\)</span>.
éªŒè¯ <span class="math inline">\((1/\\sqrt{2})(1,1)\)</span> å’Œ <span
class="math inline">\((1/\\sqrt{2})(1,-1)\)</span> æ˜¯å¦æ„æˆ <span
class="math inline">\(\mathbb{R}^2\)</span> çš„æ­£äº¤åŸºã€‚</li>
<li>Express <span class="math inline">\((3,4)\)</span> in terms of the
orthonormal basis <span class="math inline">\(\{(1/\\sqrt{2})(1,1),
(1/\\sqrt{2})(1,-1)\}\)</span>. ç”¨æ­£äº¤åŸº <span
class="math inline">\(\{(1/\\sqrt{2})(1,1),
(1/\\sqrt{2})(1,-1)\}\)</span> è¡¨ç¤º <span
class="math inline">\((3,4)\)</span> ã€‚</li>
<li>Prove Parsevalâ€™s identity for <span
class="math inline">\(\\mathbb{R}^n\)</span> with the dot product.
ä½¿ç”¨ç‚¹ç§¯è¯æ˜ <span class="math inline">\(\\mathbb{R}^n\)</span>
çš„å¸•å¡ç“¦å°”æ’ç­‰å¼ã€‚</li>
<li>Find an orthonormal basis for the plane <span
class="math inline">\(x+y+z=0\)</span> in <span
class="math inline">\(\\mathbb{R}^3\)</span>. åœ¨ <span
class="math inline">\(\\mathbb{R}^3\)</span> ä¸­æ‰¾å‡ºå¹³é¢ <span
class="math inline">\(x+y+z=0\)</span> çš„æ­£äº¤åŸºã€‚</li>
<li>Explain why orthonormal bases are numerically more stable than
arbitrary bases in computations.
è§£é‡Šä¸ºä»€ä¹ˆæ­£äº¤åŸºåœ¨è®¡ç®—ä¸­æ¯”ä»»æ„åŸºåœ¨æ•°å€¼ä¸Šæ›´ç¨³å®šã€‚</li>
</ol>
<h1 id="chapter-8.-eigenvalues-and-eigenvectors">Chapter 8. Eigenvalues
and eigenvectors</h1>
<p>ç¬¬ 8 ç«  ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡</p>
<h2 id="definitions-and-intuition">8.1 Definitions and Intuition</h2>
<p>8.1 å®šä¹‰å’Œç›´è§‰</p>
<p>The concepts of eigenvalues and eigenvectors reveal the most
fundamental behavior of linear transformations. They identify the
special directions in which a transformation acts by simple stretching
or compressing, without rotation or distortion.
ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„æ¦‚å¿µæ­ç¤ºäº†çº¿æ€§å˜æ¢æœ€åŸºæœ¬çš„è¡Œä¸ºã€‚å®ƒä»¬é€šè¿‡ç®€å•çš„æ‹‰ä¼¸æˆ–å‹ç¼©ï¼ˆä¸è¿›è¡Œæ—‹è½¬æˆ–å˜å½¢ï¼‰æ¥è¯†åˆ«å˜æ¢æ‰€ä½œç”¨çš„ç‰¹å®šæ–¹å‘ã€‚</p>
<h3 id="definition-4">Definition</h3>
<p>å®šä¹‰</p>
<p>Let <span class="math inline">\(T: V \to V\)</span> be a linear
transformation on a vector space <span class="math inline">\(V\)</span>.
A nonzero vector <span class="math inline">\(\mathbf{v} \in V\)</span>
is called an eigenvector of <span class="math inline">\(T\)</span> if ä»¤
<span class="math inline">\(T: V \to V\)</span> ä¸ºå‘é‡ç©ºé—´ <span
class="math inline">\(V\)</span> ä¸Šçš„çº¿æ€§å˜æ¢ã€‚éé›¶å‘é‡ <span
class="math inline">\(\mathbf{v} \in V\)</span> ç§°ä¸º <span
class="math inline">\(T\)</span> çš„ç‰¹å¾å‘é‡ï¼Œè‹¥</p>
<p><span class="math display">\[
T(\mathbf{v}) = \lambda \mathbf{v}
\]</span></p>
<p>for some scalar <span class="math inline">\(\lambda \in
\mathbb{R}\)</span> (or <span
class="math inline">\(\mathbb{C}\)</span>). The scalar <span
class="math inline">\(\lambda\)</span> is the eigenvalue corresponding
to <span class="math inline">\(\mathbf{v}\)</span>. æŸä¸ªæ ‡é‡ <span
class="math inline">\(\lambda \in \mathbb{R}\)</span> ï¼ˆæˆ– <span
class="math inline">\(\mathbb{C}\)</span> ï¼‰ã€‚æ ‡é‡ <span
class="math inline">\(\lambda\)</span> æ˜¯å¯¹åº”äº <span
class="math inline">\(\mathbf{v}\)</span> çš„ç‰¹å¾å€¼ã€‚</p>
<p>Equivalently, if <span class="math inline">\(A\)</span> is the matrix
of <span class="math inline">\(T\)</span>, then eigenvalues and
eigenvectors satisfy ç­‰ä»·åœ°ï¼Œå¦‚æœ <span class="math inline">\(A\)</span>
æ˜¯ <span class="math inline">\(T\)</span>
çš„çŸ©é˜µï¼Œåˆ™ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ»¡è¶³</p>
<p><span class="math display">\[
A\mathbf{v} = \lambda \mathbf{v}.
\]</span></p>
<h3 id="basic-examples">Basic Examples</h3>
<p>åŸºæœ¬ç¤ºä¾‹</p>
<p>Example 8.1.1. Let ä¾‹ 8.1.1. è®¾</p>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 0 \\ 0 &amp; 3 \end{bmatrix}.
\]</span></p>
<p>Then ç„¶å</p>
<p><span class="math display">\[
A(1,0)^T = 2(1,0)^T, \quad A(0,1)^T = 3(0,1)^T.
\]</span></p>
<p>So <span class="math inline">\((1,0)\)</span> is an eigenvector with
eigenvalue <span class="math inline">\(2\)</span>, and <span
class="math inline">\((0,1) is an eigenvector with eigenvalue
\\3\)</span>. å› æ­¤ <span class="math inline">\((1,0)\)</span> æ˜¯ç‰¹å¾å€¼ä¸º
$2 çš„ç‰¹å¾å‘é‡ï¼Œ $, and $ (0,1) æ˜¯ç‰¹å¾å€¼ä¸º \ 3$ çš„ç‰¹å¾å‘é‡ ã€‚</p>
<p>Example 8.1.2. Rotation matrix in <span
class="math inline">\(\mathbb{R}^2\)</span>: ä¾‹ 8.1.2ã€‚ <span
class="math inline">\(\mathbb{R}^2\)</span> ä¸­çš„æ—‹è½¬çŸ©é˜µï¼š</p>
<p><span class="math display">\[
R_\theta = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta
&amp; \cos\theta \end{bmatrix}.
\]</span></p>
<p>If <span class="math inline">\(\theta \neq 0, \pi\)</span>, <span
class="math inline">\(R_\theta\)</span> has no real eigenvalues: every
vector is rotated, not scaled. Over <span
class="math inline">\(\mathbb{C}\)</span>, however, it has eigenvalues
<span class="math inline">\(e^{i\theta}, e^{-i\theta}\)</span>. å¦‚æœ
<span class="math inline">\(\theta \neq 0, \pi\)</span> ï¼Œğ‘… ğœƒ R Î¸ â€‹
æ²¡æœ‰å®æ•°ç‰¹å¾å€¼ï¼šæ¯ä¸ªå‘é‡éƒ½ç»è¿‡æ—‹è½¬ï¼Œè€Œä¸æ˜¯ç¼©æ”¾ã€‚ç„¶è€Œï¼Œåœ¨ <span
class="math inline">\(\mathbb{C}\)</span> ä¸Šï¼Œå®ƒçš„ç‰¹å¾å€¼ä¸º <span
class="math inline">\(e^{i\theta}, e^{-i\theta}\)</span> ã€‚</p>
<h3 id="algebraic-formulation">Algebraic Formulation</h3>
<p>ä»£æ•°å…¬å¼</p>
<p>Eigenvalues arise from solving the characteristic equation:
ç‰¹å¾å€¼ç”±æ±‚è§£ç‰¹å¾æ–¹ç¨‹å¾—å‡ºï¼š</p>
<p><span class="math display">\[
\det(A - \lambda I) = 0.
\]</span></p>
<p>This polynomial in <span class="math inline">\(\lambda\)</span> is
the characteristic polynomial. Its roots are the eigenvalues. <span
class="math inline">\(\lambda\)</span>
ä¸­çš„è¿™ä¸ªå¤šé¡¹å¼æ˜¯ç‰¹å¾å¤šé¡¹å¼ã€‚å®ƒçš„æ ¹å°±æ˜¯ç‰¹å¾å€¼ã€‚</p>
<h3 id="geometric-intuition">Geometric Intuition</h3>
<p>å‡ ä½•ç›´è§‰</p>
<ul>
<li>Eigenvectors are directions that remain unchanged in orientation
under a transformation; only their length is scaled.
ç‰¹å¾å‘é‡æ˜¯åœ¨å˜æ¢ä¸‹æ–¹å‘ä¿æŒä¸å˜çš„æ–¹å‘ï¼›åªæœ‰å®ƒä»¬çš„é•¿åº¦è¢«ç¼©æ”¾ã€‚</li>
<li>Eigenvalues tell us the scaling factor along those directions.
ç‰¹å¾å€¼å‘Šè¯‰æˆ‘ä»¬æ²¿è¿™äº›æ–¹å‘çš„ç¼©æ”¾å› å­ã€‚</li>
<li>If a matrix has many independent eigenvectors, it can often be
simplified (diagonalized) by changing basis.
å¦‚æœçŸ©é˜µå…·æœ‰è®¸å¤šç‹¬ç«‹çš„ç‰¹å¾å‘é‡ï¼Œåˆ™é€šå¸¸å¯ä»¥é€šè¿‡æ”¹å˜åŸºæ¥ç®€åŒ–ï¼ˆå¯¹è§’åŒ–ï¼‰ã€‚</li>
</ul>
<h3 id="applications-in-geometry-and-science">Applications in Geometry
and Science</h3>
<p>å‡ ä½•å’Œç§‘å­¦ä¸­çš„åº”ç”¨</p>
<ul>
<li>Stretching along principal axes of an ellipse (quadratic forms).
æ²¿æ¤­åœ†çš„ä¸»è½´æ‹‰ä¼¸ï¼ˆäºŒæ¬¡å‹ï¼‰ã€‚</li>
<li>Stable directions of dynamical systems. åŠ¨åŠ›ç³»ç»Ÿçš„ç¨³å®šæ–¹å‘ã€‚</li>
<li>Principal components in statistics and machine learning.
ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ä¸­çš„ä¸»è¦æˆåˆ†ã€‚</li>
<li>Quantum mechanics, where observables correspond to operators with
eigenvalues. é‡å­åŠ›å­¦ï¼Œå…¶ä¸­å¯è§‚æµ‹é‡å¯¹åº”äºå…·æœ‰ç‰¹å¾å€¼çš„ç®—å­ã€‚</li>
</ul>
<h3 id="why-this-matters-27">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Eigenvalues and eigenvectors are a bridge between algebra and
geometry. They provide a lens for understanding linear transformations
in their simplest form. Nearly every application of linear
algebra-differential equations, statistics, physics, computer
science-relies on eigen-analysis.
ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ˜¯ä»£æ•°å’Œå‡ ä½•ä¹‹é—´çš„æ¡¥æ¢ã€‚å®ƒä»¬ä¸ºç†è§£æœ€ç®€å½¢å¼çš„çº¿æ€§å˜æ¢æä¾›äº†ä¸€ä¸ªè§†è§’ã€‚å‡ ä¹æ‰€æœ‰çº¿æ€§ä»£æ•°çš„åº”ç”¨â€”â€”å¾®åˆ†æ–¹ç¨‹ã€ç»Ÿè®¡å­¦ã€ç‰©ç†å­¦ã€è®¡ç®—æœºç§‘å­¦â€”â€”éƒ½ä¾èµ–äºç‰¹å¾åˆ†æã€‚</p>
<h3 id="exercises-8.1">Exercises 8.1</h3>
<p>ç»ƒä¹  8.1</p>
<ol type="1">
<li>Find the eigenvalues and eigenvectors of <span
class="math inline">\(\begin{bmatrix} 4 &amp; 0 \\ 0 &amp; -1
\end{bmatrix}\)</span>. æ‰¾åˆ°ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ <span
class="math inline">\(\begin{bmatrix} 4 &amp; 0 \\ 0 &amp; -1
\end{bmatrix}\)</span> .</li>
<li>Show that every scalar multiple of an eigenvector is again an
eigenvector for the same eigenvalue.
è¯æ˜ç‰¹å¾å‘é‡çš„æ¯ä¸ªæ ‡é‡å€æ•°åˆæ˜¯åŒä¸€ç‰¹å¾å€¼çš„ç‰¹å¾å‘é‡ã€‚</li>
<li>Verify that the rotation matrix <span
class="math inline">\(R_\theta\)</span> has no real eigenvalues unless
<span class="math inline">\(\theta = 0\)</span> or <span
class="math inline">\(\pi\)</span>. éªŒè¯æ—‹è½¬çŸ©é˜µğ‘… ğœƒ R Î¸ â€‹ é™¤é <span
class="math inline">\(\theta = 0\)</span> æˆ– <span
class="math inline">\(\pi\)</span> ï¼Œå¦åˆ™æ²¡æœ‰å®æ•°ç‰¹å¾å€¼ã€‚</li>
<li>Compute the characteristic polynomial of <span
class="math inline">\(\begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 1
\end{bmatrix}\)</span>. è®¡ç®—ç‰¹å¾å¤šé¡¹å¼ <span
class="math inline">\(\begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 1
\end{bmatrix}\)</span> .</li>
<li>Explain geometrically what eigenvectors and eigenvalues represent
for the shear matrix <span class="math inline">\(\begin{bmatrix} 1 &amp;
1 \\ 0 &amp; 1 \end{bmatrix}\)</span>.
ä»å‡ ä½•è§’åº¦è§£é‡Šç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼å¯¹äºå‰ªåˆ‡çŸ©é˜µçš„æ„ä¹‰ <span
class="math inline">\(\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1
\end{bmatrix}\)</span> .</li>
</ol>
<h2 id="diagonalization">8.2 Diagonalization</h2>
<p>8.2 å¯¹è§’åŒ–</p>
<p>A central goal in linear algebra is to simplify the action of a
matrix by choosing a good basis. Diagonalization is the process of
rewriting a matrix so that it acts by simple scaling along independent
directions. This makes computations such as powers, exponentials, and
solving differential equations far easier.
çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒç›®æ ‡æ˜¯é€šè¿‡é€‰æ‹©åˆé€‚çš„åŸºæ¥ç®€åŒ–çŸ©é˜µçš„è¿ç®—ã€‚å¯¹è§’åŒ–æ˜¯å°†çŸ©é˜µé‡å†™ï¼Œä½¿å…¶èƒ½å¤Ÿæ²¿ç‹¬ç«‹æ–¹å‘è¿›è¡Œç®€å•çš„ç¼©æ”¾ã€‚è¿™ä½¿å¾—å¹‚ã€æŒ‡æ•°å’Œå¾®åˆ†æ–¹ç¨‹ç­‰è®¡ç®—å˜å¾—æ›´åŠ å®¹æ˜“ã€‚</p>
<h3 id="definition-5">Definition</h3>
<p>å®šä¹‰</p>
<p>A square matrix <span class="math inline">\(A \in \mathbb{R}^{n
\times n}\)</span> is diagonalizable if there exists an invertible
matrix <span class="math inline">\(P\)</span> such that å¦‚æœå­˜åœ¨å¯é€†çŸ©é˜µ
<span class="math inline">\(P\)</span> å¹¶ä¸”æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™æ–¹é˜µ <span
class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>
å¯å¯¹è§’åŒ–</p>
<p><span class="math display">\[
P^{-1} A P = D,
\]</span></p>
<p>where <span class="math inline">\(D\)</span> is a diagonal matrix.
å…¶ä¸­ <span class="math inline">\(D\)</span> æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µã€‚</p>
<p>The diagonal entries of <span class="math inline">\(D\)</span> are
eigenvalues of <span class="math inline">\(A\)</span>, and the columns
of <span class="math inline">\(P\)</span> are the corresponding
eigenvectors. <span class="math inline">\(D\)</span> çš„å¯¹è§’çº¿é¡¹æ˜¯ <span
class="math inline">\(A\)</span> çš„ç‰¹å¾å€¼ï¼Œ <span
class="math inline">\(P\)</span> çš„åˆ—æ˜¯ç›¸åº”çš„ç‰¹å¾å‘é‡ã€‚</p>
<h3 id="when-is-a-matrix-diagonalizable">When is a Matrix
Diagonalizable?</h3>
<p>çŸ©é˜µä½•æ—¶å¯å¯¹è§’åŒ–ï¼Ÿ</p>
<ul>
<li>A matrix is diagonalizable if it has <span
class="math inline">\(n\)</span> linearly independent eigenvectors.
å¦‚æœçŸ©é˜µå…·æœ‰ <span class="math inline">\(n\)</span>
ä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ï¼Œåˆ™è¯¥çŸ©é˜µå¯å¯¹è§’åŒ–ã€‚</li>
<li>Equivalently, the sum of the dimensions of its eigenspaces equals
<span class="math inline">\(n\)</span>. ç­‰æ•ˆåœ°ï¼Œå…¶ç‰¹å¾ç©ºé—´çš„ç»´æ•°ä¹‹å’Œç­‰äº
<span class="math inline">\(n\)</span> ã€‚</li>
<li>Symmetric matrices (over <span
class="math inline">\(\mathbb{R}\)</span>) are always diagonalizable,
with an orthonormal basis of eigenvectors. å¯¹ç§°çŸ©é˜µï¼ˆåœ¨ <span
class="math inline">\(\mathbb{R}\)</span>
ä¸Šï¼‰å§‹ç»ˆå¯å¯¹è§’åŒ–ï¼Œä¸”å…·æœ‰ç‰¹å¾å‘é‡çš„æ­£äº¤åŸºã€‚</li>
</ul>
<h3 id="example-8.2.1">Example 8.2.1</h3>
<p>ä¾‹ 8.2.1</p>
<p>Let è®©</p>
<p><span class="math display">\[
A = \begin{bmatrix} 4 &amp; 1 \\ 0 &amp; 2 \end{bmatrix}.
\]</span></p>
<ol type="1">
<li>Characteristic polynomial: ç‰¹å¾å¤šé¡¹å¼ï¼š</li>
</ol>
<p><span class="math display">\[
\det(A - \lambda I) = (4-\lambda)(2-\lambda).
\]</span></p>
<p>So eigenvalues are <span class="math inline">\(\lambda_1 =
4\)</span>, <span class="math inline">\(\lambda_2 = 2\)</span>.
æ‰€ä»¥ç‰¹å¾å€¼æ˜¯ <span class="math inline">\(\lambda_1 = 4\)</span> ï¼Œ <span
class="math inline">\(\lambda_2 = 2\)</span> ã€‚</p>
<ol start="2" type="1">
<li>Eigenvectors: ç‰¹å¾å‘é‡ï¼š</li>
</ol>
<ul>
<li>For <span class="math inline">\(\lambda = 4\)</span>, solve <span
class="math inline">\((A-4I)\mathbf{v}=0\)</span>: <span
class="math inline">\(\begin{bmatrix} 0 &amp; 1 \\ 0 &amp; -2
\end{bmatrix}\mathbf{v} = 0\)</span>, giving <span
class="math inline">\(\mathbf{v}_1 = (1,0)\)</span>. å¯¹äº <span
class="math inline">\(\lambda = 4\)</span> ï¼Œæ±‚è§£ <span
class="math inline">\((A-4I)\mathbf{v}=0\)</span> ï¼š <span
class="math inline">\(\begin{bmatrix} 0 &amp; 1 \\ 0 &amp; -2
\end{bmatrix}\mathbf{v} = 0\)</span> ï¼Œå¾—åˆ° <span
class="math inline">\(\mathbf{v}_1 = (1,0)\)</span> ã€‚</li>
<li>For <span class="math inline">\(\lambda = 2\)</span>: <span
class="math inline">\((A-2I)\mathbf{v}=0\)</span>, giving <span
class="math inline">\(\mathbf{v}_2 = (1,-2)\)</span>. å¯¹äº <span
class="math inline">\(\lambda = 2\)</span> ï¼š <span
class="math inline">\((A-2I)\mathbf{v}=0\)</span> ï¼Œç»™å‡º <span
class="math inline">\(\mathbf{v}_2 = (1,-2)\)</span> ã€‚</li>
</ul>
<ol start="3" type="1">
<li>Construct <span class="math inline">\(P = \begin{bmatrix} 1 &amp; 1
\\ 0 &amp; -2 \end{bmatrix}\)</span>. Then æ„é€  <span
class="math inline">\(P = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; -2
\end{bmatrix}\)</span> ã€‚ç„¶å</li>
</ol>
<p><span class="math display">\[
P^{-1} A P = \begin{bmatrix} 4 &amp; 0 \\ 0 &amp; 2 \end{bmatrix}.
\]</span></p>
<p>Thus, <span class="math inline">\(A\)</span> is diagonalizable.
å› æ­¤ï¼Œ <span class="math inline">\(A\)</span> æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚</p>
<h3 id="why-diagonalize">Why Diagonalize?</h3>
<p>ä¸ºä»€ä¹ˆè¦å¯¹è§’åŒ–ï¼Ÿ</p>
<ul>
<li><p>Computing powers: If <span class="math inline">\(A = P D
P^{-1}\)</span>, then è®¡ç®—èƒ½åŠ›ï¼š å¦‚æœ <span class="math inline">\(A = P
D P^{-1}\)</span> ï¼Œåˆ™</p>
<p><span class="math display">\[
A^k = P D^k P^{-1}.
\]</span></p>
<p>Since <span class="math inline">\(D\)</span> is diagonal, <span
class="math inline">\(D^k\)</span> is easy to compute. ç”±äº <span
class="math inline">\(D\)</span> æ˜¯å¯¹è§’çº¿ï¼Œå› æ­¤ <span
class="math inline">\(D^k\)</span> å¾ˆå®¹æ˜“è®¡ç®—ã€‚</p></li>
<li><p>Matrix exponentials: <span class="math inline">\(e^A = P e^D
P^{-1}\)</span>, useful in solving differential equations. çŸ©é˜µæŒ‡æ•°ï¼š
<span class="math inline">\(e^A = P e^D P^{-1}\)</span>
ï¼Œæœ‰åŠ©äºè§£å†³å¾®åˆ†æ–¹ç¨‹ã€‚</p></li>
<li><p>Understanding geometry: Diagonalization reveals the directions
along which a transformation stretches or compresses space
independently.
ç†è§£å‡ ä½•ï¼šå¯¹è§’åŒ–æ­ç¤ºäº†å˜æ¢ç‹¬ç«‹æ‹‰ä¼¸æˆ–å‹ç¼©ç©ºé—´çš„æ–¹å‘ã€‚</p></li>
</ul>
<h3 id="non-diagonalizable-example">Non-Diagonalizable Example</h3>
<p>ä¸å¯å¯¹è§’åŒ–çš„ä¾‹å­</p>
<p>Not all matrices can be diagonalized. å¹¶éæ‰€æœ‰çŸ©é˜µéƒ½å¯ä»¥å¯¹è§’åŒ–ã€‚</p>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>has only one eigenvalue <span class="math inline">\(\lambda =
1\)</span>, with eigenspace dimension 1. Since <span
class="math inline">\(n=2\)</span> but we only have 1 independent
eigenvector, <span class="math inline">\(A\)</span> is not
diagonalizable. åªæœ‰ä¸€ä¸ªç‰¹å¾å€¼ <span class="math inline">\(\lambda =
1\)</span> ï¼Œç‰¹å¾ç©ºé—´ç»´æ•°ä¸º 1ã€‚ç”±äº <span
class="math inline">\(n=2\)</span> ä½†æˆ‘ä»¬åªæœ‰ 1 ä¸ªç‹¬ç«‹ç‰¹å¾å‘é‡ï¼Œå› æ­¤
<span class="math inline">\(A\)</span> ä¸å¯å¯¹è§’åŒ–ã€‚</p>
<h3 id="geometric-interpretation-16">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>Diagonalization means we have found a basis of eigenvectors. In this
basis, the matrix acts by simple scaling along each coordinate axis. It
transforms complicated motion into independent 1D motions.
å¯¹è§’åŒ–æ„å‘³ç€æˆ‘ä»¬æ‰¾åˆ°äº†ç‰¹å¾å‘é‡çš„åŸºã€‚åœ¨æ­¤åŸºä¸Šï¼ŒçŸ©é˜µé€šè¿‡æ²¿æ¯ä¸ªåæ ‡è½´è¿›è¡Œç®€å•çš„ç¼©æ”¾æ¥å‘æŒ¥ä½œç”¨ã€‚å®ƒå°†å¤æ‚çš„è¿åŠ¨è½¬åŒ–ä¸ºç‹¬ç«‹çš„ä¸€ç»´è¿åŠ¨ã€‚</p>
<h3 id="why-this-matters-28">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Diagonalization is a cornerstone of linear algebra. It simplifies
computation, reveals structure, and is the starting point for the
spectral theorem, Jordan form, and many applications in physics,
engineering, and data science.
å¯¹è§’åŒ–æ˜¯çº¿æ€§ä»£æ•°çš„åŸºçŸ³ã€‚å®ƒç®€åŒ–äº†è®¡ç®—ï¼Œæ­ç¤ºäº†ç»“æ„ï¼Œå¹¶ä¸”æ˜¯è°±å®šç†ã€è‹¥å°”å½“å½¢å¼ä»¥åŠç‰©ç†ã€å·¥ç¨‹å’Œæ•°æ®ç§‘å­¦ä¸­è®¸å¤šåº”ç”¨çš„èµ·ç‚¹ã€‚</p>
<h3 id="exercises-8.2">Exercises 8.2</h3>
<p>ç»ƒä¹  8.2</p>
<ol type="1">
<li><p>Diagonalize å¯¹è§’åŒ–</p>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 0 \\ 0 &amp; 3 \end{bmatrix}.
\]</span></p></li>
<li><p>Determine whether ç¡®å®šæ˜¯å¦</p>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>is diagonalizable. Why or why not?
æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚ä¸ºä»€ä¹ˆæˆ–ä¸ºä»€ä¹ˆä¸ï¼Ÿ</p></li>
<li><p>Find <span class="math inline">\(A^5\)</span> for æŸ¥æ‰¾ <span
class="math inline">\(A^5\)</span></p>
<p><span class="math display">\[
A = \begin{bmatrix} 4 &amp; 1 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<p>using diagonalization. ä½¿ç”¨å¯¹è§’åŒ–ã€‚</p></li>
<li><p>Show that any <span class="math inline">\(n \times n\)</span>
matrix with <span class="math inline">\(n\)</span> distinct eigenvalues
is diagonalizable. è¯æ˜ä»»ä½•å…·æœ‰ <span class="math inline">\(n\)</span>
ä¸ªä¸åŒç‰¹å¾å€¼çš„ <span class="math inline">\(n \times n\)</span>
çŸ©é˜µéƒ½æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚</p></li>
<li><p>Explain why real symmetric matrices are always diagonalizable.
è§£é‡Šä¸ºä»€ä¹ˆå®å¯¹ç§°çŸ©é˜µæ€»æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚</p></li>
</ol>
<h2 id="characteristic-polynomials">8.3 Characteristic Polynomials</h2>
<p>8.3 ç‰¹å¾å¤šé¡¹å¼</p>
<p>The key to finding eigenvalues is the characteristic polynomial of a
matrix. This polynomial encodes the values of <span
class="math inline">\(\lambda\)</span> for which the matrix <span
class="math inline">\(A - \lambda I\)</span> fails to be invertible.
å¯»æ‰¾ç‰¹å¾å€¼çš„å…³é”®æ˜¯çŸ©é˜µçš„ç‰¹å¾å¤šé¡¹å¼ã€‚è¯¥å¤šé¡¹å¼å¯¹å€¼è¿›è¡Œç¼–ç  çŸ©é˜µ <span
class="math inline">\(A - \lambda I\)</span> ä¸å¯é€†ï¼Œå…¶ä¸­ <span
class="math inline">\(\lambda\)</span> ã€‚</p>
<h3 id="definition-6">Definition</h3>
<p>å®šä¹‰</p>
<p>For an <span class="math inline">\(n \times n\)</span> matrix <span
class="math inline">\(A\)</span>, the characteristic polynomial is å¯¹äº
<span class="math inline">\(n \times n\)</span> çŸ©é˜µ <span
class="math inline">\(A\)</span> ï¼Œç‰¹å¾å¤šé¡¹å¼ä¸º</p>
<p><span class="math display">\[
p_A(\lambda) = \det(A - \lambda I).
\]</span></p>
<p>The roots of <span class="math inline">\(p_A(\lambda)\)</span> are
the eigenvalues of <span class="math inline">\(A\)</span>. <span
class="math inline">\(p_A(\lambda)\)</span> çš„æ ¹æ˜¯ <span
class="math inline">\(A\)</span> çš„ç‰¹å¾å€¼ã€‚</p>
<h3 id="examples-7">Examples</h3>
<p>ç¤ºä¾‹</p>
<p>Example 8.3.1. Let ä¾‹ 8.3.1. è®¾</p>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}.
\]</span></p>
<p>Then ç„¶å</p>
<p><span class="math display">\[
p_A(\lambda) = \det\!\begin{bmatrix} 2-\lambda &amp; 1 \\ 1 &amp;
2-\lambda \end{bmatrix}= (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
\]</span></p>
<p>Thus eigenvalues are <span class="math inline">\(\lambda = 1,
3\)</span>. å› æ­¤ç‰¹å¾å€¼ä¸º <span class="math inline">\(\lambda = 1,
3\)</span> ã€‚</p>
<p>Example 8.3.2. For ä¾‹ 8.3.2. å¯¹äº</p>
<p><span class="math display">\[
A = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}
\]</span></p>
<p>(rotation by 90Â°), ï¼ˆæ—‹è½¬ 90Â°ï¼‰ï¼Œ</p>
<p><span class="math display">\[
p_A(\lambda) = \det\!\begin{bmatrix} -\lambda &amp; -1 \\ 1 &amp;
-\lambda \end{bmatrix}= \lambda^2 + 1.
\]</span></p>
<p>Eigenvalues are <span class="math inline">\(\lambda = \pm i\)</span>.
No real eigenvalues exist, consistent with pure rotation. ç‰¹å¾å€¼ä¸º <span
class="math inline">\(\lambda = \pm i\)</span>
ã€‚ä¸å­˜åœ¨å®æ•°ç‰¹å¾å€¼ï¼Œä¸çº¯æ—‹è½¬ä¸€è‡´ã€‚</p>
<p>Example 8.3.3. For a triangular matrix ä¾‹ 8.3.3. å¯¹äºä¸‰è§’çŸ©é˜µ</p>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 0 &amp; 3 &amp; 5 \\ 0 &amp; 0
&amp; 4 \end{bmatrix},
\]</span></p>
<p>the determinant is simply the product of diagonal entries minus <span
class="math inline">\(\lambda\)</span>: è¡Œåˆ—å¼ä»…ä»…æ˜¯å¯¹è§’çº¿é¡¹çš„ä¹˜ç§¯å‡å»
<span class="math inline">\(\lambda\)</span> ï¼š</p>
<p><span class="math display">\[
p_A(\lambda) = (2-\lambda)(3-\lambda)(4-\lambda).
\]</span></p>
<p>So eigenvalues are 2,3,4. æ‰€ä»¥ç‰¹å¾å€¼ä¸º 2,3,4 ã€‚</p>
<h3 id="properties-1">Properties</h3>
<p>ç‰¹æ€§</p>
<ol type="1">
<li><p>The characteristic polynomial of an <span class="math inline">\(n
\times n\)</span> matrix has degree <span
class="math inline">\(n\)</span>. <span class="math inline">\(n \times
n\)</span> çŸ©é˜µçš„ç‰¹å¾å¤šé¡¹å¼çš„åº¦ä¸º <span class="math inline">\(n\)</span>
ã€‚</p></li>
<li><p>The sum of the eigenvalues (counted with multiplicity) equals the
trace of <span class="math inline">\(A\)</span>:
ç‰¹å¾å€¼ï¼ˆæŒ‰é‡æ•°è®¡ç®—ï¼‰çš„å’Œç­‰äº <span class="math inline">\(A\)</span>
çš„è¿¹ï¼š</p>
<p><span class="math display">\[
\text{tr}(A) = \lambda_1 + \cdots + \lambda_n.
\]</span></p></li>
<li><p>The product of the eigenvalues equals the determinant of <span
class="math inline">\(A\)</span>: ç‰¹å¾å€¼çš„ä¹˜ç§¯ç­‰äº <span
class="math inline">\(A\)</span> çš„è¡Œåˆ—å¼ï¼š</p>
<p><span class="math display">\[
\det(A) = \lambda_1 \cdots \lambda_n.
\]</span></p></li>
<li><p>Similar matrices have the same characteristic polynomial, hence
the same eigenvalues.
ç›¸ä¼¼çš„çŸ©é˜µå…·æœ‰ç›¸åŒçš„ç‰¹å¾å¤šé¡¹å¼ï¼Œå› æ­¤å…·æœ‰ç›¸åŒçš„ç‰¹å¾å€¼ã€‚</p></li>
</ol>
<h3 id="geometric-interpretation-17">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>The characteristic polynomial captures when <span
class="math inline">\(A - \lambda I\)</span> collapses space: its
determinant is zero precisely when the transformation <span
class="math inline">\(A - \lambda I\)</span> is singular. Thus,
eigenvalues mark the critical scalings where the matrix loses
invertibility. ç‰¹å¾å¤šé¡¹å¼æ•æ‰äº† <span class="math inline">\(A - \lambda
I\)</span> ä½•æ—¶ä½¿ç©ºé—´åç¼©ï¼šå½“å˜æ¢ <span class="math inline">\(A -
\lambda I\)</span>
ä¸ºå¥‡å¼‚æ—¶ï¼Œå…¶è¡Œåˆ—å¼æ°å¥½ä¸ºé›¶ã€‚å› æ­¤ï¼Œç‰¹å¾å€¼æ ‡è®°äº†çŸ©é˜µå¤±å»å¯é€†æ€§çš„ä¸´ç•Œå°ºåº¦ã€‚</p>
<h3 id="why-this-matters-29">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Characteristic polynomials provide the computational tool to extract
eigenvalues. They connect matrix invariants (trace and determinant) with
geometry, and form the foundation for diagonalization, spectral
theorems, and stability analysis in dynamical systems.
ç‰¹å¾å¤šé¡¹å¼æä¾›äº†æå–ç‰¹å¾å€¼çš„è®¡ç®—å·¥å…·ã€‚å®ƒä»¬å°†çŸ©é˜µä¸å˜é‡ï¼ˆè¿¹å’Œè¡Œåˆ—å¼ï¼‰ä¸å‡ ä½•è”ç³»èµ·æ¥ï¼Œå¹¶æ„æˆäº†åŠ¨åŠ›ç³»ç»Ÿä¸­å¯¹è§’åŒ–ã€è°±å®šç†å’Œç¨³å®šæ€§åˆ†æçš„åŸºç¡€ã€‚</p>
<h3 id="exercises-8.3">Exercises 8.3</h3>
<p>ç»ƒä¹  8.3</p>
<ol type="1">
<li><p>Compute the characteristic polynomial of è®¡ç®—ç‰¹å¾å¤šé¡¹å¼</p>
<p><span class="math display">\[
A = \begin{bmatrix} 4 &amp; 2 \\ 1 &amp; 3 \end{bmatrix}.
\]</span></p></li>
<li><p>Verify that the sum of the eigenvalues of <span
class="math inline">\(\begin{bmatrix} 5 &amp; 0 \\ 0 &amp; -2
\end{bmatrix}\)</span> equals its trace, and their product equals its
determinant. éªŒè¯ç‰¹å¾å€¼ä¹‹å’Œ <span class="math inline">\(\begin{bmatrix}
5 &amp; 0 \\ 0 &amp; -2 \end{bmatrix}\)</span>
ç­‰äºå®ƒçš„è¿¹ï¼Œå®ƒä»¬çš„ä¹˜ç§¯ç­‰äºå®ƒçš„è¡Œåˆ—å¼ã€‚</p></li>
<li><p>Show that for any triangular matrix, the eigenvalues are just the
diagonal entries. è¯æ˜å¯¹äºä»»ä½•ä¸‰è§’çŸ©é˜µï¼Œç‰¹å¾å€¼åªæ˜¯å¯¹è§’çº¿é¡¹ã€‚</p></li>
<li><p>Prove that if <span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> are similar matrices, then <span
class="math inline">\(p_A(\lambda) = p_B(\lambda)\)</span>. è¯æ˜å¦‚æœ
<span class="math inline">\(A\)</span> å’Œ <span
class="math inline">\(B\)</span> æ˜¯ç›¸ä¼¼çŸ©é˜µï¼Œåˆ™ <span
class="math inline">\(p_A(\lambda) = p_B(\lambda)\)</span> ã€‚</p></li>
<li><p>Compute the characteristic polynomial of <span
class="math inline">\(\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 0 &amp; 1
&amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)</span>. è®¡ç®—ç‰¹å¾å¤šé¡¹å¼ [ 1
1 0 0 1 1 0 0 1 ] â€‹ 1 0 0 â€‹ 1 1 0 â€‹ 0 1 1 â€‹ â€‹ .</p></li>
</ol>
<h2 id="applications-differential-equations-markov-chains">8.4
Applications (Differential Equations, Markov Chains)</h2>
<p>8.4 åº”ç”¨ï¼ˆå¾®åˆ†æ–¹ç¨‹ã€é©¬å°”å¯å¤«é“¾ï¼‰</p>
<p>Eigenvalues and eigenvectors are not only central to the theory of
linear algebra-they are indispensable tools across mathematics and
applied science. Two classic applications are solving systems of
differential equations and analyzing Markov chains.
ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ä¸ä»…æ˜¯çº¿æ€§ä»£æ•°ç†è®ºçš„æ ¸å¿ƒï¼Œä¹Ÿæ˜¯æ•°å­¦å’Œåº”ç”¨ç§‘å­¦é¢†åŸŸä¸­ä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚ä¸¤ä¸ªç»å…¸çš„åº”ç”¨æ˜¯æ±‚è§£å¾®åˆ†æ–¹ç¨‹ç»„å’Œåˆ†æé©¬å°”å¯å¤«é“¾ã€‚</p>
<h3 id="linear-differential-equations">Linear Differential
Equations</h3>
<p>çº¿æ€§å¾®åˆ†æ–¹ç¨‹</p>
<p>Consider the system è€ƒè™‘ç³»ç»Ÿ</p>
<p><span class="math display">\[
\frac{d\mathbf{x}}{dt} = A \mathbf{x},
\]</span></p>
<p>where <span class="math inline">\(A\)</span> is an <span
class="math inline">\(n \times n\)</span> matrix and <span
class="math inline">\(\mathbf{x}(t)\)</span> is a vector-valued
function. å…¶ä¸­ <span class="math inline">\(A\)</span> æ˜¯ <span
class="math inline">\(n \times n\)</span> çŸ©é˜µï¼Œ <span
class="math inline">\(\mathbf{x}(t)\)</span> æ˜¯çŸ¢é‡å€¼å‡½æ•°ã€‚</p>
<p>If <span class="math inline">\(\mathbf{v}\)</span> is an eigenvector
of <span class="math inline">\(A\)</span> with eigenvalue <span
class="math inline">\(\lambda\)</span>, then the function å¦‚æœ <span
class="math inline">\(\mathbf{v}\)</span> æ˜¯ <span
class="math inline">\(A\)</span> çš„ç‰¹å¾å‘é‡ï¼Œå…¶ç‰¹å¾å€¼ä¸º <span
class="math inline">\(\lambda\)</span> ï¼Œåˆ™å‡½æ•°</p>
<p><span class="math display">\[
\mathbf{x}(t) = e^{\lambda t}\mathbf{v}
\]</span></p>
<p>is a solution. æ˜¯ä¸€ä¸ªè§£å†³æ–¹æ¡ˆã€‚</p>
<ul>
<li><p>Eigenvalues determine the growth or decay rate:
ç‰¹å¾å€¼å†³å®šå¢é•¿ç‡æˆ–è¡°å‡ç‡ï¼š</p>
<ul>
<li>If <span class="math inline">\(\lambda &lt; 0\)</span>, solutions
decay (stable). å¦‚æœ <span class="math inline">\(\lambda &lt; 0\)</span>
ï¼Œåˆ™è§£å†³æ–¹æ¡ˆè¡°å‡ï¼ˆç¨³å®šï¼‰ã€‚</li>
<li>If <span class="math inline">\(\lambda &gt; 0\)</span>, solutions
grow (unstable). å¦‚æœ <span class="math inline">\(\lambda &gt;
0\)</span> ï¼Œåˆ™è§£å†³æ–¹æ¡ˆä¼šå¢é•¿ï¼ˆä¸ç¨³å®šï¼‰ã€‚</li>
<li>If <span class="math inline">\(\lambda\)</span> is complex,
oscillations occur. å¦‚æœ <span class="math inline">\(\lambda\)</span>
æ˜¯å¤æ•°ï¼Œåˆ™ä¼šå‘ç”ŸæŒ¯è¡ã€‚</li>
</ul></li>
</ul>
<p>By combining eigenvector solutions, we can solve general initial
conditions. é€šè¿‡ç»“åˆç‰¹å¾å‘é‡è§£ï¼Œæˆ‘ä»¬å¯ä»¥è§£å†³ä¸€èˆ¬çš„åˆå§‹æ¡ä»¶ã€‚</p>
<p>Example 8.4.1. Let ä¾‹ 8.4.1. è®¾</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 0 \\0 &amp; -1 \end{bmatrix}.
\]</span></p>
<p>Then eigenvalues are <span class="math inline">\(2, -1\)</span>with
eigenvectors<span class="math inline">\((1,0)\)</span>, <span
class="math inline">\((0,1)\)</span>. Solutions are åˆ™ç‰¹å¾å€¼ä¸º $2, -1
<span class="math inline">\(with eigenvectors\)</span> (1,0) $, $
(0,1)$ã€‚è§£ä¸º</p>
<p><span class="math display">\[
\mathbf{x}(t) = c_1 e^{2t}(1,0) + c_2 e^{-t}(0,1).
\]</span></p>
<p>Thus one component grows exponentially, the other decays.
å› æ­¤ï¼Œä¸€ä¸ªéƒ¨åˆ†å‘ˆæŒ‡æ•°å¢é•¿ï¼Œå¦ä¸€ä¸ªéƒ¨åˆ†åˆ™è¡°å‡ã€‚</p>
<h3 id="markov-chains">Markov Chains</h3>
<p>é©¬å°”å¯å¤«é“¾</p>
<p>A Markov chain is described by a stochastic matrix <span
class="math inline">\(P\)</span>, where each column sums to 1 and
entries are nonnegative. If <span
class="math inline">\(\mathbf{x}_k\)</span> represents the probability
distribution after <span class="math inline">\(k\)</span> steps, then
é©¬å°”å¯å¤«é“¾å¯ä»¥ç”¨éšæœºçŸ©é˜µ <span class="math inline">\(P\)</span>
æ¥æè¿°ï¼Œå…¶ä¸­æ¯åˆ—å’Œä¸º 1ï¼Œä¸”å…ƒç´ ä¸ºéè´Ÿå€¼ã€‚å¦‚æœ ğ‘¥ ğ‘˜ x k â€‹ è¡¨ç¤º <span
class="math inline">\(k\)</span> æ­¥åçš„æ¦‚ç‡åˆ†å¸ƒï¼Œåˆ™</p>
<p><span class="math display">\[
\mathbf{x}_{k+1} = P \mathbf{x}_k.
\]</span></p>
<p>Iterating gives è¿­ä»£å¾—åˆ°</p>
<p><span class="math display">\[
\mathbf{x}_k = P^k \mathbf{x}_0.
\]</span></p>
<p>Understanding long-term behavior reduces to analyzing powers of <span
class="math inline">\(P\)</span>. ç†è§£é•¿æœŸè¡Œä¸ºå¯ä»¥å½’ç»“ä¸ºåˆ†æ <span
class="math inline">\(P\)</span> çš„åŠ›é‡ã€‚</p>
<ul>
<li>The eigenvalue <span class="math inline">\(\lambda = 1\)</span>
always exists. Its eigenvector gives the steady-state distribution.
ç‰¹å¾å€¼ <span class="math inline">\(\lambda = 1\)</span>
å§‹ç»ˆå­˜åœ¨ã€‚å…¶ç‰¹å¾å‘é‡ç»™å‡ºäº†ç¨³æ€åˆ†å¸ƒã€‚</li>
<li>All other eigenvalues satisfy <span class="math inline">\(|\lambda|
\leq 1\)</span>. Their influence decays as <span class="math inline">\(k
\to \infty\)</span>. æ‰€æœ‰å…¶ä»–ç‰¹å¾å€¼éƒ½æ»¡è¶³ <span
class="math inline">\(|\lambda| \leq 1\)</span> ã€‚å®ƒä»¬çš„å½±å“è¡°å‡ä¸º <span
class="math inline">\(k \to \infty\)</span> ã€‚</li>
</ul>
<p>Example 8.4.2. Consider ä¾‹ 8.4.2. è€ƒè™‘</p>
<p><span class="math display">\[
P = \begin{bmatrix}0.9 &amp; 0.5 \\0.1 &amp; 0.5 \end{bmatrix}.
\]</span></p>
<p>Eigenvalues are <span class="math inline">\(\lambda_1 = 1\)</span>,
<span class="math inline">\(\lambda_2 = 0.4\)</span>. The eigenvector
for <span class="math inline">\(\lambda = 1\)</span> is proportional to
<span class="math inline">\((5,1)\)</span>. Normalizing gives the steady
state ç‰¹å¾å€¼ä¸º <span class="math inline">\(\lambda_1 = 1\)</span> ,
<span class="math inline">\(\lambda_2 = 0.4\)</span> ã€‚ <span
class="math inline">\(\lambda = 1\)</span> çš„ç‰¹å¾å‘é‡ä¸ <span
class="math inline">\((5,1)\)</span> æˆæ­£æ¯”ã€‚å½’ä¸€åŒ–åå¯å¾—åˆ°ç¨³æ€</p>
<p><span class="math display">\[
\pi = \left(\tfrac{5}{6}, \tfrac{1}{6}\right).
\]</span></p>
<p>Thus, regardless of the starting distribution, the chain converges to
<span class="math inline">\(\pi\)</span>.
å› æ­¤ï¼Œæ— è®ºèµ·å§‹åˆ†å¸ƒå¦‚ä½•ï¼Œé“¾éƒ½ä¼šæ”¶æ•›åˆ° <span
class="math inline">\(\pi\)</span> ã€‚</p>
<h3 id="geometric-interpretation-18">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<ul>
<li>In differential equations, eigenvalues determine the time evolution:
exponential growth, decay, or oscillation.
åœ¨å¾®åˆ†æ–¹ç¨‹ä¸­ï¼Œç‰¹å¾å€¼å†³å®šæ—¶é—´çš„æ¼”å˜ï¼šæŒ‡æ•°å¢é•¿ã€è¡°å‡æˆ–æŒ¯è¡ã€‚</li>
<li>In Markov chains, eigenvalues determine the long-term equilibrium of
stochastic processes.
åœ¨é©¬å°”å¯å¤«é“¾ä¸­ï¼Œç‰¹å¾å€¼å†³å®šäº†éšæœºè¿‡ç¨‹çš„é•¿æœŸå‡è¡¡ã€‚</li>
</ul>
<h3 id="why-this-matters-30">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Eigenvalue methods turn complex iterative or dynamical systems into
tractable problems. In physics, engineering, and finance, they describe
stability and resonance. In computer science and statistics, they power
algorithms from Googleâ€™s PageRank to modern machine learning.
ç‰¹å¾å€¼æ–¹æ³•å°†å¤æ‚çš„è¿­ä»£æˆ–åŠ¨æ€ç³»ç»Ÿè½¬åŒ–ä¸ºæ˜“äºå¤„ç†çš„é—®é¢˜ã€‚åœ¨ç‰©ç†å­¦ã€å·¥ç¨‹å­¦å’Œé‡‘èå­¦é¢†åŸŸï¼Œå®ƒä»¬æè¿°ç¨³å®šæ€§å’Œå…±æŒ¯ã€‚åœ¨è®¡ç®—æœºç§‘å­¦å’Œç»Ÿè®¡å­¦é¢†åŸŸï¼Œå®ƒä»¬ä¸ºä»è°·æ­Œçš„
PageRank åˆ°ç°ä»£æœºå™¨å­¦ä¹ ç­‰å„ç§ç®—æ³•æä¾›æ”¯æŒã€‚</p>
<h3 id="exercises-8.4">Exercises 8.4</h3>
<p>ç»ƒä¹  8.4</p>
<ol type="1">
<li><p>Solve <span class="math inline">\(\tfrac{d}{dt}\mathbf{x} =
\begin{bmatrix} 3 &amp; 0 \\ 0 &amp; -2
\end{bmatrix}\mathbf{x}\)</span>. è§£å‡º <span
class="math inline">\(\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 &amp;
0 \\ 0 &amp; -2 \end{bmatrix}\mathbf{x}\)</span> ã€‚</p></li>
<li><p>Show that if <span class="math inline">\(A\)</span> has a complex
eigenvalue <span class="math inline">\(\alpha \pm i\beta\)</span>, then
solutions of <span class="math inline">\(\tfrac{d}{dt}\mathbf{x} =
A\mathbf{x}\)</span> involve oscillations of frequency <span
class="math inline">\(\beta\)</span>. è¯æ˜å¦‚æœ <span
class="math inline">\(A\)</span> å…·æœ‰å¤ç‰¹å¾å€¼ <span
class="math inline">\(\alpha \pm i\beta\)</span> ï¼Œåˆ™ <span
class="math inline">\(\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}\)</span>
çš„è§£æ¶‰åŠé¢‘ç‡ <span class="math inline">\(\beta\)</span>
çš„æŒ¯è¡ã€‚</p></li>
<li><p>Find the steady-state distribution of æ‰¾åˆ°ç¨³æ€åˆ†å¸ƒ</p>
<p><span class="math display">\[
P = \begin{bmatrix} 0.7 &amp; 0.2 \\ 0.3 &amp; 0.8 \end{bmatrix}.
\]</span></p></li>
<li><p>Prove that for any stochastic matrix <span
class="math inline">\(P\)</span>, 1 is always an eigenvalue.
è¯æ˜å¯¹äºä»»ä½•éšæœºçŸ©é˜µ <span class="math inline">\(P\)</span> ï¼Œ 1
å§‹ç»ˆæ˜¯ç‰¹å¾å€¼ã€‚</p></li>
<li><p>Explain why all eigenvalues of a stochastic matrix satisfy <span
class="math inline">\(|\lambda| \leq 1\)</span>.
è§£é‡Šä¸ºä»€ä¹ˆéšæœºçŸ©é˜µçš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ»¡è¶³ <span
class="math inline">\(|\lambda| \leq 1\)</span> ã€‚</p></li>
</ol>
<h1 id="chapter-9.-quadratic-forms-and-spectral-theorems">Chapter 9.
Quadratic Forms and Spectral Theorems</h1>
<p>ç¬¬ä¹ç« äºŒæ¬¡å‹å’Œè°±å®šç†</p>
<h2 id="quadratic-forms">9.1 Quadratic Forms</h2>
<p>9.1 äºŒæ¬¡å‹</p>
<p>A quadratic form is a polynomial of degree two in several variables,
expressed neatly using matrices. Quadratic forms appear throughout
mathematics: in optimization, geometry of conic sections, statistics
(variance), and physics (energy functions).
äºŒæ¬¡å‹æ˜¯å¤šå…ƒäºŒæ¬¡å¤šé¡¹å¼ï¼Œå¯ä»¥ç”¨çŸ©é˜µç®€æ´åœ°è¡¨ç¤ºã€‚äºŒæ¬¡å‹åœ¨æ•°å­¦ä¸­éšå¤„å¯è§ï¼šä¼˜åŒ–ã€åœ†é”¥æ›²çº¿å‡ ä½•ã€ç»Ÿè®¡å­¦ï¼ˆæ–¹å·®ï¼‰å’Œç‰©ç†å­¦ï¼ˆèƒ½é‡å‡½æ•°ï¼‰ã€‚</p>
<h3 id="definition-7">Definition</h3>
<p>å®šä¹‰</p>
<p>Let <span class="math inline">\(A\)</span> be an <span
class="math inline">\(n \times n\)</span> symmetric matrix and <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>. The
quadratic form associated with <span class="math inline">\(A\)</span> is
ä»¤ <span class="math inline">\(A\)</span> ä¸º <span
class="math inline">\(n \times n\)</span> å¯¹ç§°çŸ©é˜µï¼Œ <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> ã€‚ä¸ <span
class="math inline">\(A\)</span> ç›¸å…³çš„äºŒæ¬¡å¼ä¸º</p>
<p><span class="math display">\[
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}.
\]</span></p>
<p>Expanded, æ‰©å±•ï¼Œ</p>
<p><span class="math display">\[
Q(\mathbf{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j.
\]</span></p>
<p>Because <span class="math inline">\(A\)</span> is symmetric (<span
class="math inline">\(a_{ij} = a_{ji}\)</span>), the cross-terms can be
grouped naturally. å› ä¸º <span class="math inline">\(A\)</span> æ˜¯å¯¹ç§°çš„
(ğ‘ ğ‘– ğ‘— = ğ‘ ğ‘— ğ‘– a ä¼Šå¥‡ â€‹ =a å§¬ â€‹ )ï¼Œäº¤å‰é¡¹å¯ä»¥è‡ªç„¶åˆ†ç»„ã€‚</p>
<h3 id="examples-8">Examples</h3>
<p>ç¤ºä¾‹</p>
<p>Example 9.1.1. For ä¾‹ 9.1.1. å¯¹äº</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 1 \\1 &amp; 3 \end{bmatrix}, \quad \mathbf{x}
= \begin{bmatrix}x \\y \end{bmatrix},
\]</span></p>
<p><span class="math display">\[
Q(x,y) = \begin{bmatrix} x &amp; y \end{bmatrix}\begin{bmatrix}2 &amp; 1
\\1 &amp; 3 \end{bmatrix}\begin{bmatrix}x \\y \end{bmatrix}= 2x^2 + 2xy
+ 3y^2.
\]</span></p>
<p>Example 9.1.2. The quadratic form ä¾‹ 9.1.2. äºŒæ¬¡å‹</p>
<p><span class="math display">\[
Q(x,y) = x^2 + y^2
\]</span></p>
<p>corresponds to the matrix <span class="math inline">\(A =
I_2\)</span>. It measures squared Euclidean distance from the origin.
å¯¹åº”äºçŸ©é˜µğ´ = ğ¼ 2 A=I 2 â€‹ . å®ƒæµ‹é‡è·ç¦»åŸç‚¹çš„å¹³æ–¹æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚</p>
<p>Example 9.1.3. The conic section equation ä¾‹ 9.1.3 åœ†é”¥æ›²çº¿æ–¹ç¨‹</p>
<p><span class="math display">\[
4x^2 + 2xy + 5y^2 = 1
\]</span></p>
<p>is described by the quadratic form <span
class="math inline">\(\mathbf{x}^T A \mathbf{x} = 1\)</span> with
ç”±äºŒæ¬¡å‹ <span class="math inline">\(\mathbf{x}^T A \mathbf{x} =
1\)</span> æè¿°</p>
<p><span class="math display">\[
A = \begin{bmatrix}4 &amp; 1 \\1 &amp; 5\end{bmatrix}.
\]</span></p>
<h3 id="diagonalization-of-quadratic-forms">Diagonalization of Quadratic
Forms</h3>
<p>äºŒæ¬¡å‹çš„å¯¹è§’åŒ–</p>
<p>By choosing a new basis consisting of eigenvectors of <span
class="math inline">\(A\)</span>, we can rewrite the quadratic form
without cross terms. If <span class="math inline">\(A =
PDP^{-1}\)</span> with <span class="math inline">\(D\)</span> diagonal,
then é€šè¿‡é€‰æ‹©ç”± <span class="math inline">\(A\)</span>
çš„ç‰¹å¾å‘é‡ç»„æˆçš„æ–°åŸºï¼Œæˆ‘ä»¬å¯ä»¥é‡å†™æ²¡æœ‰äº¤å‰é¡¹çš„äºŒæ¬¡å‹ã€‚å¦‚æœ <span
class="math inline">\(A = PDP^{-1}\)</span> ä»¥ <span
class="math inline">\(D\)</span> ä¸ºå¯¹è§’çº¿ï¼Œåˆ™</p>
<p><span class="math display">\[
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} = (P^{-1}\mathbf{x})^T D
(P^{-1}\mathbf{x}).
\]</span></p>
<p>Thus quadratic forms can always be expressed as a sum of weighted
squares: å› æ­¤äºŒæ¬¡å‹æ€»æ˜¯å¯ä»¥è¡¨ç¤ºä¸ºåŠ æƒå¹³æ–¹å’Œï¼š</p>
<p><span class="math display">\[
Q(\mathbf{y}) = \lambda_1 y_1^2 + \cdots + \lambda_n y_n^2,
\]</span></p>
<p>where <span class="math inline">\(\lambda_i\)</span> are the
eigenvalues of <span class="math inline">\(A\)</span>. å…¶ä¸­ğœ† ğ‘– Î» i â€‹ æ˜¯
<span class="math inline">\(A\)</span> çš„ç‰¹å¾å€¼ã€‚</p>
<h3 id="geometric-interpretation-19">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>Quadratic forms describe geometric shapes: äºŒæ¬¡å‹æè¿°å‡ ä½•å½¢çŠ¶ï¼š</p>
<ul>
<li>In 2D: ellipses, parabolas, hyperbolas.
äºŒç»´ï¼šæ¤­åœ†ã€æŠ›ç‰©çº¿ã€åŒæ›²çº¿ã€‚</li>
<li>In 3D: ellipsoids, paraboloids, hyperboloids. åœ¨ 3D
ä¸­ï¼šæ¤­åœ†ä½“ã€æŠ›ç‰©é¢ã€åŒæ›²é¢ã€‚</li>
<li>In higher dimensions: generalizations of ellipsoids.
åœ¨æ›´é«˜ç»´åº¦ä¸­ï¼šæ¤­åœ†ä½“çš„æ¦‚æ‹¬ã€‚</li>
</ul>
<p>Diagonalization aligns the coordinate axes with the principal axes of
the shape. å¯¹è§’åŒ–å°†åæ ‡è½´ä¸å½¢çŠ¶çš„ä¸»è½´å¯¹é½ã€‚</p>
<h3 id="why-this-matters-31">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Quadratic forms unify geometry and algebra. They are central in
optimization (minimizing energy functions), statistics ( covariance
matrices and variance), mechanics (kinetic energy), and numerical
analysis. Understanding quadratic forms leads directly to the spectral
theorem.
äºŒæ¬¡å‹ç»Ÿä¸€äº†å‡ ä½•å’Œä»£æ•°ã€‚å®ƒä»¬åœ¨ä¼˜åŒ–ï¼ˆæœ€å°åŒ–èƒ½é‡å‡½æ•°ï¼‰ã€ç»Ÿè®¡å­¦ï¼ˆåæ–¹å·®çŸ©é˜µå’Œæ–¹å·®ï¼‰ã€åŠ›å­¦ï¼ˆåŠ¨èƒ½ï¼‰å’Œæ•°å€¼åˆ†æä¸­éƒ½è‡³å…³é‡è¦ã€‚ç†è§£äºŒæ¬¡å‹å¯ä»¥ç›´æ¥å¼•å‡ºè°±å®šç†ã€‚</p>
<h3 id="exercises-9.1">Exercises 9.1</h3>
<p>ç»ƒä¹  9.1</p>
<ol type="1">
<li>Write the quadratic form <span class="math inline">\(Q(x,y) = 3x^2 +
4xy + y^2\)</span> as <span class="math inline">\(\mathbf{x}^T A
\mathbf{x}\)</span> for some symmetric matrix <span
class="math inline">\(A\)</span>. å¯¹äºæŸäº›å¯¹ç§°çŸ©é˜µ <span
class="math inline">\(A\)</span> ï¼Œå°†äºŒæ¬¡å‹ <span
class="math inline">\(Q(x,y) = 3x^2 + 4xy + y^2\)</span> å†™ä¸º <span
class="math inline">\(\mathbf{x}^T A \mathbf{x}\)</span> ã€‚</li>
<li>For <span class="math inline">\(A = \begin{bmatrix} 1 &amp; 2 \\ 2
&amp; 1 \end{bmatrix}\)</span>, compute <span
class="math inline">\(Q(x,y)\)</span> explicitly. å¯¹äº <span
class="math inline">\(A = \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 1
\end{bmatrix}\)</span> ï¼Œæ˜ç¡®è®¡ç®— <span
class="math inline">\(Q(x,y)\)</span> ã€‚</li>
<li>Diagonalize the quadratic form <span class="math inline">\(Q(x,y) =
2x^2 + 2xy + 3y^2\)</span>. å°†äºŒæ¬¡å‹ <span class="math inline">\(Q(x,y)
= 2x^2 + 2xy + 3y^2\)</span> å¯¹è§’åŒ–ã€‚</li>
<li>Identify the conic section given by <span
class="math inline">\(Q(x,y) = x^2 - y^2\)</span>. ç¡®å®šç”± <span
class="math inline">\(Q(x,y) = x^2 - y^2\)</span> ç»™å‡ºçš„åœ†é”¥æˆªé¢ã€‚</li>
<li>Show that if <span class="math inline">\(A\)</span> is symmetric,
quadratic forms defined by <span class="math inline">\(A\)</span> and
<span class="math inline">\(A^T\)</span> are identical. è¯æ˜å¦‚æœ <span
class="math inline">\(A\)</span> æ˜¯å¯¹ç§°çš„ï¼Œåˆ™ç”± <span
class="math inline">\(A\)</span> å’Œ <span
class="math inline">\(A^T\)</span> å®šä¹‰çš„äºŒæ¬¡å‹æ˜¯ç›¸åŒçš„ã€‚</li>
</ol>
<h2 id="positive-definite-matrices">9.2 Positive Definite Matrices</h2>
<p>9.2 æ­£å®šçŸ©é˜µ</p>
<p>Quadratic forms are especially important when their associated
matrices are positive definite, since these guarantee positivity of
energy, distance, or variance. Positive definiteness is a cornerstone in
optimization, numerical analysis, and statistics.
å½“äºŒæ¬¡å‹çš„ç›¸å…³çŸ©é˜µä¸ºæ­£å®šçŸ©é˜µæ—¶ï¼Œå®ƒä»¬å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ä¿è¯èƒ½é‡ã€è·ç¦»æˆ–æ–¹å·®çš„æ­£æ€§ã€‚æ­£å®šæ€§æ˜¯ä¼˜åŒ–ã€æ•°å€¼åˆ†æå’Œç»Ÿè®¡å­¦çš„åŸºçŸ³ã€‚</p>
<h3 id="definition-8">Definition</h3>
<p>å®šä¹‰</p>
<p>A symmetric matrix <span class="math inline">\(A \in \mathbb{R}^{n
\times n}\)</span> is called: å¯¹ç§°çŸ©é˜µ <span class="math inline">\(A \in
\mathbb{R}^{n \times n}\)</span> ç§°ä¸ºï¼š</p>
<ul>
<li><p>Positive definite if æ­£å®šå¦‚æœ</p>
<p><span class="math display">\[
\mathbf{x}^T A \mathbf{x} &gt; 0 \quad \text{for all nonzero }
\mathbf{x} \in \mathbb{R}^n.
\]</span></p></li>
<li><p>Positive semidefinite if æ­£åŠå®šçš„ï¼Œå¦‚æœ</p>
<p><span class="math display">\[
\mathbf{x}^T A \mathbf{x} \geq 0 \quad \text{for all } \mathbf{x}.
\]</span></p></li>
</ul>
<p>Similarly, negative definite (always &lt; 0) and indefinite (can be
both &lt; 0 and &gt; 0) matrices are defined. ç±»ä¼¼åœ°ï¼Œå®šä¹‰äº†è´Ÿå®šï¼ˆå§‹ç»ˆ
&lt; 0ï¼‰å’Œä¸å®šï¼ˆå¯ä»¥åŒæ—¶ &lt; 0 å’Œ &gt; 0ï¼‰çŸ©é˜µã€‚</p>
<h3 id="examples-9">Examples</h3>
<p>ç¤ºä¾‹</p>
<p>Example 9.2.1. ä¾‹ 9.2.1ã€‚</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 0 \\0 &amp; 3 \end{bmatrix}
\]</span></p>
<p>is positive definite, since æ˜¯æ­£å®šçš„ï¼Œå› ä¸º</p>
<p><span class="math display">\[
Q(x,y) = 2x^2 + 3y^2 &gt; 0
\]</span></p>
<p>for all <span class="math inline">\((x,y) \neq (0,0)\)</span>.
å¯¹äºæ‰€æœ‰ <span class="math inline">\((x,y) \neq (0,0)\)</span> ã€‚</p>
<p>Example 9.2.2. ä¾‹ 9.2.2ã€‚</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\2 &amp; 1 \end{bmatrix}
\]</span></p>
<p>has quadratic form å…·æœ‰äºŒæ¬¡å½¢å¼</p>
<p><span class="math display">\[
Q(x,y) = x^2 + 4xy + y^2.
\]</span></p>
<p>This matrix is not positive definite, since <span
class="math inline">\(Q(1,-1) = -2 &lt; 0\)</span>.
è¯¥çŸ©é˜µä¸æ˜¯æ­£å®šçš„ï¼Œå› ä¸º <span class="math inline">\(Q(1,-1) = -2 &lt;
0\)</span> ã€‚</p>
<h3 id="characterizations">Characterizations</h3>
<p>ç‰¹å¾</p>
<p>For a symmetric matrix <span class="math inline">\(A\)</span>:
å¯¹äºå¯¹ç§°çŸ©é˜µ <span class="math inline">\(A\)</span> ï¼š</p>
<ol type="1">
<li><p>Eigenvalue test: <span class="math inline">\(A\)</span> is
positive definite if and only if all eigenvalues of <span
class="math inline">\(A\)</span> are positive. ç‰¹å¾å€¼æ£€éªŒï¼šå½“ä¸”ä»…å½“
<span class="math inline">\(A\)</span> çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½ä¸ºæ­£æ—¶ï¼Œ <span
class="math inline">\(A\)</span> æ‰æ˜¯æ­£å®šçš„ã€‚</p></li>
<li><p>Principal minors test (Sylvesterâ€™s criterion): <span
class="math inline">\(A\)</span> is positive definite if and only if all
leading principal minors ( determinants of top-left <span
class="math inline">\(k \times k\)</span> submatrices) are positive.
ä¸»å­å¼æ£€éªŒï¼ˆè¥¿å°”ç»´æ–¯ç‰¹æ ‡å‡†ï¼‰ï¼šå½“ä¸”ä»…å½“æ‰€æœ‰é¦–é¡¹ä¸»å­å¼ï¼ˆå·¦ä¸Šè§’ <span
class="math inline">\(k \times k\)</span> å­çŸ©é˜µçš„è¡Œåˆ—å¼ï¼‰å‡ä¸ºæ­£æ—¶ï¼Œ
<span class="math inline">\(A\)</span> æ‰æ˜¯æ­£å®šçš„ã€‚</p></li>
<li><p>Cholesky factorization: <span class="math inline">\(A\)</span> is
positive definite if and only if it can be written as Cholesky åˆ†è§£ï¼š
<span class="math inline">\(A\)</span> ä¸ºæ­£å®šå½“ä¸”ä»…å½“å®ƒå¯ä»¥å†™æˆ</p>
<p><span class="math display">\[
A = R^T R,
\]</span></p>
<p>where <span class="math inline">\(R\)</span> is an upper triangular
matrix with positive diagonal entries. å…¶ä¸­ <span
class="math inline">\(R\)</span> æ˜¯å…·æœ‰æ­£å¯¹è§’çº¿é¡¹çš„ä¸Šä¸‰è§’çŸ©é˜µã€‚</p></li>
</ol>
<h3 id="geometric-interpretation-20">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<ul>
<li>Positive definite matrices correspond to quadratic forms that define
ellipsoids centered at the origin.
æ­£å®šçŸ©é˜µå¯¹åº”äºå®šä¹‰ä»¥åŸç‚¹ä¸ºä¸­å¿ƒçš„æ¤­åœ†ä½“çš„äºŒæ¬¡å‹ã€‚</li>
<li>Positive semidefinite matrices define flattened ellipsoids (possibly
degenerate). æ­£åŠå®šçŸ©é˜µå®šä¹‰æ‰å¹³çš„æ¤­çƒä½“ï¼ˆå¯èƒ½æ˜¯é€€åŒ–çš„ï¼‰ã€‚</li>
<li>Indefinite matrices define hyperbolas or saddle-shaped surfaces.
ä¸å®šçŸ©é˜µå®šä¹‰åŒæ›²çº¿æˆ–é©¬éå½¢æ›²é¢ã€‚</li>
</ul>
<h3 id="applications">Applications</h3>
<p>åº”ç”¨</p>
<ul>
<li>Optimization: Hessians of convex functions are positive
semidefinite; strict convexity corresponds to positive definite
Hessians. ä¼˜åŒ–ï¼šå‡¸å‡½æ•°çš„ Hessian çŸ©é˜µæ˜¯æ­£åŠå®šçš„ï¼›ä¸¥æ ¼å‡¸æ€§å¯¹åº”äºæ­£å®šçš„
Hessian çŸ©é˜µã€‚</li>
<li>Statistics: Covariance matrices are positive semidefinite.
ç»Ÿè®¡ï¼šåæ–¹å·®çŸ©é˜µæ˜¯æ­£åŠå®šçš„ã€‚</li>
<li>Numerical methods: Cholesky decomposition is widely used to solve
systems with positive definite matrices efficiently. æ•°å€¼æ–¹æ³•ï¼šCholesky
åˆ†è§£è¢«å¹¿æ³›ç”¨äºæœ‰æ•ˆåœ°è§£å†³å…·æœ‰æ­£å®šçŸ©é˜µçš„ç³»ç»Ÿã€‚</li>
</ul>
<h3 id="why-this-matters-32">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Positive definiteness provides stability and guarantees in
mathematics and computation. It ensures energy functions are bounded
below, optimization problems have unique solutions, and statistical
models are meaningful.
æ­£å®šæ€§åœ¨æ•°å­¦å’Œè®¡ç®—ä¸­æä¾›äº†ç¨³å®šæ€§å’Œä¿è¯ã€‚å®ƒç¡®ä¿èƒ½é‡å‡½æ•°æœ‰ç•Œï¼Œä¼˜åŒ–é—®é¢˜æœ‰å”¯ä¸€è§£ï¼Œç»Ÿè®¡æ¨¡å‹æœ‰æ„ä¹‰ã€‚</p>
<h3 id="exercises-9.2">Exercises 9.2</h3>
<p>ç»ƒä¹  9.2</p>
<ol type="1">
<li><p>Use Sylvesterâ€™s criterion to check whether ä½¿ç”¨ Sylvester
æ ‡å‡†æ£€æŸ¥</p>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{bmatrix}
\]</span></p>
<p>is positive definite. æ˜¯æ­£å®šçš„ã€‚</p></li>
<li><p>Determine whether ç¡®å®šæ˜¯å¦</p>
<p><span class="math display">\[
A = \begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}
\]</span></p>
<p>is positive definite, semidefinite, or indefinite.
æ˜¯æ­£å®šçš„ã€åŠå®šçš„æˆ–ä¸å®šçš„ã€‚</p></li>
<li><p>Find the eigenvalues of æ‰¾åˆ°ç‰¹å¾å€¼</p>
<p><span class="math display">\[
A = \begin{bmatrix} 4 &amp; 2 \\ 2 &amp; 3 \end{bmatrix},
\]</span></p>
<p>and use them to classify definiteness.
å¹¶ç”¨å®ƒä»¬æ¥å¯¹ç¡®å®šæ€§è¿›è¡Œåˆ†ç±»ã€‚</p></li>
<li><p>Prove that all diagonal matrices with positive entries are
positive definite. è¯æ˜æ‰€æœ‰å…·æœ‰æ­£é¡¹çš„å¯¹è§’çŸ©é˜µéƒ½æ˜¯æ­£å®šçš„ã€‚</p></li>
<li><p>Show that if <span class="math inline">\(A\)</span> is positive
definite, then so is <span class="math inline">\(P^T A P\)</span> for
any invertible matrix <span class="math inline">\(P\)</span>. è¯æ˜å¦‚æœ
<span class="math inline">\(A\)</span> ä¸ºæ­£å®šçŸ©é˜µï¼Œåˆ™å¯¹äºä»»ä½•å¯é€†çŸ©é˜µ
<span class="math inline">\(P\)</span> ï¼Œ <span
class="math inline">\(P^T A P\)</span> ä¹Ÿä¸ºæ­£å®šçŸ©é˜µã€‚</p></li>
</ol>
<h2 id="spectral-theorem">9.3 Spectral Theorem</h2>
<p>9.3 è°±å®šç†</p>
<p>The spectral theorem is one of the most powerful results in linear
algebra. It states that symmetric matrices can always be diagonalized by
an orthogonal basis of eigenvectors. This links algebra (eigenvalues),
geometry (orthogonal directions), and applications (stability,
optimization, statistics).
è°±å®šç†æ˜¯çº¿æ€§ä»£æ•°ä¸­æœ€æœ‰åŠ›çš„ç»“è®ºä¹‹ä¸€ã€‚å®ƒæŒ‡å‡ºå¯¹ç§°çŸ©é˜µæ€»æ˜¯å¯ä»¥é€šè¿‡ç‰¹å¾å‘é‡çš„æ­£äº¤åŸºå¯¹è§’åŒ–ã€‚è¿™è¿æ¥äº†ä»£æ•°ï¼ˆç‰¹å¾å€¼ï¼‰ã€å‡ ä½•ï¼ˆæ­£äº¤æ–¹å‘ï¼‰å’Œåº”ç”¨ï¼ˆç¨³å®šæ€§ã€ä¼˜åŒ–ã€ç»Ÿè®¡ï¼‰ã€‚</p>
<h3 id="statement-of-the-spectral-theorem">Statement of the Spectral
Theorem</h3>
<p>è°±å®šç†è¡¨è¿°</p>
<p>If <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>
is symmetric (<span class="math inline">\(A^T = A\)</span>), then: å¦‚æœ
<span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>
æ˜¯å¯¹ç§°çš„ï¼ˆ <span class="math inline">\(A^T = A\)</span> ï¼‰ï¼Œåˆ™ï¼š</p>
<ol type="1">
<li><p>All eigenvalues of <span class="math inline">\(A\)</span> are
real. <span class="math inline">\(A\)</span>
çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯å®æ•°ã€‚</p></li>
<li><p>There exists an orthonormal basis of <span
class="math inline">\(\mathbb{R}^n\)</span> consisting of eigenvectors
of <span class="math inline">\(A\)</span>. å­˜åœ¨ç”± <span
class="math inline">\(A\)</span> çš„ç‰¹å¾å‘é‡ç»„æˆçš„ <span
class="math inline">\(\mathbb{R}^n\)</span> æ­£äº¤åŸºã€‚</p></li>
<li><p>Thus, <span class="math inline">\(A\)</span> can be written as
å› æ­¤ï¼Œ <span class="math inline">\(A\)</span> å¯ä»¥å†™æˆ</p>
<p><span class="math display">\[
A = Q \Lambda Q^T,
\]</span></p>
<p>where <span class="math inline">\(Q\)</span> is an orthogonal matrix
(<span class="math inline">\(Q^T Q = I\)</span>) and <span
class="math inline">\(\Lambda\)</span> is diagonal with eigenvalues of
<span class="math inline">\(A\)</span> on the diagonal. å…¶ä¸­ <span
class="math inline">\(Q\)</span> æ˜¯æ­£äº¤çŸ©é˜µ ( <span
class="math inline">\(Q^T Q = I\)</span> )ï¼Œ <span
class="math inline">\(\Lambda\)</span> æ˜¯å¯¹è§’çŸ©é˜µï¼Œå…¶ç‰¹å¾å€¼ <span
class="math inline">\(A\)</span> ä½äºå¯¹è§’çº¿ä¸Šã€‚</p></li>
</ol>
<h3 id="consequences">Consequences</h3>
<p>ç»“æœ</p>
<ul>
<li>Symmetric matrices are always diagonalizable, and the
diagonalization is numerically stable.
å¯¹ç§°çŸ©é˜µæ€»æ˜¯å¯å¯¹è§’åŒ–çš„ï¼Œå¹¶ä¸”å¯¹è§’åŒ–åœ¨æ•°å€¼ä¸Šæ˜¯ç¨³å®šçš„ã€‚</li>
<li>Quadratic forms <span class="math inline">\(\mathbf{x}^T A
\mathbf{x}\)</span> can be expressed in terms of eigenvalues and
eigenvectors, showing ellipsoids aligned with eigen-directions. äºŒæ¬¡å‹
<span class="math inline">\(\mathbf{x}^T A \mathbf{x}\)</span>
å¯ä»¥ç”¨ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ¥è¡¨ç¤ºï¼Œæ˜¾ç¤ºä¸ç‰¹å¾æ–¹å‘å¯¹é½çš„æ¤­åœ†ä½“ã€‚</li>
<li>Positive definiteness can be checked by confirming that all
eigenvalues are positive.
å¯ä»¥é€šè¿‡ç¡®è®¤æ‰€æœ‰ç‰¹å¾å€¼éƒ½ä¸ºæ­£æ¥æ£€æŸ¥æ­£å®šæ€§ã€‚</li>
</ul>
<h3 id="example-9.3.1">Example 9.3.1</h3>
<p>ä¾‹ 9.3.1</p>
<p>Let è®©</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 1 \\1 &amp; 2 \end{bmatrix}.
\]</span></p>
<ol type="1">
<li>Characteristic polynomial: ç‰¹å¾å¤šé¡¹å¼ï¼š</li>
</ol>
<p><span class="math display">\[
p(\lambda) = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
\]</span></p>
<p>Eigenvalues: <span class="math inline">\(\lambda_1 = 1, \ \lambda_2 =
3\)</span>. ç‰¹å¾å€¼ï¼š <span class="math inline">\(\lambda_1 = 1, \
\lambda_2 = 3\)</span> ã€‚</p>
<ol start="2" type="1">
<li>Eigenvectors: ç‰¹å¾å‘é‡ï¼š</li>
</ol>
<ul>
<li>For <span class="math inline">\(\lambda=1\)</span>: solve <span
class="math inline">\((A-I)\mathbf{v} = 0\)</span>, giving <span
class="math inline">\((1,-1)\)</span>. å¯¹äº <span
class="math inline">\(\lambda=1\)</span> ï¼šæ±‚è§£ <span
class="math inline">\((A-I)\mathbf{v} = 0\)</span> ï¼Œå¾—åˆ° <span
class="math inline">\((1,-1)\)</span> ã€‚</li>
<li>For <span class="math inline">\(\lambda=3\)</span>: solve <span
class="math inline">\((A-3I)\mathbf{v} = 0\)</span>, giving <span
class="math inline">\((1,1)\)</span>. å¯¹äº <span
class="math inline">\(\lambda=3\)</span> ï¼šæ±‚è§£ <span
class="math inline">\((A-3I)\mathbf{v} = 0\)</span> ï¼Œå¾—åˆ° <span
class="math inline">\((1,1)\)</span> ã€‚</li>
</ul>
<ol start="3" type="1">
<li>Normalize eigenvectors: å½’ä¸€åŒ–ç‰¹å¾å‘é‡ï¼š</li>
</ol>
<p><span class="math display">\[
\mathbf{u}_1 = \tfrac{1}{\sqrt{2}}(1,-1), \quad \mathbf{u}_2 =
\tfrac{1}{\sqrt{2}}(1,1).
\]</span></p>
<ol start="4" type="1">
<li>Then ç„¶å</li>
</ol>
<p><span class="math display">\[
Q =\begin{bmatrix}\tfrac{1}{\sqrt{2}} &amp; \tfrac{1}{\sqrt{2}} \\[6pt]
-\tfrac{1}{\sqrt{2}} &amp; \tfrac{1}{\sqrt{2}}\end{bmatrix},
\quad\Lambda =\begin{bmatrix}1 &amp; 0 \\0 &amp; 3\end{bmatrix}.
\]</span></p>
<p>So æ‰€ä»¥</p>
<p><span class="math display">\[
A = Q \Lambda Q^T.
\]</span></p>
<h3 id="geometric-interpretation-21">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<p>The spectral theorem says every symmetric matrix acts like
independent scaling along orthogonal directions. In geometry, this
corresponds to stretching space along perpendicular axes.
è°±å®šç†æŒ‡å‡ºï¼Œæ¯ä¸ªå¯¹ç§°çŸ©é˜µéƒ½åƒæ²¿æ­£äº¤æ–¹å‘çš„ç‹¬ç«‹ç¼©æ”¾ä¸€æ ·ã€‚åœ¨å‡ ä½•å­¦ä¸­ï¼Œè¿™ç›¸å½“äºæ²¿å‚ç›´è½´æ‹‰ä¼¸ç©ºé—´ã€‚</p>
<ul>
<li>Ellipses, ellipsoids, and quadratic surfaces can be fully understood
via eigenvalues and eigenvectors.
é€šè¿‡ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡å¯ä»¥å……åˆ†ç†è§£æ¤­åœ†ã€æ¤­åœ†ä½“å’ŒäºŒæ¬¡æ›²é¢ã€‚</li>
<li>Orthogonality ensures directions remain perpendicular after
transformation. æ­£äº¤æ€§ç¡®ä¿æ–¹å‘åœ¨å˜æ¢åä¿æŒå‚ç›´ã€‚</li>
</ul>
<h3 id="applications-1">Applications</h3>
<p>åº”ç”¨</p>
<ul>
<li>Optimization: The spectral theorem underlies classification of
critical points via eigenvalues of the Hessian. ä¼˜åŒ–ï¼šè°±å®šç†æ˜¯é€šè¿‡
Hessian çš„ç‰¹å¾å€¼å¯¹ä¸´ç•Œç‚¹è¿›è¡Œåˆ†ç±»çš„åŸºç¡€ã€‚</li>
<li>PCA (Principal Component Analysis): Data covariance matrices are
symmetric, and PCA finds orthogonal directions of maximum variance.
PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ï¼šæ•°æ®åæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼ŒPCA
æ‰¾åˆ°æœ€å¤§æ–¹å·®çš„æ­£äº¤æ–¹å‘ã€‚</li>
<li>Differential equations &amp; physics: Symmetric operators correspond
to measurable quantities with real eigenvalues ( stability, energy).
å¾®åˆ†æ–¹ç¨‹å’Œç‰©ç†å­¦ï¼šå¯¹ç§°ç®—å­å¯¹åº”äºå…·æœ‰å®ç‰¹å¾å€¼ï¼ˆç¨³å®šæ€§ã€èƒ½é‡ï¼‰çš„å¯æµ‹é‡é‡ã€‚</li>
</ul>
<h3 id="why-this-matters-33">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>The spectral theorem guarantees that symmetric matrices are as simple
as possible: they can always be analyzed in terms of real, orthogonal
eigenvectors. This provides both deep theoretical insight and powerful
computational tools.
è°±å®šç†ä¿è¯å¯¹ç§°çŸ©é˜µå°½å¯èƒ½ç®€å•ï¼šå®ƒä»¬æ€»æ˜¯å¯ä»¥ç”¨å®æ•°æ­£äº¤ç‰¹å¾å‘é‡æ¥åˆ†æã€‚è¿™æ—¢æä¾›äº†æ·±åˆ»çš„ç†è®ºè§è§£ï¼Œä¹Ÿæä¾›äº†å¼ºå¤§çš„è®¡ç®—å·¥å…·ã€‚</p>
<h3 id="exercises-9.3">Exercises 9.3</h3>
<p>ç»ƒä¹  9.3</p>
<ol type="1">
<li><p>Diagonalize å¯¹è§’åŒ–</p>
<p><span class="math display">\[
A = \begin{bmatrix} 4 &amp; 2 \\ 2 &amp; 3 \end{bmatrix}
\]</span></p>
<p>using the spectral theorem. ä½¿ç”¨è°±å®šç†ã€‚</p></li>
<li><p>Prove that all eigenvalues of a real symmetric matrix are real.
è¯æ˜å®å¯¹ç§°çŸ©é˜µçš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯å®æ•°ã€‚</p></li>
<li><p>Show that eigenvectors corresponding to distinct eigenvalues of a
symmetric matrix are orthogonal.
è¯æ˜å¯¹ç§°çŸ©é˜µçš„ä¸åŒç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡æ˜¯æ­£äº¤çš„ã€‚</p></li>
<li><p>Explain geometrically how the spectral theorem describes
ellipsoids defined by quadratic forms.
ä»å‡ ä½•è§’åº¦è§£é‡Šè°±å®šç†å¦‚ä½•æè¿°ç”±äºŒæ¬¡å‹å®šä¹‰çš„æ¤­çƒä½“ã€‚</p></li>
<li><p>Apply the spectral theorem to the covariance matrix
å°†è°±å®šç†åº”ç”¨äºåæ–¹å·®çŸ©é˜µ</p>
<p><span class="math display">\[
\Sigma = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix},
\]</span></p>
<p>and interpret the eigenvectors as principal directions of variance.
å¹¶å°†ç‰¹å¾å‘é‡è§£é‡Šä¸ºæ–¹å·®çš„ä¸»æ–¹å‘ã€‚</p></li>
</ol>
<h2 id="principal-component-analysis-pca">9.4 Principal Component
Analysis (PCA)</h2>
<p>9.4 ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰</p>
<p>Principal Component Analysis (PCA) is a widely used technique in data
science, machine learning, and statistics. At its core, PCA is an
application of the spectral theorem to covariance matrices: it finds
orthogonal directions (principal components) that capture the maximum
variance in data. ä¸»æˆåˆ†åˆ†æ (PCA)
æ˜¯æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡å­¦ä¸­å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ã€‚PCA
çš„æ ¸å¿ƒæ˜¯è°±å®šç†åœ¨åæ–¹å·®çŸ©é˜µä¸­çš„åº”ç”¨ï¼šå®ƒæ‰¾åˆ°èƒ½å¤Ÿæ•æ‰æ•°æ®ä¸­æœ€å¤§æ–¹å·®çš„æ­£äº¤æ–¹å‘ï¼ˆä¸»æˆåˆ†ï¼‰ã€‚</p>
<h3 id="the-idea-1">The Idea</h3>
<p>ç†å¿µ</p>
<p>Given a dataset of vectors <span class="math inline">\(\mathbf{x}_1,
\mathbf{x}_2, \dots, \mathbf{x}_m \in \mathbb{R}^n\)</span>:
ç»™å®šå‘é‡æ•°æ®é›† <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2,
\dots, \mathbf{x}_m \in \mathbb{R}^n\)</span> ï¼š</p>
<ol type="1">
<li><p>Center the data by subtracting the mean vector <span
class="math inline">\(\bar{\mathbf{x}}\)</span>. é€šè¿‡å‡å»å¹³å‡å‘é‡ <span
class="math inline">\(\bar{\mathbf{x}}\)</span> ä½¿æ•°æ®å±…ä¸­ã€‚</p></li>
<li><p>Form the covariance matrix å½¢æˆåæ–¹å·®çŸ©é˜µ</p>
<p><span class="math display">\[
\Sigma = \frac{1}{m} \sum_{i=1}^m (\mathbf{x}_i -
\bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T.
\]</span></p></li>
<li><p>Apply the spectral theorem: <span class="math inline">\(\Sigma =
Q \Lambda Q^T\)</span>. åº”ç”¨è°±å®šç†ï¼š <span class="math inline">\(\Sigma
= Q \Lambda Q^T\)</span> ã€‚</p>
<ul>
<li>Columns of <span class="math inline">\(Q\)</span> are orthonormal
eigenvectors (principal directions). <span
class="math inline">\(Q\)</span> çš„åˆ—æ˜¯æ­£äº¤ç‰¹å¾å‘é‡ï¼ˆä¸»æ–¹å‘ï¼‰ã€‚</li>
<li>Eigenvalues in <span class="math inline">\(\Lambda\)</span> measure
variance explained by each direction. <span
class="math inline">\(\Lambda\)</span>
ä¸­çš„ç‰¹å¾å€¼æµ‹é‡æ¯ä¸ªæ–¹å‘è§£é‡Šçš„æ–¹å·®ã€‚</li>
</ul></li>
</ol>
<p>The first principal component is the eigenvector corresponding to the
largest eigenvalue; it is the direction of maximum variance.
ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ˜¯æœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œæ˜¯æ–¹å·®æœ€å¤§çš„æ–¹å‘ã€‚</p>
<h3 id="example-9.4.1">Example 9.4.1</h3>
<p>ä¾‹ 9.4.1</p>
<p>Suppose we have two-dimensional data points roughly aligned along the
line <span class="math inline">\(y = x\)</span>. The covariance matrix
is approximately å‡è®¾æˆ‘ä»¬æœ‰äºŒç»´æ•°æ®ç‚¹å¤§è‡´æ²¿ç€ç›´çº¿ <span
class="math inline">\(y = x\)</span> æ’åˆ—ã€‚åæ–¹å·®çŸ©é˜µå¤§çº¦ä¸º</p>
<p><span class="math display">\[
\Sigma =\begin{bmatrix}2 &amp; 1.9 \\1.9 &amp; 2\end{bmatrix}.
\]</span></p>
<p>Eigenvalues are about <span class="math inline">\(3.9 and
\\0.1\)</span>. The eigenvector for <span class="math inline">\(\\lambda
= 3.9\)</span>is approximately<span
class="math inline">\((1,1)/\\sqrt{2}\)</span>. ç‰¹å¾å€¼çº¦ä¸º $3.9 å’Œ \ 0.1
$. The eigenvector for $ \lambda = 3.9 <span class="math inline">\(is
approximately\)</span> (1,1)/\sqrt{2}$ã€‚</p>
<ul>
<li>First principal component: the line <span class="math inline">\(y =
x\)</span>. ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ï¼šçº¿ <span class="math inline">\(y = x\)</span>
ã€‚</li>
<li>Most variance lies along this direction.
å¤§éƒ¨åˆ†å·®å¼‚éƒ½å‘ç”Ÿåœ¨è¿™ä¸ªæ–¹å‘ã€‚</li>
<li>Second component is nearly orthogonal (<span class="math inline">\(y
= -x\)</span>), but variance there is tiny. ç¬¬äºŒä¸ªæˆåˆ†å‡ ä¹æ­£äº¤ï¼ˆ <span
class="math inline">\(y = -x\)</span> ï¼‰ï¼Œä½†é‚£é‡Œçš„æ–¹å·®å¾ˆå°ã€‚</li>
</ul>
<p>Thus PCA reduces the data to essentially one dimension. å› æ­¤ï¼ŒPCA
å°†æ•°æ®ç®€åŒ–ä¸ºä¸€ä¸ªç»´åº¦ã€‚</p>
<h3 id="applications-of-pca">Applications of PCA</h3>
<p>PCA çš„åº”ç”¨</p>
<ol type="1">
<li>Dimensionality reduction: Represent data with fewer features while
retaining most variance.
é™ç»´ï¼šç”¨è¾ƒå°‘çš„ç‰¹å¾è¡¨ç¤ºæ•°æ®ï¼ŒåŒæ—¶ä¿ç•™å¤§éƒ¨åˆ†çš„æ–¹å·®ã€‚</li>
<li>Noise reduction: Small eigenvalues correspond to noise; discarding
them filters data.
é™å™ªï¼šè¾ƒå°çš„ç‰¹å¾å€¼å¯¹åº”å™ªå£°ï¼›ä¸¢å¼ƒå®ƒä»¬å¯ä»¥è¿‡æ»¤æ•°æ®ã€‚</li>
<li>Visualization: Projecting high-dimensional data onto top 2 or 3
principal components reveals structure. å¯è§†åŒ–ï¼šå°†é«˜ç»´æ•°æ®æŠ•å½±åˆ°å‰ 2
ä¸ªæˆ– 3 ä¸ªä¸»æˆåˆ†ä¸Šå¯ä»¥æ­ç¤ºç»“æ„ã€‚</li>
<li>Compression: PCA is used in image and signal compression. å‹ç¼©ï¼šPCA
ç”¨äºå›¾åƒå’Œä¿¡å·å‹ç¼©ã€‚</li>
</ol>
<h3 id="connection-to-the-spectral-theorem">Connection to the Spectral
Theorem</h3>
<p>ä¸è°±å®šç†çš„è”ç³»</p>
<p>The covariance matrix <span class="math inline">\(\Sigma\)</span> is
always symmetric and positive semidefinite. Hence by the spectral
theorem, it has an orthonormal basis of eigenvectors and nonnegative
real eigenvalues. PCA is nothing more than re-expressing data in this
eigenbasis. åæ–¹å·®çŸ©é˜µ <span class="math inline">\(\Sigma\)</span>
å§‹ç»ˆæ˜¯å¯¹ç§°çš„ï¼Œä¸”ä¸ºåŠæ­£å®šçŸ©é˜µã€‚å› æ­¤ï¼Œæ ¹æ®è°±å®šç†ï¼Œå®ƒæœ‰ä¸€ä¸ªç”±ç‰¹å¾å‘é‡å’Œéè´Ÿå®ç‰¹å¾å€¼ç»„æˆçš„æ­£äº¤åŸºã€‚PCA
åªä¸è¿‡æ˜¯åœ¨è¿™ä¸ªç‰¹å¾åŸºä¸Šé‡æ–°è¡¨è¾¾æ•°æ®ã€‚</p>
<h3 id="why-this-matters-34">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>PCA demonstrates how abstract linear algebra directly powers modern
applications. Eigenvalues and eigenvectors give a practical method for
simplifying data, revealing patterns, and reducing complexity. It is one
of the most important algorithms derived from the spectral theorem. PCA
å±•ç¤ºäº†æŠ½è±¡çº¿æ€§ä»£æ•°å¦‚ä½•ç›´æ¥é©±åŠ¨ç°ä»£åº”ç”¨ã€‚ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æä¾›äº†ä¸€ç§ç®€åŒ–æ•°æ®ã€æ­ç¤ºæ¨¡å¼å’Œé™ä½å¤æ‚æ€§çš„å®ç”¨æ–¹æ³•ã€‚å®ƒæ˜¯ä»è°±å®šç†ä¸­æ¨å¯¼å‡ºçš„æœ€é‡è¦çš„ç®—æ³•ä¹‹ä¸€ã€‚</p>
<h3 id="exercises-9.4">Exercises 9.4</h3>
<p>ç»ƒä¹  9.4</p>
<ol type="1">
<li>Show that the covariance matrix is symmetric and positive
semidefinite. è¯æ˜åæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„å’ŒåŠæ­£å®šçš„ã€‚</li>
<li>Compute the covariance matrix of the dataset <span
class="math inline">\((1,2), (2,3), (3,4)\)</span>, and find its
eigenvalues and eigenvectors. è®¡ç®—æ•°æ®é›† <span
class="math inline">\((1,2), (2,3), (3,4)\)</span>
çš„åæ–¹å·®çŸ©é˜µï¼Œå¹¶æ‰¾åˆ°å…¶ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚</li>
<li>Explain why the first principal component captures the maximum
variance. è§£é‡Šä¸ºä»€ä¹ˆç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ•è·æœ€å¤§æ–¹å·®ã€‚</li>
<li>In image compression, explain how PCA can reduce storage by keeping
only the top <span class="math inline">\(k\)</span> principal
components. åœ¨å›¾åƒå‹ç¼©ä¸­ï¼Œè§£é‡Š PCA å¦‚ä½•é€šè¿‡ä»…ä¿ç•™å‰ <span
class="math inline">\(k\)</span> ä¸ªä¸»æˆåˆ†æ¥å‡å°‘å­˜å‚¨ã€‚</li>
<li>Prove that the sum of the eigenvalues of the covariance matrix
equals the total variance of the dataset.
è¯æ˜åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼ä¹‹å’Œç­‰äºæ•°æ®é›†çš„æ€»æ–¹å·®ã€‚</li>
</ol>
<h1 id="chapter-10.-linear-algebra-in-practice">Chapter 10. Linear
Algebra in Practice</h1>
<p>ç¬¬ 10 ç«  çº¿æ€§ä»£æ•°å®è·µ</p>
<h2 id="computer-graphics-rotations-projections">10.1 Computer Graphics
(Rotations, Projections)</h2>
<p>10.1 è®¡ç®—æœºå›¾å½¢å­¦ï¼ˆæ—‹è½¬ã€æŠ•å½±ï¼‰</p>
<p>Linear algebra is the language of modern computer graphics. Every
image rendered on a screen, every 3D model rotated or projected, is
ultimately the result of applying matrices to vectors. Rotations,
reflections, scalings, and projections are all linear transformations,
making matrices the natural tool for manipulating geometry.
çº¿æ€§ä»£æ•°æ˜¯ç°ä»£è®¡ç®—æœºå›¾å½¢å­¦çš„è¯­è¨€ã€‚å±å¹•ä¸Šæ¸²æŸ“çš„æ¯ä¸€å¹…å›¾åƒï¼Œä»¥åŠæ—‹è½¬æˆ–æŠ•å½±çš„æ¯ä¸€ä¸ª
3D
æ¨¡å‹ï¼Œæœ€ç»ˆéƒ½æ˜¯å°†çŸ©é˜µåº”ç”¨äºå‘é‡çš„ç»“æœã€‚æ—‹è½¬ã€åå°„ã€ç¼©æ”¾å’ŒæŠ•å½±éƒ½æ˜¯çº¿æ€§å˜æ¢ï¼Œè¿™ä½¿å¾—çŸ©é˜µæˆä¸ºå¤„ç†å‡ ä½•å›¾å½¢çš„å¤©ç„¶å·¥å…·ã€‚</p>
<h3 id="rotations-in-2d">Rotations in 2D</h3>
<p>äºŒç»´æ—‹è½¬</p>
<p>A counterclockwise rotation by an angle <span
class="math inline">\(\theta\)</span> in the plane is represented by
åœ¨å¹³é¢ä¸Šé€†æ—¶é’ˆæ—‹è½¬è§’åº¦ <span class="math inline">\(\theta\)</span>
è¡¨ç¤ºä¸º</p>
<p><span class="math display">\[
R_\theta =\begin{bmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta &amp;
\cos\theta\end{bmatrix}.
\]</span></p>
<p>For any vector <span class="math inline">\(\mathbf{v} \in
\mathbb{R}^2\)</span>, the rotated vector is å¯¹äºä»»æ„å‘é‡ <span
class="math inline">\(\mathbf{v} \in \mathbb{R}^2\)</span>
ï¼Œæ—‹è½¬åçš„å‘é‡ä¸º</p>
<p><span class="math display">\[
\mathbf{v}&#39; = R_\theta \mathbf{v}.
\]</span></p>
<p>This preserves lengths and angles, since <span
class="math inline">\(R_\theta\)</span> is orthogonal with determinant
1. è¿™ä¿ç•™äº†é•¿åº¦å’Œè§’åº¦ï¼Œå› ä¸ºğ‘… ğœƒ R Î¸ â€‹ ä¸è¡Œåˆ—å¼ 1 æ­£äº¤ã€‚</p>
<h3 id="rotations-in-3d">Rotations in 3D</h3>
<p>3D æ—‹è½¬</p>
<p>In three dimensions, rotations are represented by <span
class="math inline">\(3 \\times 3 orthogonal matrices with determinant
\\1\)</span>. For example, arotation about the <span
class="math inline">\(z\)</span>-axis is åœ¨ä¸‰ç»´ç©ºé—´ä¸­ï¼Œæ—‹è½¬ç”±$3 \times 3
æ­£äº¤çŸ©é˜µè¡¨ç¤ºï¼Œå…¶è¡Œåˆ—å¼ä¸º \ 1 $. For example, arotation about the $
z$è½´ä¸º</p>
<p><span class="math display">\[
R_z(\theta) =\begin{bmatrix}\cos\theta &amp; -\sin\theta &amp; 0
\\\sin\theta &amp; \cos\theta &amp; 0 \\0 &amp; 0 &amp; 1\end{bmatrix}.
\]</span></p>
<p>Similar formulas exist for rotations about the <span
class="math inline">\(x\)</span>- and <span
class="math inline">\(y\)</span>-axes. å¯¹äºç»• <span
class="math inline">\(x\)</span> è½´å’Œ <span
class="math inline">\(y\)</span> è½´çš„æ—‹è½¬ä¹Ÿå­˜åœ¨ç±»ä¼¼çš„å…¬å¼ã€‚</p>
<p>More general 3D rotations can be described by axisâ€“angle
representation or quaternions, but the underlying idea is still linear
transformations represented by matrices. æ›´ä¸€èˆ¬çš„ 3D
æ—‹è½¬å¯ä»¥ç”¨è½´è§’è¡¨ç¤ºæˆ–å››å…ƒæ•°æ¥æè¿°ï¼Œä½†å…¶åŸºæœ¬æ€æƒ³ä»ç„¶æ˜¯çŸ©é˜µè¡¨ç¤ºçš„çº¿æ€§å˜æ¢ã€‚</p>
<h3 id="projections-1">Projections</h3>
<p>é¢„æµ‹</p>
<p>To display 3D objects on a 2D screen, we use projections: ä¸ºäº†åœ¨ 2D
å±å¹•ä¸Šæ˜¾ç¤º 3D å¯¹è±¡ï¼Œæˆ‘ä»¬ä½¿ç”¨æŠ•å½±ï¼š</p>
<ol type="1">
<li><p>Orthogonal projection: drops the <span
class="math inline">\(z\)</span>-coordinate, mapping <span
class="math inline">\((x,y,z) \mapsto (x,y)\)</span>. æ­£äº¤æŠ•å½±ï¼šåˆ é™¤
<span class="math inline">\(z\)</span> åæ ‡ï¼Œæ˜ å°„ <span
class="math inline">\((x,y,z) \mapsto (x,y)\)</span> ã€‚</p>
<p><span class="math display">\[
P = \begin{bmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0\end{bmatrix}.
\]</span></p></li>
<li><p>Perspective projection: mimics the effect of a camera. A point
<span class="math inline">\((x,y,z)\)</span> projects to
é€è§†æŠ•å½±ï¼šæ¨¡æ‹Ÿç›¸æœºçš„æ•ˆæœã€‚ç‚¹ <span
class="math inline">\((x,y,z)\)</span> æŠ•å½±åˆ°</p>
<p><span class="math display">\[
\left(\frac{x}{z}, \frac{y}{z}\right),
\]</span></p>
<p>capturing how distant objects appear smaller.
æ•æ‰è¿œå¤„ç‰©ä½“å¦‚ä½•æ˜¾å¾—æ›´å°ã€‚</p></li>
</ol>
<p>These operations are linear (orthogonal projection) or nearly linear
(perspective projection becomes linear in homogeneous coordinates).
è¿™äº›æ“ä½œæ˜¯çº¿æ€§çš„ï¼ˆæ­£äº¤æŠ•å½±ï¼‰æˆ–è¿‘ä¼¼çº¿æ€§çš„ï¼ˆé€è§†æŠ•å½±åœ¨é½æ¬¡åæ ‡ä¸­å˜ä¸ºçº¿æ€§ï¼‰ã€‚</p>
<h3 id="homogeneous-coordinates">Homogeneous Coordinates</h3>
<p>é½æ¬¡åæ ‡</p>
<p>To unify translations and projections with linear transformations,
computer graphics uses homogeneous coordinates. A 3D point <span
class="math inline">\((x,y,z)\)</span> is represented as a 4D vector
<span class="math inline">\((x,y,z,1)\)</span>. Transformations are then
4Ã—4 matrices, which can represent rotations, scalings, and translations
in a single framework.
ä¸ºäº†å°†å¹³ç§»å’ŒæŠ•å½±ä¸çº¿æ€§å˜æ¢ç»Ÿä¸€èµ·æ¥ï¼Œè®¡ç®—æœºå›¾å½¢å­¦ä½¿ç”¨é½æ¬¡åæ ‡ã€‚3D ç‚¹
<span class="math inline">\((x,y,z)\)</span> è¡¨ç¤ºä¸ºå››ç»´å‘é‡ <span
class="math inline">\((x,y,z,1)\)</span> ã€‚å˜æ¢åˆ™è¡¨ç¤ºä¸ºçŸ©é˜µ 4Ã—4
ï¼Œå¯ä»¥åœ¨å•ä¸ªæ¡†æ¶ä¸­è¡¨ç¤ºæ—‹è½¬ã€ç¼©æ”¾å’Œå¹³ç§»ã€‚</p>
<p>Example: Translation by <span class="math inline">\((a,b,c)\)</span>:
ä¾‹å¦‚ï¼š <span class="math inline">\((a,b,c)\)</span> ç¿»è¯‘ï¼š</p>
<p><span class="math display">\[
T = \begin{bmatrix}1 &amp; 0 &amp; 0 &amp; a \\0 &amp; 1 &amp; 0 &amp; b
\\0 &amp; 0 &amp; 1 &amp; c \\0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix}.
\]</span></p>
<h3 id="geometric-interpretation-22">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<ul>
<li>Rotations preserve shape and size, only changing orientation.
æ—‹è½¬ä¿æŒå½¢çŠ¶å’Œå¤§å°ï¼Œä»…æ”¹å˜æ–¹å‘ã€‚</li>
<li>Projections reduce dimension: from 3D world space to 2D screen
space. æŠ•å½±å‡å°‘ç»´åº¦ï¼šä» 3D ä¸–ç•Œç©ºé—´åˆ° 2D å±å¹•ç©ºé—´ã€‚</li>
<li>Homogeneous coordinates allow us to combine multiple transformations
(rotation + translation + projection) into a single matrix
multiplication.
é½æ¬¡åæ ‡å…è®¸æˆ‘ä»¬å°†å¤šä¸ªå˜æ¢ï¼ˆæ—‹è½¬+å¹³ç§»+æŠ•å½±ï¼‰ç»„åˆæˆå•ä¸ªçŸ©é˜µä¹˜æ³•ã€‚</li>
</ul>
<h3 id="why-this-matters-35">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Linear algebra enables all real-time graphics: video games,
simulations, CAD software, and movie effects. By chaining simple matrix
operations, complex transformations are applied efficiently to millions
of points per second. çº¿æ€§ä»£æ•°æ”¯æŒæ‰€æœ‰å®æ—¶å›¾å½¢ï¼šè§†é¢‘æ¸¸æˆã€æ¨¡æ‹Ÿã€CAD
è½¯ä»¶å’Œç”µå½±ç‰¹æ•ˆã€‚é€šè¿‡é“¾æ¥ç®€å•çš„çŸ©é˜µè¿ç®—ï¼Œå¤æ‚çš„å˜æ¢å¯ä»¥é«˜æ•ˆåœ°åº”ç”¨äºæ¯ç§’æ•°ç™¾ä¸‡ä¸ªç‚¹ã€‚</p>
<h3 id="exercises-10.1">Exercises 10.1</h3>
<p>ç»ƒä¹ 10.1</p>
<ol type="1">
<li>Write the rotation matrix for a 90Â° counterclockwise rotation in
<span class="math inline">\(\mathbb{R}^2\)</span>. Apply it to <span
class="math inline">\((1,0)\)</span>. åœ¨ <span
class="math inline">\(\mathbb{R}^2\)</span> ä¸­å†™å‡ºé€†æ—¶é’ˆæ—‹è½¬ 90Â°
çš„æ—‹è½¬çŸ©é˜µã€‚å°†å…¶åº”ç”¨åˆ° <span class="math inline">\((1,0)\)</span>
ã€‚</li>
<li>Rotate the point <span class="math inline">\((1,1,0)\)</span> about
the <span class="math inline">\(z\)</span>-axis by 180Â°. å°†ç‚¹ <span
class="math inline">\((1,1,0)\)</span> ç»• <span
class="math inline">\(z\)</span> è½´æ—‹è½¬ 180Â°ã€‚</li>
<li>Show that the determinant of any 2D or 3D rotation matrix is 1.
è¯æ˜ä»»ä½•äºŒç»´æˆ–ä¸‰ç»´æ—‹è½¬çŸ©é˜µçš„è¡Œåˆ—å¼ä¸º 1ã€‚</li>
<li>Derive the orthogonal projection matrix from <span
class="math inline">\(\mathbb{R}^3\)</span> to the <span
class="math inline">\(xy\)</span>-plane. æ¨å¯¼ä» <span
class="math inline">\(\mathbb{R}^3\)</span> åˆ° <span
class="math inline">\(xy\)</span> å¹³é¢çš„æ­£äº¤æŠ•å½±çŸ©é˜µã€‚</li>
<li>Explain how homogeneous coordinates allow translations to be
represented as matrix multiplications.
è§£é‡Šé½æ¬¡åæ ‡å¦‚ä½•å…è®¸å¹³ç§»è¡¨ç¤ºä¸ºçŸ©é˜µä¹˜æ³•ã€‚</li>
</ol>
<h2 id="data-science-dimensionality-reduction-least-squares">10.2 Data
Science (Dimensionality Reduction, Least Squares)</h2>
<p>10.2 æ•°æ®ç§‘å­¦ï¼ˆé™ç»´ã€æœ€å°äºŒä¹˜ï¼‰</p>
<p>Linear algebra provides the foundation for many data science
techniques. Two of the most important are dimensionality reduction,
where high-dimensional datasets are compressed while preserving
essential information, and the least squares method, which underlies
regression and model fitting.
çº¿æ€§ä»£æ•°ä¸ºè®¸å¤šæ•°æ®ç§‘å­¦æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚å…¶ä¸­æœ€é‡è¦çš„ä¸¤ä¸ªæŠ€æœ¯æ˜¯é™ç»´ï¼ˆåœ¨ä¿ç•™åŸºæœ¬ä¿¡æ¯çš„åŒæ—¶å‹ç¼©é«˜ç»´æ•°æ®é›†ï¼‰å’Œæœ€å°äºŒä¹˜æ³•ï¼ˆå›å½’å’Œæ¨¡å‹æ‹Ÿåˆçš„åŸºç¡€ï¼‰ã€‚</p>
<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>
<p>é™ç»´</p>
<p>High-dimensional data often contains redundancy: many features are
correlated, meaning the data essentially lies near a lower-dimensional
subspace. Dimensionality reduction identifies these subspaces.
é«˜ç»´æ•°æ®é€šå¸¸åŒ…å«å†—ä½™ï¼šè®¸å¤šç‰¹å¾ç›¸äº’å…³è”ï¼Œè¿™æ„å‘³ç€æ•°æ®æœ¬è´¨ä¸Šä½äºä½ç»´å­ç©ºé—´é™„è¿‘ã€‚é™ç»´å¯ä»¥è¯†åˆ«è¿™äº›å­ç©ºé—´ã€‚</p>
<ul>
<li><p>PCA (Principal Component Analysis): As introduced earlier, PCA
diagonalizes the covariance matrix of the data.
PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ï¼šå¦‚å‰æ‰€è¿°ï¼ŒPCA å°†æ•°æ®çš„åæ–¹å·®çŸ©é˜µå¯¹è§’åŒ–ã€‚</p>
<ul>
<li>Eigenvectors (principal components) define orthogonal directions of
maximum variance. ç‰¹å¾å‘é‡ï¼ˆä¸»æˆåˆ†ï¼‰å®šä¹‰æœ€å¤§æ–¹å·®çš„æ­£äº¤æ–¹å‘ã€‚</li>
<li>Eigenvalues measure how much variance lies along each direction.
ç‰¹å¾å€¼è¡¡é‡æ¯ä¸ªæ–¹å‘ä¸Šçš„æ–¹å·®ã€‚</li>
<li>Keeping only the top <span class="math inline">\(k\)</span>
components reduces data from <span
class="math inline">\(n\)</span>-dimensional space to <span
class="math inline">\(k\)</span>-dimensional space while retaining most
variability. ä»…ä¿ç•™å‰ <span class="math inline">\(k\)</span>
ä¸ªæˆåˆ†å¯å°†æ•°æ®ä» <span class="math inline">\(n\)</span> ç»´ç©ºé—´å‡å°‘åˆ°
<span class="math inline">\(k\)</span>
ç»´ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å¤§éƒ¨åˆ†å¯å˜æ€§ã€‚</li>
</ul></li>
</ul>
<p>Example 10.2.1. A dataset of 1000 images, each with 1024 pixels, may
have most variance captured by just 50 eigenvectors of the covariance
matrix. Projecting onto these components compresses the data while
preserving essential features. ä¾‹ 10.2.1ã€‚ä¸€ä¸ªåŒ…å« 1000
å¹…å›¾åƒçš„æ•°æ®é›†ï¼Œæ¯å¹…å›¾åƒæœ‰ 1024 ä¸ªåƒç´ ï¼Œå…¶å¤§éƒ¨åˆ†æ–¹å·®å¯èƒ½ä»…ç”±åæ–¹å·®çŸ©é˜µçš„
50 ä¸ªç‰¹å¾å‘é‡æ•è·ã€‚æŠ•å½±åˆ°è¿™äº›åˆ†é‡ä¸Šå¯ä»¥å‹ç¼©æ•°æ®ï¼ŒåŒæ—¶ä¿ç•™åŸºæœ¬ç‰¹å¾ã€‚</p>
<h3 id="least-squares">Least Squares</h3>
<p>æœ€å°äºŒä¹˜æ³•</p>
<p>Often, we have more equations than unknowns-an overdetermined system:
é€šå¸¸ï¼Œæˆ‘ä»¬çš„æ–¹ç¨‹æ¯”æœªçŸ¥æ•°è¿˜å¤šâ€”â€”ä¸€ä¸ªè¶…å®šç³»ç»Ÿï¼š</p>
<p><span class="math display">\[
A\mathbf{x} \approx \mathbf{b}, \quad A \in \mathbb{R}^{m \times n}, \ m
&gt; n.
\]</span></p>
<p>An exact solution may not exist. Instead, we seek <span
class="math inline">\(\mathbf{x}\)</span> that minimizes the error
ç²¾ç¡®è§£å¯èƒ½ä¸å­˜åœ¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯»æ±‚æœ€å°åŒ–è¯¯å·®çš„ <span
class="math inline">\(\mathbf{x}\)</span></p>
<p><span class="math display">\[
\|A\mathbf{x} - \mathbf{b}\|^2.
\]</span></p>
<p>This leads to the normal equations: è¿™å¯¼è‡´äº†æ­£è§„æ–¹ç¨‹ï¼š</p>
<p><span class="math display">\[
A^T A \mathbf{x} = A^T \mathbf{b}.
\]</span></p>
<p>The solution is the orthogonal projection of <span
class="math inline">\(\mathbf{b}\)</span> onto the column space of <span
class="math inline">\(A\)</span>. è§£å†³æ–¹æ¡ˆæ˜¯å°† <span
class="math inline">\(\mathbf{b}\)</span> æ­£äº¤æŠ•å½±åˆ° <span
class="math inline">\(A\)</span> çš„åˆ—ç©ºé—´ä¸Šã€‚</p>
<h3 id="example-10.2.2">Example 10.2.2</h3>
<p>ä¾‹ 10.2.2</p>
<p>Fit a line <span class="math inline">\(y = mx + c\)</span> to data
points <span class="math inline">\((x_i, y_i)\)</span>. å°†çº¿ <span
class="math inline">\(y = mx + c\)</span> ä¸æ•°æ®ç‚¹ <span
class="math inline">\((x_i, y_i)\)</span> æ‹Ÿåˆã€‚</p>
<p>Matrix form: çŸ©é˜µå½¢å¼ï¼š</p>
<p><span class="math display">\[
A = \begin{bmatrix}x_1 &amp; 1 \\x_2 &amp; 1 \\\vdots &amp; \vdots \\x_m
&amp; 1\end{bmatrix},\quad\mathbf{b} =\begin{bmatrix}y_1 \\y_2 \\\vdots
\\y_m \end{bmatrix},\quad\mathbf{x} =\begin{bmatrix}m \\c \end{bmatrix}.
\]</span></p>
<p>Solve <span class="math inline">\(A^T A \mathbf{x} = A^T
\mathbf{b}\)</span>. This yields the best-fit line in the least squares
sense. æ±‚è§£ <span class="math inline">\(A^T A \mathbf{x} = A^T
\mathbf{b}\)</span> ã€‚è¿™å°†å¾—å‡ºæœ€å°äºŒä¹˜æ„ä¹‰ä¸Šçš„æœ€ä½³æ‹Ÿåˆçº¿ã€‚</p>
<h3 id="geometric-interpretation-23">Geometric Interpretation</h3>
<p>å‡ ä½•è§£é‡Š</p>
<ul>
<li>Dimensionality reduction: Find the best subspace capturing most
variance. é™ç»´ï¼šæ‰¾åˆ°æ•è·æœ€å¤šæ–¹å·®çš„æœ€ä½³å­ç©ºé—´ã€‚</li>
<li>Least squares: Project the target vector onto the subspace spanned
by predictors. æœ€å°äºŒä¹˜ï¼šå°†ç›®æ ‡å‘é‡æŠ•å½±åˆ°é¢„æµ‹å˜é‡æ‰€è·¨è¶Šçš„å­ç©ºé—´ä¸Šã€‚</li>
</ul>
<p>Both are projection problems, solved using inner products and
orthogonality. ä¸¤è€…éƒ½æ˜¯æŠ•å½±é—®é¢˜ï¼Œä½¿ç”¨å†…ç§¯å’Œæ­£äº¤æ€§æ¥è§£å†³ã€‚</p>
<h3 id="why-this-matters-36">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Dimensionality reduction makes large datasets tractable, filters
noise, and reveals structure. Least squares fitting powers regression,
statistics, and machine learning. Both rely directly on eigenvalues,
eigenvectors, and projections-core tools of linear algebra.
é™ç»´ä½¿å¤§å‹æ•°æ®é›†æ›´æ˜“äºå¤„ç†ï¼Œè¿‡æ»¤å™ªå£°å¹¶æ­ç¤ºç»“æ„ã€‚æœ€å°äºŒä¹˜æ‹Ÿåˆä¸ºå›å½’ã€ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ æä¾›æ”¯æŒã€‚ä¸¤è€…éƒ½ç›´æ¥ä¾èµ–äºç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡å’ŒæŠ•å½±â€”â€”çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒå·¥å…·ã€‚</p>
<h3 id="exercises-10.2">Exercises 10.2</h3>
<p>ç»ƒä¹ 10.2</p>
<ol type="1">
<li>Explain why PCA reduces noise in datasets by discarding small
eigenvalue components. è§£é‡Šä¸ºä»€ä¹ˆ PCA
é€šè¿‡ä¸¢å¼ƒè¾ƒå°çš„ç‰¹å¾å€¼åˆ†é‡æ¥å‡å°‘æ•°æ®é›†ä¸­çš„å™ªå£°ã€‚</li>
<li>Compute the least squares solution to fitting a line through <span
class="math inline">\((0,0), (1,1), (2,2)\)</span>. è®¡ç®—é€šè¿‡ <span
class="math inline">\((0,0), (1,1), (2,2)\)</span>
æ‹Ÿåˆç›´çº¿çš„æœ€å°äºŒä¹˜è§£ã€‚</li>
<li>Show that the least squares solution is unique if and only if <span
class="math inline">\(A^T A\)</span> is invertible.
è¯æ˜æœ€å°äºŒä¹˜è§£æ˜¯å”¯ä¸€çš„å½“ä¸”ä»…å½“ <span class="math inline">\(A^T
A\)</span> å¯é€†ã€‚</li>
<li>Prove that the least squares solution minimizes the squared error by
projection arguments. è¯æ˜æœ€å°äºŒä¹˜è§£é€šè¿‡æŠ•å½±å‚æ•°æœ€å°åŒ–å¹³æ–¹è¯¯å·®ã€‚</li>
<li>Apply PCA to the data points <span class="math inline">\((1,0),
(2,1), (3,2)\)</span> and find the first principal component. å°† PCA
åº”ç”¨äºæ•°æ®ç‚¹ <span class="math inline">\((1,0), (2,1), (3,2)\)</span>
å¹¶æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ã€‚</li>
</ol>
<h2 id="networks-and-markov-chains">10.3 Networks and Markov Chains</h2>
<p>10.3 ç½‘ç»œå’Œé©¬å°”å¯å¤«é“¾</p>
<p>Graphs and networks provide a natural setting where linear algebra
comes to life. From modeling flows and connectivity to predicting
long-term behavior, matrices translate network structure into algebraic
form. Markov chains, already introduced in Section 8.4, are a central
example of networks evolving over time.
å›¾å’Œç½‘ç»œä¸ºçº¿æ€§ä»£æ•°çš„è¿ç”¨æä¾›äº†è‡ªç„¶çš„å¹³å°ã€‚ä»å»ºæ¨¡æµå’Œè¿æ¥åˆ°é¢„æµ‹é•¿æœŸè¡Œä¸ºï¼ŒçŸ©é˜µå°†ç½‘ç»œç»“æ„è½¬åŒ–ä¸ºä»£æ•°å½¢å¼ã€‚é©¬å°”å¯å¤«é“¾ï¼ˆå·²åœ¨
8.4 èŠ‚ä»‹ç»ï¼‰æ˜¯ç½‘ç»œéšæ—¶é—´æ¼”åŒ–çš„ä¸€ä¸ªå…¸å‹ä¾‹å­ã€‚</p>
<h3 id="adjacency-matrices">Adjacency Matrices</h3>
<p>é‚»æ¥çŸ©é˜µ</p>
<p>A network (graph) with <span class="math inline">\(n\)</span> nodes
can be represented by an adjacency matrix <span class="math inline">\(A
\in \mathbb{R}^{n \times n}\)</span>: å…·æœ‰ <span
class="math inline">\(n\)</span> ä¸ªèŠ‚ç‚¹çš„ç½‘ç»œï¼ˆå›¾ï¼‰å¯ä»¥ç”¨é‚»æ¥çŸ©é˜µ <span
class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> è¡¨ç¤ºï¼š</p>
<p><span class="math display">\[
A_{ij} =\begin{cases}1 &amp; \text{if there is an edge from node \(i\)
to node \(j\)} \\0 &amp; \text{otherwise.}\end{cases}
\]</span></p>
<p>For weighted graphs, entries may be positive weights instead of 0/1.
å¯¹äºåŠ æƒå›¾ï¼Œæ¡ç›®å¯èƒ½æ˜¯æ­£æƒé‡è€Œä¸æ˜¯ 0/1 ã€‚</p>
<ul>
<li>The number of walks of length <span class="math inline">\(k\)</span>
from node <span class="math inline">\(i\)</span> to node <span
class="math inline">\(j\)</span> is given by the entry <span
class="math inline">\((A^k)_{ij}\)</span>. ä»èŠ‚ç‚¹ <span
class="math inline">\(i\)</span> åˆ°èŠ‚ç‚¹ <span
class="math inline">\(j\)</span> çš„é•¿åº¦ä¸º <span
class="math inline">\(k\)</span> çš„æ­¥è¡Œæ¬¡æ•°ç”±æ¡ç›® ( ğ´ ğ‘˜ ) ğ‘– ğ‘— ï¼ˆä¸€ä¸ª k )
ä¼Šå¥‡ â€‹ .</li>
<li>Powers of adjacency matrices thus encode connectivity over time.
å› æ­¤ï¼Œé‚»æ¥çŸ©é˜µçš„å¹‚å¯ä»¥å¯¹éšæ—¶é—´å˜åŒ–çš„è¿é€šæ€§è¿›è¡Œç¼–ç ã€‚</li>
</ul>
<h3 id="laplacian-matrices">Laplacian Matrices</h3>
<p>æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ</p>
<p>Another important matrix is the graph Laplacian:
å¦ä¸€ä¸ªé‡è¦çš„çŸ©é˜µæ˜¯å›¾æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼š</p>
<p><span class="math display">\[
L = D - A,
\]</span></p>
<p>where <span class="math inline">\(D\)</span> is the diagonal degree
matrix ( <span class="math inline">\(D_{ii} = \text{degree}(i)\)</span>
). å…¶ä¸­ <span class="math inline">\(D\)</span> æ˜¯å¯¹è§’åº¦çŸ©é˜µ ( <span
class="math inline">\(D_{ii} = \text{degree}(i)\)</span> )ã€‚</p>
<ul>
<li><span class="math inline">\(L\)</span> is symmetric and positive
semidefinite. <span class="math inline">\(L\)</span>
æ˜¯å¯¹ç§°çš„å¹¶ä¸”æ˜¯æ­£åŠå®šçš„ã€‚</li>
<li>The smallest eigenvalue is always <span
class="math inline">\(0\)</span>, with eigenvector <span
class="math inline">\((1,1,\\dots,1)\)</span>. æœ€å°ç‰¹å¾å€¼å§‹ç»ˆæ˜¯ $0 $,
with eigenvector $ (1,1,\dots,1)$ã€‚</li>
<li>The multiplicity of eigenvalue 0 equals the number of connected
components in the graph. ç‰¹å¾å€¼ 0 çš„å¤šé‡æ€§ç­‰äºå›¾ä¸­è¿é€šåˆ†é‡çš„æ•°é‡ã€‚</li>
</ul>
<p>This connection between eigenvalues and connectivity forms the basis
of spectral graph theory.
ç‰¹å¾å€¼å’Œè¿é€šæ€§ä¹‹é—´çš„è¿™ç§è”ç³»æ„æˆäº†è°±å›¾ç†è®ºçš„åŸºç¡€ã€‚</p>
<h3 id="markov-chains-on-graphs">Markov Chains on Graphs</h3>
<p>å›¾ä¸Šçš„é©¬å°”å¯å¤«é“¾</p>
<p>A Markov chain can be viewed as a random walk on a graph. If <span
class="math inline">\(P\)</span> is the transition matrix where <span
class="math inline">\(P_{ij}\)</span> is the probability of moving from
node <span class="math inline">\(i\)</span> to node <span
class="math inline">\(j\)</span>, then
é©¬å°”å¯å¤«é“¾å¯ä»¥çœ‹ä½œå›¾ä¸Šçš„éšæœºæ¸¸åŠ¨ã€‚è®¾ <span
class="math inline">\(P\)</span> ä¸ºè½¬ç§»çŸ©é˜µï¼Œå…¶ä¸­ ğ‘ƒ ğ‘– ğ‘— P ä¼Šå¥‡ â€‹ æ˜¯ä»èŠ‚ç‚¹
<span class="math inline">\(i\)</span> ç§»åŠ¨åˆ°èŠ‚ç‚¹ <span
class="math inline">\(j\)</span> çš„æ¦‚ç‡ï¼Œé‚£ä¹ˆ</p>
<p><span class="math display">\[
\mathbf{x}_{k+1} = P \mathbf{x}_k
\]</span></p>
<p>describes the distribution of positions after <span
class="math inline">\(k\)</span> steps. æè¿° <span
class="math inline">\(k\)</span> æ­¥ä¹‹åçš„ä½ç½®åˆ†å¸ƒã€‚</p>
<ul>
<li>The steady-state distribution is given by the eigenvector of <span
class="math inline">\(P\)</span> with eigenvalue 1. ç¨³æ€åˆ†å¸ƒç”±ç‰¹å¾å‘é‡
<span class="math inline">\(P\)</span> ç»™å‡ºï¼Œç‰¹å¾å€¼ä¸º 1 ã€‚</li>
<li>The speed of convergence depends on the gap between the largest
eigenvalue (which is always 1) and the second largest eigenvalue.
æ”¶æ•›é€Ÿåº¦å–å†³äºæœ€å¤§ç‰¹å¾å€¼ï¼ˆå§‹ç»ˆä¸º 1 ï¼‰ä¸ç¬¬äºŒå¤§ç‰¹å¾å€¼ä¹‹é—´çš„å·®è·ã€‚</li>
</ul>
<h3 id="example-10.3.1">Example 10.3.1</h3>
<p>ä¾‹ 10.3.1</p>
<p>Consider a simple 3-node cycle graph: è€ƒè™‘ä¸€ä¸ªç®€å•çš„ 3
èŠ‚ç‚¹å¾ªç¯å›¾ï¼š</p>
<p><span class="math display">\[
P = \begin{bmatrix}0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 \\1 &amp; 0
&amp; 0\end{bmatrix}.
\]</span></p>
<p>This Markov chain cycles deterministically among the nodes.
Eigenvalues are the cube roots of unity: <span class="math inline">\(1,
e^{2\\pi i/3}, e^{4\\pi i/3}. The eigenvalue \\1\)</span>corresponds to
the steady state, which is the uniformdistribution<span
class="math inline">\((1/3,1/3,1/3)\)</span>.
è¿™ä¸ªé©¬å°”å¯å¤«é“¾åœ¨èŠ‚ç‚¹ä¹‹é—´ç¡®å®šæ€§åœ°å¾ªç¯ã€‚ç‰¹å¾å€¼æ˜¯ ç»Ÿä¸€ï¼š$1,e^{2\pi
i/3},e^{4\pi i/3} ã€‚ç‰¹å¾å€¼ \ 1 <span class="math inline">\(corresponds
to the steady state, which is the uniformdistribution\)</span>
(1/3,1/3,1/3)$ã€‚</p>
<h3 id="applications-2">Applications</h3>
<p>åº”ç”¨</p>
<ul>
<li>Search engines: Googleâ€™s PageRank algorithm models the web as a
Markov chain, where steady-state probabilities rank pages.
æœç´¢å¼•æ“ï¼šGoogle çš„ PageRank
ç®—æ³•å°†ç½‘ç»œå»ºæ¨¡ä¸ºé©¬å°”å¯å¤«é“¾ï¼Œå…¶ä¸­ç¨³æ€æ¦‚ç‡å¯¹ç½‘é¡µè¿›è¡Œæ’åã€‚</li>
<li>Network analysis: Eigenvalues of adjacency or Laplacian matrices
reveal communities, bottlenecks, and robustness.
ç½‘ç»œåˆ†æï¼šé‚»æ¥çŸ©é˜µæˆ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å€¼æ­ç¤ºç¤¾åŒºã€ç“¶é¢ˆå’Œç¨³å¥æ€§ã€‚</li>
<li>Epidemiology and information flow: Random walks model how diseases
or ideas spread through networks.
æµè¡Œç—…å­¦å’Œä¿¡æ¯æµï¼šéšæœºæ¸¸åŠ¨æ¨¡æ‹Ÿç–¾ç—…æˆ–æ€æƒ³å¦‚ä½•é€šè¿‡ç½‘ç»œä¼ æ’­ã€‚</li>
</ul>
<h3 id="why-this-matters-37">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Linear algebra transforms network problems into matrix problems.
Eigenvalues and eigenvectors reveal connectivity, flow, stability, and
long-term dynamics. Networks are everywhere-social media, biology,
finance, and the internet-so these tools are indispensable.
çº¿æ€§ä»£æ•°å°†ç½‘ç»œé—®é¢˜è½¬åŒ–ä¸ºçŸ©é˜µé—®é¢˜ã€‚ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ­ç¤ºäº†è¿é€šæ€§ã€æµåŠ¨ã€ç¨³å®šæ€§å’Œé•¿æœŸåŠ¨æ€ã€‚ç½‘ç»œæ— å¤„ä¸åœ¨â€”â€”ç¤¾äº¤åª’ä½“ã€ç”Ÿç‰©ã€é‡‘èå’Œäº’è”ç½‘â€”â€”å› æ­¤è¿™äº›å·¥å…·ä¸å¯æˆ–ç¼ºã€‚</p>
<h3 id="exercises-10.3">Exercises 10.3</h3>
<p>ç»ƒä¹ 10.3</p>
<ol type="1">
<li><p>Write the adjacency matrix of a square graph with 4 nodes.
Compute <span class="math inline">\(A^2\)</span> and interpret the
entries. å†™å‡ºä¸€ä¸ªæœ‰ 4 ä¸ªèŠ‚ç‚¹çš„æ­£æ–¹å½¢å›¾çš„é‚»æ¥çŸ©é˜µã€‚è®¡ç®— <span
class="math inline">\(A^2\)</span> å¹¶è§£é‡Šå…¶ä¸­çš„å…ƒç´ ã€‚</p></li>
<li><p>Show that the Laplacian of a connected graph has exactly one zero
eigenvalue. è¯æ˜è¿é€šå›¾çš„æ‹‰æ™®æ‹‰æ–¯ç®—å­æ°å¥½æœ‰ä¸€ä¸ªé›¶ç‰¹å¾å€¼ã€‚</p></li>
<li><p>Find the steady-state distribution of the Markov chain with
æ‰¾åˆ°é©¬å°”å¯å¤«é“¾çš„ç¨³æ€åˆ†å¸ƒ</p>
<p><span class="math display">\[
P = \begin{bmatrix} 0.5 &amp; 0.5 \\ 0.4 &amp; 0.6 \end{bmatrix}.
\]</span></p></li>
<li><p>Explain how eigenvalues of the Laplacian can detect disconnected
components of a graph.
è§£é‡Šæ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å€¼å¦‚ä½•æ£€æµ‹å›¾ä¸­ä¸è¿ç»­çš„ç»„æˆéƒ¨åˆ†ã€‚</p></li>
<li><p>Describe how PageRank modifies the transition matrix of the web
graph to ensure a unique steady-state distribution. æè¿° PageRank
å¦‚ä½•ä¿®æ”¹ç½‘ç»œå›¾çš„è½¬æ¢çŸ©é˜µä»¥ç¡®ä¿å”¯ä¸€çš„ç¨³æ€åˆ†å¸ƒã€‚</p></li>
</ol>
<h2 id="machine-learning-connections">10.4 Machine Learning
Connections</h2>
<p>10.4 æœºå™¨å­¦ä¹ è¿æ¥</p>
<p>Modern machine learning is built on linear algebra. From the
representation of data as matrices to the optimization of large-scale
models, nearly every step relies on concepts such as vector spaces,
projections, eigenvalues, and matrix decompositions.
ç°ä»£æœºå™¨å­¦ä¹ å»ºç«‹åœ¨çº¿æ€§ä»£æ•°çš„åŸºç¡€ä¸Šã€‚ä»æ•°æ®çŸ©é˜µè¡¨ç¤ºåˆ°å¤§è§„æ¨¡æ¨¡å‹çš„ä¼˜åŒ–ï¼Œå‡ ä¹æ¯ä¸€æ­¥éƒ½ä¾èµ–äºå‘é‡ç©ºé—´ã€æŠ•å½±ã€ç‰¹å¾å€¼å’ŒçŸ©é˜µåˆ†è§£ç­‰æ¦‚å¿µã€‚</p>
<h3 id="data-as-matrices">Data as Matrices</h3>
<p>æ•°æ®ä½œä¸ºçŸ©é˜µ</p>
<p>A dataset with <span class="math inline">\(m\)</span> examples and
<span class="math inline">\(n\)</span> features is represented as a
matrix <span class="math inline">\(X \in \mathbb{R}^{m \times
n}\)</span>: å…·æœ‰ <span class="math inline">\(m\)</span> ä¸ªç¤ºä¾‹å’Œ <span
class="math inline">\(n\)</span> ä¸ªç‰¹å¾çš„æ•°æ®é›†è¡¨ç¤ºä¸ºçŸ©é˜µ <span
class="math inline">\(X \in \mathbb{R}^{m \times n}\)</span> ï¼š</p>
<p><span class="math display">\[
X = \begin{bmatrix}
\text{-} &amp; \mathbf{x}_1^T &amp; \text{-} \\
\text{-} &amp; \mathbf{x}_2^T &amp; \text{-} \\
  &amp; \vdots &amp;   \\
\text{-} &amp; \mathbf{x}_m^T &amp; \text{-}
\end{bmatrix},
\]</span></p>
<p>where each row <span class="math inline">\(\mathbf{x}_i \in
\mathbb{R}^n\)</span> is a feature vector. Linear algebra provides tools
to analyze, compress, and transform this data. å…¶ä¸­æ¯è¡Œ <span
class="math inline">\(\mathbf{x}_i \in \mathbb{R}^n\)</span>
æ˜¯ä¸€ä¸ªç‰¹å¾å‘é‡ã€‚çº¿æ€§ä»£æ•°æä¾›äº†åˆ†æã€å‹ç¼©å’Œè½¬æ¢æ­¤ç±»æ•°æ®çš„å·¥å…·ã€‚</p>
<h3 id="linear-models">Linear Models</h3>
<p>çº¿æ€§æ¨¡å‹</p>
<p>At the heart of machine learning are linear predictors:
æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ˜¯çº¿æ€§é¢„æµ‹å™¨ï¼š</p>
<p><span class="math inline">\(\hat{y} = X\mathbf{w},\)</span></p>
<p>where <span class="math inline">\(\mathbf{w}\)</span> is the weight
vector. Training often involves solving a least squares problem or a
regularized variant such as ridge regression: å…¶ä¸­ <span
class="math inline">\(\mathbf{w}\)</span>
æ˜¯æƒé‡å‘é‡ã€‚è®­ç»ƒé€šå¸¸æ¶‰åŠæ±‚è§£æœ€å°äºŒä¹˜é—®é¢˜æˆ–æ­£åˆ™åŒ–å˜ä½“ï¼Œä¾‹å¦‚å²­å›å½’ï¼š</p>
<p><span class="math inline">\(\min_{\mathbf{w}} \|X\mathbf{w} -
\mathbf{y}\|^2 + \lambda \|\mathbf{w}\|^2.\)</span></p>
<p>This is solved efficiently using matrix factorizations.
ä½¿ç”¨çŸ©é˜µåˆ†è§£å¯ä»¥æœ‰æ•ˆåœ°è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p>
<h3 id="singular-value-decomposition-svd">Singular Value Decomposition
(SVD)</h3>
<p>å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰</p>
<p>The SVD of a matrix <span class="math inline">\(X\)</span> is çŸ©é˜µ
<span class="math inline">\(X\)</span> çš„ SVD ä¸º</p>
<p><span class="math inline">\(X = U \Sigma V^T,\)</span></p>
<p>where <span class="math inline">\(U, V\)</span> are orthogonal and
<span class="math inline">\(\Sigma\)</span> is diagonal with nonnegative
entries (singular values). å…¶ä¸­ <span class="math inline">\(U,
V\)</span> æ˜¯æ­£äº¤çš„ï¼Œ <span class="math inline">\(\Sigma\)</span>
æ˜¯å¯¹è§’çš„ï¼Œå…·æœ‰éè´Ÿé¡¹ï¼ˆå¥‡å¼‚å€¼ï¼‰ã€‚</p>
<ul>
<li>Singular values measure the importance of directions in feature
space. å¥‡å¼‚å€¼è¡¡é‡ç‰¹å¾ç©ºé—´ä¸­æ–¹å‘çš„é‡è¦æ€§ã€‚</li>
<li>SVD is used for dimensionality reduction (low-rank approximations),
topic modeling, and recommender systems. SVD
ç”¨äºé™ç»´ï¼ˆä½ç§©è¿‘ä¼¼ï¼‰ã€ä¸»é¢˜å»ºæ¨¡å’Œæ¨èç³»ç»Ÿã€‚</li>
</ul>
<h3 id="eigenvalues-in-machine-learning">Eigenvalues in Machine
Learning</h3>
<p>æœºå™¨å­¦ä¹ ä¸­çš„ç‰¹å¾å€¼</p>
<ul>
<li>PCA (Principal Component Analysis): diagonalization of the
covariance matrix identifies directions of maximal variance.
PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ï¼šåæ–¹å·®çŸ©é˜µçš„å¯¹è§’åŒ–ç¡®å®šäº†æœ€å¤§æ–¹å·®çš„æ–¹å‘ã€‚</li>
<li>Spectral clustering: uses eigenvectors of the Laplacian to group
data points into clusters.
è°±èšç±»ï¼šä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å‘é‡å°†æ•°æ®ç‚¹åˆ†ç»„æˆèšç±»ã€‚</li>
<li>Stability analysis: eigenvalues of Hessian matrices determine
whether optimization converges to a minimum. ç¨³å®šæ€§åˆ†æï¼šHessian
çŸ©é˜µçš„ç‰¹å¾å€¼å†³å®šä¼˜åŒ–æ˜¯å¦æ”¶æ•›åˆ°æœ€å°å€¼ã€‚</li>
</ul>
<h3 id="neural-networks">Neural Networks</h3>
<p>ç¥ç»ç½‘ç»œ</p>
<p>Even deep learning, though nonlinear, uses linear algebra at its
core: å³ä½¿æ˜¯æ·±åº¦å­¦ä¹ ï¼Œå°½ç®¡æ˜¯éçº¿æ€§çš„ï¼Œå…¶æ ¸å¿ƒä¹Ÿä½¿ç”¨çº¿æ€§ä»£æ•°ï¼š</p>
<ul>
<li>Each layer is a matrix multiplication followed by a nonlinear
activation. æ¯ä¸€å±‚éƒ½æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œç„¶åæ˜¯éçº¿æ€§æ¿€æ´»ã€‚</li>
<li>Training requires computing gradients, which are expressed in terms
of matrix calculus. è®­ç»ƒéœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œä»¥çŸ©é˜µå¾®ç§¯åˆ†æ¥è¡¨ç¤ºã€‚</li>
<li>Backpropagation is essentially repeated applications of the chain
rule with linear algebra.
åå‘ä¼ æ’­æœ¬è´¨ä¸Šæ˜¯é“¾å¼æ³•åˆ™ä¸çº¿æ€§ä»£æ•°çš„é‡å¤åº”ç”¨ã€‚</li>
</ul>
<h3 id="why-this-matters-38">Why this matters</h3>
<p>ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦</p>
<p>Machine learning models often involve datasets with millions of
features and parameters. Linear algebra provides the algorithms and
abstractions that make training and inference possible. Without it,
large-scale computation in AI would be intractable.
æœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸æ¶‰åŠå…·æœ‰æ•°ç™¾ä¸‡ä¸ªç‰¹å¾å’Œå‚æ•°çš„æ•°æ®é›†ã€‚çº¿æ€§ä»£æ•°æä¾›äº†ä½¿è®­ç»ƒå’Œæ¨ç†æˆä¸ºå¯èƒ½çš„ç®—æ³•å’ŒæŠ½è±¡ã€‚å¦‚æœæ²¡æœ‰å®ƒï¼Œäººå·¥æ™ºèƒ½ä¸­çš„å¤§è§„æ¨¡è®¡ç®—å°†å˜å¾—éš¾ä»¥å¤„ç†ã€‚</p>
<h3 id="exercises-10.4">Exercises 10.4</h3>
<p>ç»ƒä¹ 10.4</p>
<ol type="1">
<li>Show that ridge regression leads to the normal equations
è¯æ˜å²­å›å½’å¯ä»¥å¾—å‡ºæ­£æ€æ–¹ç¨‹</li>
</ol>
<p><span class="math display">\[
(X^T X + \lambda I)\mathbf{w} = X^T \mathbf{y}.
\]</span></p>
<ol start="2" type="1">
<li><p>Explain how SVD can be used to compress an image represented as a
matrix of pixel intensities. è§£é‡Šå¦‚ä½•ä½¿ç”¨ SVD
æ¥å‹ç¼©ä»¥åƒç´ å¼ºåº¦çŸ©é˜µè¡¨ç¤ºçš„å›¾åƒã€‚</p></li>
<li><p>For a covariance matrix <span
class="math inline">\(\Sigma\)</span>, show why its eigenvalues
represent variances along principal components. å¯¹äºåæ–¹å·®çŸ©é˜µ <span
class="math inline">\(\Sigma\)</span>
ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆå®ƒçš„ç‰¹å¾å€¼è¡¨ç¤ºæ²¿ä¸»æˆåˆ†çš„æ–¹å·®ã€‚</p></li>
<li><p>Give an example of how eigenvectors of the Laplacian matrix can
be used for clustering a small graph.
ä¸¾ä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å‘é‡å¯¹å°å›¾è¿›è¡Œèšç±»ã€‚</p></li>
<li><p>In a neural network with one hidden layer, write the forward pass
in matrix form.
åœ¨å…·æœ‰ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œä¸­ï¼Œä»¥çŸ©é˜µå½¢å¼å†™å‡ºå‰å‘ä¼ é€’ã€‚</p></li>
</ol>
</article>
</body>
</html>