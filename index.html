<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>The Little Book of Linear Algebra</title>
  <link rel="stylesheet" href="css/github-markdown.css">
<link rel="stylesheet" href="libs//katex/katex.min.css">
<script defer src="libs//katex/katex.min.js"></script>
<script defer src="libs//katex/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>

</head>
<body>
<article class="markdown-body">
<p>https://github.com/little-book-of/linear-algebra</p>
<h1 id="the-little-book-of-linear-algebra">The Little Book of Linear
Algebra</h1>
<p>线性代数小册子</p>
<p>A concise, beginner-friendly introduction to the core ideas of linear
algebra. 简洁、适合初学者的线性代数核心思想介绍。</p>
<h1 id="chapter-1.-vectors">Chapter 1. Vectors</h1>
<p>第 1 章 向量</p>
<h2 id="scalars-and-vectors">1.1 Scalars and Vectors</h2>
<p>1.1 标量和矢量</p>
<p>A scalar is a single numerical quantity, most often taken from the
real numbers, denoted by <span
class="math inline">\(\mathbb{R}\)</span>. Scalars are the fundamental
building blocks of arithmetic: they can be added, subtracted,
multiplied, and, except in the case of zero, divided. In linear algebra,
scalars play the role of coefficients, scaling factors, and entries of
larger structures such as vectors and matrices. They provide the weights
by which more complex objects are measured and combined. A vector is an
ordered collection of scalars, arranged either in a row or a column.
When the scalars are real numbers, the vector is said to belong to
<em>real</em> <span class="math inline">\(n\)</span>-dimensional space,
written 标量是一个单一的数值，通常取自实数，用 <span
class="math inline">\(\mathbb{R}\)</span>
表示。标量是算术的基本组成部分：它们可以进行加、减、乘和除（零除外）。在线性代数中，标量充当系数、比例因子以及向量和矩阵等更大结构中的元素。它们提供权重，用于测量和组合更复杂的对象。向量是按行或列排列的标量的有序集合。当标量为实数时，该向量被称为属于<em>实</em>
<span class="math inline">\(n\)</span> 维空间，写为</p>
<p><span class="math display">\[
\mathbb{R}^n = \{ (x_1, x_2, \dots, x_n) \mid x_i \in \mathbb{R} \}.
\]</span></p>
<p>An element of <span class="math inline">\(\mathbb{R}^n\)</span> is
called a vector of dimension <span class="math inline">\(n\)</span> or
an <em>n</em>-vector. The number <span class="math inline">\(n\)</span>
is called the dimension of the vector space. Thus <span
class="math inline">\(\mathbb{R}^2\)</span> is the space of all ordered
pairs of real numbers, <span class="math inline">\(\mathbb{R}^3\)</span>
the space of all ordered triples, and so on. <span
class="math inline">\(\mathbb{R}^n\)</span> 中的一个元素称为维度为 <span
class="math inline">\(n\)</span> 的向量或 <em>n</em> 向量。数字 <span
class="math inline">\(n\)</span> 称为向量空间的维数。因此， <span
class="math inline">\(\mathbb{R}^2\)</span> 是所有有序实数对的空间，
<span class="math inline">\(\mathbb{R}^3\)</span>
是所有有序三元组的空间，等等。</p>
<p>Example 1.1.1. 例 1.1.1。</p>
<ul>
<li>A 2-dimensional vector: <span class="math inline">\((3, -1) \in
\mathbb{R}^2\)</span>. 二维向量： <span class="math inline">\((3, -1)
\in \mathbb{R}^2\)</span> 。</li>
<li>A 3-dimensional vector: <span class="math inline">\((2, 0, 5) \in
\mathbb{R}^3\)</span>. 三维向量： <span class="math inline">\((2, 0, 5)
\in \mathbb{R}^3\)</span> 。</li>
<li>A 1-dimensional vector: <span class="math inline">\((7) \in
\mathbb{R}^1\)</span>, which corresponds to the scalar 7 itself.
一维向量： <span class="math inline">\((7) \in \mathbb{R}^1\)</span>
，对应于标量 7 本身。</li>
</ul>
<p>Vectors are often written vertically in column form, which emphasizes
their role in matrix multiplication:
向量通常以列的形式垂直书写，这强调了它们在矩阵乘法中的作用：</p>
<p><span class="math display">\[
\mathbf{v} = \begin{bmatrix}2 \\0 \\5 \end{bmatrix} \in \mathbb{R}^3.
\]</span></p>
<p>The vertical layout makes the structure clearer when we consider
linear combinations or multiply matrices by vectors.
当我们考虑线性组合或矩阵乘以向量时，垂直布局使结构更加清晰。</p>
<h3 id="geometric-interpretation">Geometric Interpretation</h3>
<p>几何解释</p>
<p>In <span class="math inline">\(\mathbb{R}^2\)</span>, a vector <span
class="math inline">\((x_1, x_2)\)</span> can be visualized as an arrow
starting at the origin <span class="math inline">\((0,0)\)</span> and
ending at the point <span class="math inline">\((x_1, x_2)\)</span>. Its
length corresponds to the distance from the origin, and its orientation
gives a direction in the plane. In <span
class="math inline">\(\mathbb{R}^3\)</span>, the same picture extends
into three dimensions: a vector is an arrow from the origin to <span
class="math inline">\((x_1, x_2, x_3)\)</span>. Beyond three dimensions,
direct visualization is no longer possible, but the algebraic rules of
vectors remain identical. Even though we cannot draw a vector in <span
class="math inline">\(\mathbb{R}^{10}\)</span>, it behaves under
addition, scaling, and transformation exactly as a 2- or 3-dimensional
vector does. This abstract point of view is what allows linear algebra
to apply to data science, physics, and machine learning, where data
often lives in very high-dimensional spaces. Thus a vector may be
regarded in three complementary ways: 在 <span
class="math inline">\(\mathbb{R}^2\)</span> 中，向量 <span
class="math inline">\((x_1, x_2)\)</span> 可以可视化为一个从原点 <span
class="math inline">\((0,0)\)</span> 开始到点 <span
class="math inline">\((x_1, x_2)\)</span>
结束的箭头。它的长度对应于与原点的距离，其方向给出了平面内的方向。在
<span class="math inline">\(\mathbb{R}^3\)</span>
中，同样的图像延伸到三维空间：向量是一个从原点指向 <span
class="math inline">\((x_1, x_2, x_3)\)</span>
的箭头。超过三维空间后，直接可视化就不再可能，但向量的代数规则保持不变。即使我们无法在
<span class="math inline">\(\mathbb{R}^{10}\)</span>
中绘制向量，它在加法、缩放和变换下的行为与二维或三维向量完全相同。这种抽象的观点使得线性代数能够应用于数据科学、物理学和机器学习，这些领域的数据通常存在于非​​常高维的空间中。因此，向量可以从三个互补的角度来看待：</p>
<ol type="1">
<li>As a point in space, described by its coordinates.
作为空间中的一个点，由其坐标描述。</li>
<li>As a displacement or arrow, described by a direction and a length.
作为位移或箭头，由方向和长度描述。</li>
<li>As an abstract element of a vector space, whose properties follow
algebraic rules independent of geometry.
作为向量空间的抽象元素，其属性遵循与几何无关的代数规则。</li>
</ol>
<h3 id="notation">Notation</h3>
<p>符号</p>
<ul>
<li>Vectors are written in boldface lowercase letters: <span
class="math inline">\(\mathbf{v}, \mathbf{w}, \mathbf{x}\)</span>.
向量以粗体小写字母表示： <span class="math inline">\(\mathbf{v},
\mathbf{w}, \mathbf{x}\)</span> 。</li>
<li>The <em>i</em>-th entry of a vector <span
class="math inline">\(\mathbf{v}\)</span> is written <span
class="math inline">\(v_i\)</span>, where indices begin at 1. 向量 <span
class="math inline">\(\mathbf{v}\)</span> 的第 - 个元素写为 𝑣 𝑖 v i ​
，其中索引从 1 开始。</li>
<li>The set of all <em>n</em>-dimensional vectors over <span
class="math inline">\(\mathbb{R}\)</span> is denoted <span
class="math inline">\(\mathbb{R}^n\)</span>. <span
class="math inline">\(\mathbb{R}\)</span> 上的所有 <em>n</em>
维向量的集合记为 <span class="math inline">\(\mathbb{R}^n\)</span>
。</li>
<li>Column vectors will be the default form unless otherwise stated.
除非另有说明，列向量将是默认形式。</li>
</ul>
<h3 id="why-begin-here">Why begin here?</h3>
<p>为什么从这里开始？</p>
<p>Scalars and vectors form the atoms of linear algebra. Every structure
we will build-vector spaces, linear transformations, matrices,
eigenvalues-relies on the basic notions of number and ordered collection
of numbers. Once vectors are understood, we can define operations such
as addition and scalar multiplication, then generalize to subspaces,
bases, and coordinate systems. Eventually, this framework grows into the
full theory of linear algebra, with powerful applications to geometry,
computation, and data.
标量和向量构成了线性代数的原子。我们将要构建的每一个结构——向量空间、线性变换、矩阵、特征值——都依赖于数和有序数集的基本概念。一旦理解了向量，我们就可以定义诸如加法和标量乘法之类的运算，然后推广到子空间、基和坐标系。最终，这个框架将发展成为完整的线性代数理论，并在几何、计算和数据领域拥有强大的应用。</p>
<h3 id="exercises-1.1">Exercises 1.1</h3>
<p>练习 1.1</p>
<ol type="1">
<li>Write three different vectors in <span
class="math inline">\(\mathbb{R}^2\)</span> and sketch them as arrows
from the origin. Identify their coordinates explicitly. 在 <span
class="math inline">\(\mathbb{R}^2\)</span>
中写出三个不同的向量，并将它们画成从原点出发的箭头。明确指出它们的坐标。</li>
<li>Give an example of a vector in <span
class="math inline">\(\mathbb{R}^4\)</span>. Can you visualize it
directly? Explain why high-dimensional visualization is challenging.
给出 <span class="math inline">\(\mathbb{R}^4\)</span>
中一个向量的例子。你能直接将其可视化吗？解释为什么高维可视化具有挑战性。</li>
<li>Let <span class="math inline">\(\mathbf{v} = (4, -3, 2)\)</span>.
Write <span class="math inline">\(\mathbf{v}\)</span> in column form and
state <span class="math inline">\(v_1, v_2, v_3\)</span>. 令 <span
class="math inline">\(\mathbf{v} = (4, -3, 2)\)</span> 。将 <span
class="math inline">\(\mathbf{v}\)</span> 写成列形式，并说明 𝑣 1 , 𝑣 2 ,
𝑣 3 v 1 ​ ，v 2 ​ ，v 3 ​ .</li>
<li>In what sense is the set <span
class="math inline">\(\mathbb{R}^1\)</span> both a line and a vector
space? Illustrate with examples. 在什么意义上集合 <span
class="math inline">\(\mathbb{R}^1\)</span>
既是线空间又是向量空间？请举例说明。</li>
<li>Consider the vector <span class="math inline">\(\mathbf{u} =
(1,1,\dots,1) \in \mathbb{R}^n\)</span>. What is special about this
vector when <span class="math inline">\(n\)</span> is large? What might
it represent in applications? 考虑向量 <span
class="math inline">\(\mathbf{u} = (1,1,\dots,1) \in
\mathbb{R}^n\)</span> 。当 <span class="math inline">\(n\)</span>
很大时，这个向量有什么特殊之处？它在应用中可能代表什么？</li>
</ol>
<h2 id="vector-addition-and-scalar-multiplication">1.2 Vector Addition
and Scalar Multiplication</h2>
<p>1.2 向量加法和标量乘法</p>
<p>Vectors in linear algebra are not static objects; their power comes
from the operations we can perform on them. Two fundamental operations
define the structure of vector spaces: addition and scalar
multiplication. These operations satisfy simple but far-reaching rules
that underpin the entire subject.
线性代数中的向量并非静态对象；它们的力量源于我们可以对它们执行的运算。两个基本运算定义了向量空间的结构：加法和标量乘法。这两个运算满足一些简单却影响深远的规则，这些规则构成了整个线性代数学科的基础。</p>
<h3 id="vector-addition">Vector Addition</h3>
<p>向量加法</p>
<p>Given two vectors of the same dimension, their sum is obtained by
adding corresponding entries. Formally, if
给定两个相同维度的向量，它们的和可以通过添加相应的元素来获得。形式上，如果</p>
<p><span class="math display">\[
\mathbf{u} = (u_1, u_2, \dots, u_n), \quad\mathbf{v} = (v_1, v_2, \dots,
v_n),
\]</span></p>
<p>then their sum is 那么它们的总和是</p>
<p><span class="math display">\[
\mathbf{u} + \mathbf{v} = (u_1+v_1, u_2+v_2, \dots, u_n+v_n).
\]</span></p>
<p>Example 1.2.1. Let <span class="math inline">\(\mathbf{u} = (2, -1,
3)\)</span> and <span class="math inline">\(\mathbf{v} = (4, 0,
-5)\)</span>. Then 例 1.2.1。 设 <span class="math inline">\(\mathbf{u}
= (2, -1, 3)\)</span> 和 <span class="math inline">\(\mathbf{v} = (4, 0,
-5)\)</span> 。则</p>
<p><span class="math display">\[
\mathbf{u} + \mathbf{v} = (2+4, -1+0, 3+(-5)) = (6, -1, -2).
\]</span></p>
<p>Geometrically, vector addition corresponds to the <em>parallelogram
rule</em>. If we draw both vectors as arrows from the origin, then
placing the tail of one vector at the head of the other produces the
sum. The diagonal of the parallelogram they form represents the
resulting vector. 从几何学上讲，向量加法对应于<em>平行四边形法则</em>
。如果我们将两个向量都画成从原点出发的箭头，那么将一个向量的尾部放在另一个向量的头部，就能得到向量的和。它们构成的平行四边形的对角线代表最终的向量。</p>
<h3 id="scalar-multiplication">Scalar Multiplication</h3>
<p>标量乘法</p>
<p>Multiplying a vector by a scalar stretches or shrinks the vector
while preserving its direction, unless the scalar is negative, in which
case the vector is also reversed. If <span class="math inline">\(c \in
\mathbb{R}\)</span> and
将矢量乘以标量会拉伸或收缩矢量，同时保持其方向，除非标量
负数，在这种情况下向量也会反转。如果 <span class="math inline">\(c \in
\mathbb{R}\)</span> 和</p>
<p><span class="math display">\[
\mathbf{v} = (v_1, v_2, \dots, v_n),
\]</span></p>
<p>then 然后</p>
<p><span class="math display">\[
c \mathbf{v} = (c v_1, c v_2, \dots, c v_n).
\]</span></p>
<p>Example 1.2.2. Let <span class="math inline">\(\mathbf{v} = (3,
-2)\)</span> and <span class="math inline">\(c = -2\)</span>. Then 例
1.2.2。 设 <span class="math inline">\(\mathbf{v} = (3, -2)\)</span> 和
<span class="math inline">\(c = -2\)</span> 。则</p>
<p><span class="math display">\[
c\mathbf{v} = -2(3, -2) = (-6, 4).
\]</span></p>
<p>This corresponds to flipping the vector through the origin and
doubling its length. 这相当于通过原点翻转向量并使其长度加倍。</p>
<h3 id="linear-combinations">Linear Combinations</h3>
<p>线性组合</p>
<p>The interaction of addition and scalar multiplication allows us to
form <em>linear combinations</em>. A linear combination of vectors <span
class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, \dots,
\mathbf{v}_k\)</span> is any vector of the form
加法和标量乘法的相互作用使我们能够形成<em>线性组合</em>
。向量𝑣的线性组合 1 , 𝑣 2 , … , 𝑣 𝑘 v 1 ​ ，v 2 ​ ，…，v k ​
是任意形式的向量</p>
<p><span class="math display">\[
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k, \quad
c_i \in \mathbb{R}.
\]</span></p>
<p>Linear combinations are the mechanism by which we generate new
vectors from existing ones. The span of a set of vectors-the collection
of all their linear combinations-will later lead us to the idea of a
subspace.
线性组合是一种从现有向量生成新向量的机制。一组向量的跨度——它们所有线性组合的集合——稍后会引出子空间的概念。</p>
<p>Example 1.2.3. Let <span class="math inline">\(\mathbf{v}_1 =
(1,0)\)</span> and <span class="math inline">\(\mathbf{v}_2 =
(0,1)\)</span>. Then any vector <span
class="math inline">\((a,b)\in\mathbb{R}^2\)</span> can be expressed as
例 1.2.3。 设 <span class="math inline">\(\mathbf{v}_1 = (1,0)\)</span>
和 <span class="math inline">\(\mathbf{v}_2 = (0,1)\)</span>
。则任意向量 <span class="math inline">\((a,b)\in\mathbb{R}^2\)</span>
可以表示为</p>
<p><span class="math display">\[
a\mathbf{v}_1 + b\mathbf{v}_2.
\]</span></p>
<p>Thus <span class="math inline">\((1,0)\)</span> and <span
class="math inline">\((0,1)\)</span> form the basic building blocks of
the plane. 因此 <span class="math inline">\((1,0)\)</span> 和 <span
class="math inline">\((0,1)\)</span> 构成了平面的基本构造块。</p>
<h3 id="notation-1">Notation</h3>
<p>符号</p>
<ul>
<li>Addition: <span class="math inline">\(\mathbf{u} +
\mathbf{v}\)</span> means component-wise addition. 加法： <span
class="math inline">\(\mathbf{u} + \mathbf{v}\)</span>
表示逐个组件的加法。</li>
<li>Scalar multiplication: <span
class="math inline">\(c\mathbf{v}\)</span> scales each entry of <span
class="math inline">\(\mathbf{v}\)</span> by <span
class="math inline">\(c\)</span>. 标量乘法： <span
class="math inline">\(c\mathbf{v}\)</span> 将 <span
class="math inline">\(\mathbf{v}\)</span> 的每个条目乘以 <span
class="math inline">\(c\)</span> 。</li>
<li>Linear combination: a sum of the form <span
class="math inline">\(c_1 \mathbf{v}_1 + \cdots + c_k
\mathbf{v}_k\)</span>. 线性组合：𝑐 形式的和 1 𝑣 1 + ⋯ + 𝑐 𝑘 𝑣 𝑘 c 1 ​ v 1
​+⋯+c k ​ v k ​ .</li>
</ul>
<h3 id="why-this-matters">Why this matters</h3>
<p>为什么这很重要</p>
<p>Vector addition and scalar multiplication are the defining operations
of linear algebra. They give structure to vector spaces, allow us to
describe geometric phenomena like translation and scaling, and provide
the foundation for solving systems of equations. Everything that
follows-basis, dimension, transformations-builds on these simple but
profound rules.
向量加法和标量乘法是线性代数的定义运算。它们赋予向量空间结构，使我们能够描述平移和缩放等几何现象，并为方程组的求解奠定基础。之后的一切——基、维度、变换——都建立在这些简单而深刻的规则之上。</p>
<h3 id="exercises-1.2">Exercises 1.2</h3>
<p>练习 1.2</p>
<ol type="1">
<li>Compute <span class="math inline">\(\mathbf{u} + \mathbf{v}\)</span>
where <span class="math inline">\(\mathbf{u} = (1,2,3)\)</span> and
<span class="math inline">\(\mathbf{v} = (4, -1, 0)\)</span>. 计算 <span
class="math inline">\(\mathbf{u} + \mathbf{v}\)</span> ，其中 <span
class="math inline">\(\mathbf{u} = (1,2,3)\)</span> 和 <span
class="math inline">\(\mathbf{v} = (4, -1, 0)\)</span> 。</li>
<li>Find <span class="math inline">\(3\\mathbf{v}\)</span>where<span
class="math inline">\(\\mathbf{v} = (-2,5)\)</span>. Sketch both vectors
to illustrate the scaling. 求 $3\mathbf{v} <span
class="math inline">\(where\)</span> \mathbf{v} =
(-2,5)$。画出两个向量的示意图，以说明缩放关系。</li>
<li>Show that <span class="math inline">\((5,7)\)</span> can be written
as a linear combination of <span class="math inline">\((1,0)\)</span>
and <span class="math inline">\((0,1)\)</span>. 证明 <span
class="math inline">\((5,7)\)</span> 可以写成 <span
class="math inline">\((1,0)\)</span> 和 <span
class="math inline">\((0,1)\)</span> 的线性组合。</li>
<li>Write <span class="math inline">\((4,4)\)</span> as a linear
combination of <span class="math inline">\((1,1)\)</span> and <span
class="math inline">\((1,-1)\)</span>. 将 <span
class="math inline">\((4,4)\)</span> 写为 <span
class="math inline">\((1,1)\)</span> 和 <span
class="math inline">\((1,-1)\)</span> 的线性组合。</li>
<li>Prove that if <span class="math inline">\(\mathbf{u}, \mathbf{v} \in
\mathbb{R}^n\)</span>, then <span
class="math inline">\((c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} +
c\mathbf{v} + d\mathbf{u} + d\mathbf{v}\)</span> for scalars <span
class="math inline">\(c,d \in \mathbb{R}\)</span>. 证明如果 <span
class="math inline">\(\mathbf{u}, \mathbf{v} \in \mathbb{R}^n\)</span>
，则对于标量 <span class="math inline">\(c,d \in \mathbb{R}\)</span> 有
<span class="math inline">\((c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} +
c\mathbf{v} + d\mathbf{u} + d\mathbf{v}\)</span> 。</li>
</ol>
<h2 id="dot-product-norms-and-angles">1.3 Dot Product, Norms, and
Angles</h2>
<p>1.3 点积、范数和角</p>
<p>The dot product is the fundamental operation that links algebra and
geometry in vector spaces. It allows us to measure lengths, compute
angles, and determine orthogonality. From this single definition flow
the notions of <em>norm</em> and <em>angle</em>, which give geometry to
abstract vector spaces.
点积是向量空间中连接代数和几何的基本运算。它使我们能够测量长度、计算角度并确定正交性。从这个单一定义中衍生出<em>范</em>数和
<em>角度</em> ，它为抽象向量空间提供几何形状。</p>
<h3 id="the-dot-product">The Dot Product</h3>
<p>点积</p>
<p>For two vectors in <span class="math inline">\(\mathbb{R}^n\)</span>,
the dot product (also called the inner product) is defined by 对于 <span
class="math inline">\(\mathbb{R}^n\)</span>
中的两个向量，点积（也称为内积）定义为</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
\]</span></p>
<p>Equivalently, in matrix notation: 等效地，用矩阵表示法表示：</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T \mathbf{v}.
\]</span></p>
<p>Example 1.3.1. Let <span class="math inline">\(\mathbf{u} = (2, -1,
3)\)</span> and <span class="math inline">\(\mathbf{v} = (4, 0,
-2)\)</span>. Then 例 1.3.1。 设 <span class="math inline">\(\mathbf{u}
= (2, -1, 3)\)</span> 和 <span class="math inline">\(\mathbf{v} = (4, 0,
-2)\)</span> 。则</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = 2\cdot 4 + (-1)\cdot 0 + 3\cdot (-2) = 8 -
6 = 2.
\]</span></p>
<p>The dot product outputs a single scalar, not another vector.
点积输出单个标量，而不是另一个向量。</p>
<h3 id="norms-length-of-a-vector">Norms (Length of a Vector)</h3>
<p>范数（向量的长度）</p>
<p>The <em>Euclidean norm</em> of a vector is the square root of its dot
product with itself:
向量的<em>欧几里得范数</em>是其与自身的点积的平方根：</p>
<p><span class="math display">\[
\|\mathbf{v}\| = \sqrt{\mathbf{v} \cdot \mathbf{v}} = \sqrt{v_1^2 +
v_2^2 + \cdots + v_n^2}.
\]</span></p>
<p>This generalizes the Pythagorean theorem to arbitrary dimensions.
这将勾股定理推广到任意维度。</p>
<p>Example 1.3.2. For <span class="math inline">\(\mathbf{v} = (3,
4)\)</span>, 例 1.3.2。 对于 <span class="math inline">\(\mathbf{v} =
(3, 4)\)</span> ，</p>
<p><span class="math display">\[
\|\mathbf{v}\| = \sqrt{3^2 + 4^2} = \sqrt{25} = 5.
\]</span></p>
<p>This is exactly the length of the vector as an arrow in the plane.
这正是平面中箭头所指的矢量的长度。</p>
<h3 id="angles-between-vectors">Angles Between Vectors</h3>
<p>向量之间的角度</p>
<p>The dot product also encodes the angle between two vectors. For
nonzero vectors <span class="math inline">\(\mathbf{u},
\mathbf{v}\)</span>, 点积也编码了两个向量之间的角度。对于非零向量 <span
class="math inline">\(\mathbf{u}, \mathbf{v}\)</span> ，</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \, \|\mathbf{v}\| \cos
\theta,
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the angle
between them. Thus, 其中 <span class="math inline">\(\theta\)</span>
是它们之间的角度。因此，</p>
<p><span class="math display">\[
\cos \theta = \frac{\mathbf{u} \cdot
\mathbf{v}}{\|\mathbf{u}\|\|\mathbf{v}\|}.
\]</span></p>
<p>Example 1.3.3. Let <span class="math inline">\(\mathbf{u} =
(1,0)\)</span> and <span class="math inline">\(\mathbf{v} =
(0,1)\)</span>. Then 例 1.3.3。 设 <span
class="math inline">\(\mathbf{u} = (1,0)\)</span> 和 <span
class="math inline">\(\mathbf{v} = (0,1)\)</span> 。则</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = 0, \quad \|\mathbf{u}\| = 1, \quad
\|\mathbf{v}\| = 1.
\]</span></p>
<p>Hence 因此</p>
<p><span class="math display">\[
\cos \theta = \frac{0}{1\cdot 1} = 0 \quad \Rightarrow \quad \theta =
\frac{\pi}{2}.
\]</span></p>
<p>The vectors are perpendicular. 这些向量是垂直的。</p>
<h3 id="orthogonality">Orthogonality</h3>
<p>正交性</p>
<p>Two vectors are said to be orthogonal if their dot product is zero:
如果两个向量的点积为零，则称它们正交：</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = 0.
\]</span></p>
<p>Orthogonality generalizes the idea of perpendicularity from geometry
to higher dimensions. 正交性将垂直性的概念从几何学推广到更高维度。</p>
<h3 id="notation-2">Notation</h3>
<p>符号</p>
<ul>
<li>Dot product: <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v}\)</span>. 点积： <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v}\)</span> 。</li>
<li>Norm (length): <span class="math inline">\(|\mathbf{v}|\)</span>.
规范（长度）： <span class="math inline">\(|\mathbf{v}|\)</span> 。</li>
<li>Orthogonality: <span class="math inline">\(\mathbf{u} \perp
\mathbf{v}\)</span> if <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v} = 0\)</span>. 正交性：如果为 <span
class="math inline">\(\mathbf{u} \cdot \mathbf{v} = 0\)</span> ，则为
<span class="math inline">\(\mathbf{u} \perp \mathbf{v}\)</span> 。</li>
</ul>
<h3 id="why-this-matters-1">Why this matters</h3>
<p>为什么这很重要</p>
<p>The dot product turns vector spaces into geometric objects: vectors
gain lengths, angles, and notions of perpendicularity. This foundation
will later support the study of orthogonal projections, Gram–Schmidt
orthogonalization, eigenvectors, and least squares problems.
点积将向量空间转化为几何对象：向量获得长度、角度和垂直度的概念。这一基础将为后续的正交投影、格拉姆-施密特正交化、特征向量和最小二乘问题的研究奠定基础。</p>
<h3 id="exercises-1.3">Exercises 1.3</h3>
<p>练习 1.3</p>
<ol type="1">
<li>Compute <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v}\)</span> for <span class="math inline">\(\mathbf{u} =
(1,2,3)\)</span>, <span class="math inline">\(\mathbf{v} =
(4,5,6)\)</span>. 计算 <span class="math inline">\(\mathbf{u} =
(1,2,3)\)</span> 、 <span class="math inline">\(\mathbf{v} =
(4,5,6)\)</span> 的 <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v}\)</span> 。</li>
<li>Find the norm of <span class="math inline">\(\mathbf{v} = (2, -2,
1)\)</span>. 求出 <span class="math inline">\(\mathbf{v} = (2, -2,
1)\)</span> 的范数。</li>
<li>Determine whether <span class="math inline">\(\mathbf{u} =
(1,1,0)\)</span> and <span class="math inline">\(\mathbf{v} =
(1,-1,2)\)</span> are orthogonal. 确定 <span
class="math inline">\(\mathbf{u} = (1,1,0)\)</span> 和 <span
class="math inline">\(\mathbf{v} = (1,-1,2)\)</span> 是否正交。</li>
<li>Let <span class="math inline">\(\mathbf{u} = (3,4)\)</span>, <span
class="math inline">\(\mathbf{v} = (4,3)\)</span>. Compute the angle
between them. 令 <span class="math inline">\(\mathbf{u} = (3,4)\)</span>
, <span class="math inline">\(\mathbf{v} = (4,3)\)</span>
。计算它们之间的角度。</li>
<li>Prove that <span class="math inline">\(|\mathbf{u} + \mathbf{v}|^2 =
|\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}\)</span>.
This identity is the algebraic version of the Law of Cosines. 证明 <span
class="math inline">\(|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 +
|\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}\)</span>
。这个恒等式是余弦定理的代数形式。</li>
</ol>
<h2 id="orthogonality-1">1.4 Orthogonality</h2>
<p>1.4 正交性</p>
<p>Orthogonality captures the notion of perpendicularity in vector
spaces. It is one of the most important geometric ideas in linear
algebra, allowing us to decompose vectors, define projections, and
construct special bases with elegant properties.
正交性捕捉了向量空间中垂直性的概念。它是线性代数中最重要的几何概念之一，它使我们能够分解向量、定义投影，并构造具有优雅性质的特殊基。</p>
<h3 id="definition">Definition</h3>
<p>定义</p>
<p>Two vectors <span class="math inline">\(\mathbf{u}, \mathbf{v} \in
\mathbb{R}^n\)</span> are said to be orthogonal if their dot product is
zero: 如果两个向量 <span class="math inline">\(\mathbf{u}, \mathbf{v}
\in \mathbb{R}^n\)</span> 的点积为零，则称它们正交：</p>
<p><span class="math display">\[
\mathbf{u} \cdot \mathbf{v} = 0.
\]</span></p>
<p>This condition ensures that the angle between them is <span
class="math inline">\(\pi/2\)</span> radians (90 degrees).
此条件确保它们之间的角度为 <span class="math inline">\(\pi/2\)</span>
弧度（90 度）。</p>
<p>Example 1.4.1. In <span class="math inline">\(\mathbb{R}^2\)</span>,
the vectors <span class="math inline">\((1,2)\)</span> and <span
class="math inline">\((2,-1)\)</span> are orthogonal since 例 1.4.1。 在
<span class="math inline">\(\mathbb{R}^2\)</span> 中，向量 <span
class="math inline">\((1,2)\)</span> 和 <span
class="math inline">\((2,-1)\)</span> 是正交的，因为</p>
<p><span class="math display">\[
(1,2) \cdot (2,-1) = 1\cdot 2 + 2\cdot (-1) = 0.
\]</span></p>
<h3 id="orthogonal-sets">Orthogonal Sets</h3>
<p>正交集</p>
<p>A collection of vectors is called orthogonal if every distinct pair
of vectors in the set is orthogonal. If, in addition, each vector has
norm 1, the set is called orthonormal.
如果一组向量中每对不同的向量都是正交的，则称该集合为正交向量。此外，如果每个向量的范数均为
1，则该集合称为标准正交向量集。</p>
<p>Example 1.4.2. In <span class="math inline">\(\mathbb{R}^3\)</span>,
the standard basis vectors 例 1.4.2。 在 <span
class="math inline">\(\mathbb{R}^3\)</span> 中，标准基向量</p>
<p><span class="math display">\[
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3
= (0,0,1)
\]</span></p>
<p>form an orthonormal set: each has length 1, and their dot products
vanish when the indices differ. 形成一个正交集：每个集的长度为
1，并且当索引不同时，它们的点积消失。</p>
<h3 id="projections">Projections</h3>
<p>预测</p>
<p>Orthogonality makes possible the decomposition of a vector into two
components: one parallel to another vector, and one orthogonal to it.
Given a nonzero vector <span class="math inline">\(\mathbf{u}\)</span>
and any vector <span class="math inline">\(\mathbf{v}\)</span>, the
projection of <span class="math inline">\(\mathbf{v}\)</span> onto <span
class="math inline">\(\mathbf{u}\)</span> is
正交性使得将一个向量分解为两个分量成为可能：一个与另一个向量平行，另一个
与其正交。给定一个非零向量 <span
class="math inline">\(\mathbf{u}\)</span> 和任意向量 <span
class="math inline">\(\mathbf{v}\)</span> ，则 <span
class="math inline">\(\mathbf{v}\)</span> 的投影 到 <span
class="math inline">\(\mathbf{u}\)</span> 是</p>
<p><span class="math display">\[
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\mathbf{u} \cdot
\mathbf{v}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}.
\]</span></p>
<p>The difference 区别</p>
<p><span class="math display">\[
\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})
\]</span></p>
<p>is orthogonal to <span class="math inline">\(\mathbf{u}\)</span>.
Thus every vector can be decomposed uniquely into a parallel and
perpendicular part with respect to another vector. 与 <span
class="math inline">\(\mathbf{u}\)</span>
正交。因此，每个向量都可以唯一地分解为相对于另一个向量平行和垂直的部分。</p>
<p>Example 1.4.3. Let <span class="math inline">\(\mathbf{u} =
(1,0)\)</span>, <span class="math inline">\(\mathbf{v} = (2,3)\)</span>.
Then 例 1.4.3。 令 <span class="math inline">\(\mathbf{u} =
(1,0)\)</span> ， <span class="math inline">\(\mathbf{v} =
(2,3)\)</span> 。然后</p>
<p><span class="math display">\[
\text{proj}_{\mathbf{u}}(\mathbf{v}) =
\frac{(1,0)\cdot(2,3)}{(1,0)\cdot(1,0)} (1,0)= \frac{2}{1}(1,0) = (2,0).
\]</span></p>
<p>Thus 因此</p>
<p><span class="math display">\[
\mathbf{v} = (2,3) = (2,0) + (0,3),
\]</span></p>
<p>where <span class="math inline">\((2,0)\)</span> is parallel to <span
class="math inline">\((1,0)\)</span> and <span
class="math inline">\((0,3)\)</span> is orthogonal to it. 其中 <span
class="math inline">\((2,0)\)</span> 与 <span
class="math inline">\((1,0)\)</span> 平行， <span
class="math inline">\((0,3)\)</span> 与 <span
class="math inline">\((1,0)\)</span> 正交。</p>
<h3 id="orthogonal-decomposition">Orthogonal Decomposition</h3>
<p>正交分解</p>
<p>In general, if <span class="math inline">\(\mathbf{u} \neq
\mathbf{0}\)</span> and <span class="math inline">\(\mathbf{v} \in
\mathbb{R}^n\)</span>, then 一般来说，如果 <span
class="math inline">\(\mathbf{u} \neq \mathbf{0}\)</span> 和 <span
class="math inline">\(\mathbf{v} \in \mathbb{R}^n\)</span> ，那么</p>
<p><span class="math display">\[
\mathbf{v} = \text{proj}\_{\mathbf{u}}(\mathbf{v}) + \big(\mathbf{v} -
\text{proj}\_{\mathbf{u}}(\mathbf{v})\big),
\]</span></p>
<p>where the first term is parallel to <span
class="math inline">\(\mathbf{u}\)</span> and the second term is
orthogonal. This decomposition underlies methods such as least squares
approximation and the Gram–Schmidt process. 其中第一项平行于 <span
class="math inline">\(\mathbf{u}\)</span>
，第二项正交。这种分解是最小二乘近似和格拉姆-施密特过程等方法的基础。</p>
<h3 id="notation-3">Notation</h3>
<p>符号</p>
<ul>
<li><span class="math inline">\(\mathbf{u} \perp \mathbf{v}\)</span>:
vectors <span class="math inline">\(\mathbf{u}\)</span> and <span
class="math inline">\(\mathbf{v}\)</span> are orthogonal. <span
class="math inline">\(\mathbf{u} \perp \mathbf{v}\)</span> ：向量 <span
class="math inline">\(\mathbf{u}\)</span> 和 <span
class="math inline">\(\mathbf{v}\)</span> 正交。</li>
<li>An orthogonal set: vectors pairwise orthogonal.
正交集：向量两两正交。</li>
<li>An orthonormal set: pairwise orthogonal, each of norm 1.
正交集：两两正交，每组范数为 1。</li>
</ul>
<h3 id="why-this-matters-2">Why this matters</h3>
<p>为什么这很重要</p>
<p>Orthogonality gives structure to vector spaces. It provides a way to
separate independent directions cleanly, simplify computations, and
minimize errors in approximations. Many powerful algorithms in numerical
linear algebra and data science (QR decomposition, least squares
regression, PCA) rely on orthogonality.
正交性赋予向量空间结构。它提供了一种清晰地分离独立方向、简化计算并最小化近似误差的方法。数值线性代数和数据科学中许多强大的算法（例如
QR 分解、最小二乘回归、主成分分析）都依赖于正交性。</p>
<h3 id="exercises-1.4">Exercises 1.4</h3>
<p>练习 1.4</p>
<ol type="1">
<li>Verify that the vectors <span class="math inline">\((1,2,2)\)</span>
and <span class="math inline">\((2,0,-1)\)</span> are orthogonal.
验证向量 <span class="math inline">\((1,2,2)\)</span> 和 <span
class="math inline">\((2,0,-1)\)</span> 是否正交。</li>
<li>Find the projection of <span class="math inline">\((3,4)\)</span>
onto <span class="math inline">\((1,1)\)</span>. 找到 <span
class="math inline">\((3,4)\)</span> 到 <span
class="math inline">\((1,1)\)</span> 的投影。</li>
<li>Show that any two distinct standard basis vectors in <span
class="math inline">\(\mathbb{R}^n\)</span> are orthogonal. 证明 <span
class="math inline">\(\mathbb{R}^n\)</span>
中的任意两个不同的标准基向量都是正交的。</li>
<li>Decompose <span class="math inline">\((5,2)\)</span> into components
parallel and orthogonal to <span class="math inline">\((2,1)\)</span>.
将 <span class="math inline">\((5,2)\)</span> 分解为与 <span
class="math inline">\((2,1)\)</span> 平行且正交的分量。</li>
<li>Let <span class="math inline">\(\mathbf{u}, \mathbf{v}\)</span> be
orthogonal nonzero vectors. (a) Show that <span
class="math inline">\((\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=\lVert
\mathbf{u}\rVert^2-\lVert \mathbf{v}\rVert^2.\)</span> (b) For what
condition on <span class="math inline">\(\mathbf{u}\)</span> and <span
class="math inline">\(\mathbf{v}\)</span> does <span
class="math inline">\((\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=0\)</span>?
令 <span class="math inline">\(\mathbf{u}, \mathbf{v}\)</span>
为正交非零向量。（a）证明 <span
class="math inline">\((\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=\lVert
\mathbf{u}\rVert^2-\lVert \mathbf{v}\rVert^2.\)</span> （b） <span
class="math inline">\((\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=0\)</span>
对 <span class="math inline">\(\mathbf{u}\)</span> 和 <span
class="math inline">\(\mathbf{v}\)</span> 满足什么条件？</li>
</ol>
<h1 id="chapter-2.-matrices">Chapter 2. Matrices</h1>
<p>第 2 章矩阵</p>
<h2 id="definition-and-notation">2.1 Definition and Notation</h2>
<p>2.1 定义和符号</p>
<p>Matrices are the central objects of linear algebra, providing a
compact way to represent and manipulate linear transformations, systems
of equations, and structured data. A matrix is a rectangular array of
numbers arranged in rows and columns.
矩阵是线性代数的核心对象，它提供了一种简洁的方式来表示和操作线性变换、方程组和结构化数据。矩阵是由按行和列排列的数字组成的矩形阵列。</p>
<h3 id="formal-definition">Formal Definition</h3>
<p>正式定义</p>
<p>An <span class="math inline">\(m \times n\)</span> matrix is an array
with <span class="math inline">\(m\)</span> rows and <span
class="math inline">\(n\)</span> columns, written <span
class="math inline">\(m \times n\)</span> 矩阵是具有 <span
class="math inline">\(m\)</span> 行和 <span
class="math inline">\(n\)</span> 列的数组，写为</p>
<p><span class="math display">\[
A =\begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\a_{21}
&amp; a_{22} &amp; \cdots &amp; a_{2n} \\\vdots &amp; \vdots &amp;
\ddots &amp; \vdots \\a_{m1} &amp; a_{m2} &amp; \cdots &amp;
a_{mn}\end{bmatrix}.
\]</span></p>
<p>Each entry <span class="math inline">\(a_{ij}\)</span> is a scalar,
located in the <em>i</em>-th row and <em>j</em>-th column. The size (or
dimension) of the matrix is denoted by <span class="math inline">\(m
\times n\)</span>. 每个条目𝑎 𝑖 𝑗 a 伊奇 ​ 是一个标量，位于第 - 行和第 -
列。矩阵的大小（或维度）用 <span class="math inline">\(m \times
n\)</span> 表示。</p>
<ul>
<li>If <span class="math inline">\(m = n\)</span>, the matrix is square.
如果为 <span class="math inline">\(m = n\)</span> ，则矩阵为方阵。</li>
<li>If <span class="math inline">\(m = 1\)</span>, the matrix is a row
vector. 如果为 <span class="math inline">\(m = 1\)</span>
，则该矩阵为行向量。</li>
<li>If <span class="math inline">\(n = 1\)</span>, the matrix is a
column vector. 如果为 <span class="math inline">\(n = 1\)</span>
，则矩阵为列向量。</li>
</ul>
<p>Thus, vectors are simply special cases of matrices.
因此，向量只是矩阵的特殊情况。</p>
<h3 id="examples">Examples</h3>
<p>示例</p>
<p>Example 2.1.1. A 2×3 matrix: 例 2.1.1. 2×3 矩阵：</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; -2 &amp; 4 \\0 &amp; 3 &amp; 5\end{bmatrix}.
\]</span></p>
<p>Here, <span class="math inline">\(a_{12} = -2\)</span>, <span
class="math inline">\(a_{23} = 5\)</span>, and the matrix has 2 rows, 3
columns. 这里， <span class="math inline">\(a_{12} = -2\)</span> ，
<span class="math inline">\(a_{23} = 5\)</span> ，矩阵有 2 行，3
列。</p>
<p>Example 2.1.2. A 3×3 square matrix: 例 2.1.2. 3×3 方阵：</p>
<p><span class="math display">\[
B = \begin{bmatrix}2 &amp; 0 &amp; 1 \\-1 &amp; 3 &amp; 4 \\0 &amp; 5
&amp; -2\end{bmatrix}.
\]</span></p>
<p>This will later serve as the representation of a linear
transformation on <span class="math inline">\(\mathbb{R}^3\)</span>.
这稍后将作为 <span class="math inline">\(\mathbb{R}^3\)</span>
的线性变换的表示。</p>
<h3 id="indexing-and-notation">Indexing and Notation</h3>
<p>索引和符号</p>
<ul>
<li>Matrices are denoted by uppercase bold letters: <span
class="math inline">\(A, B, C\)</span>. 矩阵用大写粗体字母表示： <span
class="math inline">\(A, B, C\)</span> 。</li>
<li>Entries are written as <span class="math inline">\(a_{ij}\)</span>,
with the row index first, column index second. 条目写为𝑎 𝑖 𝑗 a 伊奇 ​
，其中行索引在前，列索引在后。</li>
<li>The set of all real <span class="math inline">\(m \times n\)</span>
matrices is denoted <span class="math inline">\(\mathbb{R}^{m \times
n}\)</span>. 所有实数 <span class="math inline">\(m \times n\)</span>
矩阵的集合表示为 <span class="math inline">\(\mathbb{R}^{m \times
n}\)</span> 。</li>
</ul>
<p>Thus, a matrix is a function <span class="math inline">\(A:
{1,\dots,m} \times {1,\dots,n} \to \mathbb{R}\)</span>, assigning a
scalar to each row-column position. 因此，矩阵是一个函数 <span
class="math inline">\(A: {1,\dots,m} \times {1,\dots,n} \to
\mathbb{R}\)</span> ，为每个行列位置分配一个标量。</p>
<h3 id="why-this-matters-3">Why this matters</h3>
<p>为什么这很重要</p>
<p>Matrices generalize vectors and give us a language for describing
linear operations systematically. They encode systems of equations,
rotations, projections, and transformations of data. With matrices,
algebra and geometry come together: a single compact object can
represent both numerical data and functional rules.
矩阵推广了向量，并为我们提供了一种系统地描述线性运算的语言。它们对方程组、旋转、投影和数据变换进行编码。矩阵将代数和几何结合在一起：一个紧凑的对象既可以表示数值数据，又可以表示函数规则。</p>
<h3 id="exercises-2.1">Exercises 2.1</h3>
<p>练习 2.1</p>
<ol type="1">
<li>Write a <span class="math inline">\(3 \\times 2\)</span>matrix of
your choice and identify its entries<span
class="math inline">\(a\_{ij}\)</span>. 写出 $3 \times 2 <span
class="math inline">\(matrix of your choice and identify its
entries\)</span> a_{ij}$。</li>
<li>Is every vector a matrix? Is every matrix a vector? Explain.
每个向量都是矩阵吗？每个矩阵都是向量吗？请解释。</li>
<li>Which of the following are square matrices: <span
class="math inline">\(A \in \mathbb{R}^{4\times4}\)</span>, <span
class="math inline">\(B \in \mathbb{R}^{3\times5}\)</span>, <span
class="math inline">\(C \in \mathbb{R}^{1\times1}\)</span>?
下列哪些是正方形 矩阵： <span class="math inline">\(A \in
\mathbb{R}^{4\times4}\)</span> ， <span class="math inline">\(B \in
\mathbb{R}^{3\times5}\)</span> ， <span class="math inline">\(C \in
\mathbb{R}^{1\times1}\)</span> ？</li>
<li>Let 让</li>
</ol>
<p><span class="math display">\[
D = \begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>What kind of matrix is this? 5. Consider the matrix
这是什么类型的矩阵？5. 考虑矩阵</p>
<p><span class="math display">\[
E = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}
\]</span></p>
<p>Express <span class="math inline">\(e_{11}, e_{12}, e_{21},
e_{22}\)</span> explicitly. 快递𝑒 11 , 𝑒 12 , 𝑒 21 , 𝑒 22 e 11 ​ ，e 12 ​
，e 21 ​ ，e 22 ​ 明确地。</p>
<h2 id="matrix-addition-and-multiplication">2.2 Matrix Addition and
Multiplication</h2>
<p>2.2 矩阵加法和乘法</p>
<p>Once matrices are defined, the next step is to understand how they
combine. Just as vectors gain meaning through addition and scalar
multiplication, matrices become powerful through two operations:
addition and multiplication.
定义好矩阵后，下一步就是理解它们是如何组合的。正如向量通过加法和标量乘法获得意义一样，矩阵也通过两种运算变得强大：加法和乘法。</p>
<h3 id="matrix-addition">Matrix Addition</h3>
<p>矩阵加法</p>
<p>Two matrices of the same size are added by adding corresponding
entries. If 两个大小相同的矩阵可以通过添加相应的元素来相加。如果</p>
<p><span class="math display">\[
A = [a_{ij}] \in \mathbb{R}^{m \times n}, \quad B = [b_{ij}] \in
\mathbb{R}^{m \times n},
\]</span></p>
<p>then 然后</p>
<p><span class="math display">\[
A + B = [a_{ij} + b_{ij}] \in \mathbb{R}^{m \times n}.
\]</span></p>
<p>Example 2.2.1. Let 例 2.2.1. 设</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\3 &amp; 4\end{bmatrix}, \quad B =
\begin{bmatrix}-1 &amp; 0 \\5 &amp; 2\end{bmatrix}.
\]</span></p>
<p>Then 然后</p>
<p><span class="math display">\[
A + B = \begin{bmatrix}1 + (-1) &amp; 2 + 0 \\3 + 5 &amp; 4 +
2\end{bmatrix} =\begin{bmatrix}0 &amp; 2 \\8 &amp; 6\end{bmatrix}.
\]</span></p>
<p>Matrix addition is commutative (<span class="math inline">\(A+B =
B+A\)</span>) and associative (<span class="math inline">\((A+B)+C =
A+(B+C)\)</span>). The zero matrix, with all entries 0, acts as the
additive identity. 矩阵加法满足交换律 ( <span class="math inline">\(A+B
= B+A\)</span> ) 和结合律 ( <span class="math inline">\((A+B)+C =
A+(B+C)\)</span> )。零矩阵（所有元素均为 0）充当加法恒等式。</p>
<h3 id="scalar-multiplication-1">Scalar Multiplication</h3>
<p>标量乘法</p>
<p>For a scalar <span class="math inline">\(c \in \mathbb{R}\)</span>
and a matrix <span class="math inline">\(A = [[a_{ij}]\)</span>, we
define 对于标量 <span class="math inline">\(c \in \mathbb{R}\)</span>
和矩阵 <span class="math inline">\(A = [[a_{ij}]\)</span> ，我们定义</p>
<p><span class="math display">\[
cA = [c \cdot a_{ij}].
\]</span></p>
<p>This stretches or shrinks all entries of the matrix uniformly.
这会均匀地拉伸或收缩矩阵的所有条目。</p>
<p>Example 2.2.2. If 例 2.2.2. 如果</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; -1 \\0 &amp; 3\end{bmatrix}, \quad c = -2,
\]</span></p>
<p>then 然后</p>
<p><span class="math display">\[
cA = \begin{bmatrix}-4 &amp; 2 \\0 &amp; -6\end{bmatrix}.
\]</span></p>
<h3 id="matrix-multiplication">Matrix Multiplication</h3>
<p>矩阵乘法</p>
<p>The defining operation of matrices is multiplication. If
矩阵的定义运算是乘法。如果</p>
<p><span class="math display">\[
A \in \mathbb{R}^{m \times n}, \quad B \in \mathbb{R}^{n \times p},
\]</span></p>
<p>then their product is the <span class="math inline">\(m \times
p\)</span> matrix 那么它们的乘积就是 <span class="math inline">\(m
\times p\)</span> 矩阵</p>
<p><span class="math display">\[
AB = C = [c_{ij}], \quad c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}.
\]</span></p>
<p>Thus, the entry in the <span class="math inline">\(i\)</span>-th row
and <span class="math inline">\(j\)</span>-th column of <span
class="math inline">\(AB\)</span> is the dot product of the <span
class="math inline">\(i\)</span>-th row of <span
class="math inline">\(A\)</span> with the <span
class="math inline">\(j\)</span>-th column of <span
class="math inline">\(B\)</span>. 因此， <span
class="math inline">\(AB\)</span> 第 <span
class="math inline">\(i\)</span> 行、第 <span
class="math inline">\(j\)</span> 列的条目是 <span
class="math inline">\(A\)</span> 第 <span
class="math inline">\(i\)</span> 行与 <span
class="math inline">\(B\)</span> 第 <span
class="math inline">\(j\)</span> 列的点积。</p>
<p>Example 2.2.3. Let 例 2.2.3. 设</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\0 &amp; 3\end{bmatrix}, \quad B =
\begin{bmatrix}4 &amp; -1 \\2 &amp; 5\end{bmatrix}.
\]</span></p>
<p>Then 然后</p>
<p><span class="math display">\[
AB = \begin{bmatrix}1\cdot4 + 2\cdot2 &amp; 1\cdot(-1) + 2\cdot5
\\0\cdot4 + 3\cdot2 &amp; 0\cdot(-1) + 3\cdot5\end{bmatrix}
=\begin{bmatrix}8 &amp; 9 \\6 &amp; 15\end{bmatrix}.
\]</span></p>
<p>Notice that matrix multiplication is not commutative in general:
<span class="math inline">\(AB \neq BA\)</span>. Sometimes <span
class="math inline">\(BA\)</span> may not even be defined if dimensions
do not align. 请注意，矩阵乘法通常不满足交换律： <span
class="math inline">\(AB \neq BA\)</span>
。如果维度不一致，有时甚至可能无法定义 <span
class="math inline">\(BA\)</span> 。</p>
<h3 id="geometric-meaning">Geometric Meaning</h3>
<p>几何意义</p>
<p>Matrix multiplication corresponds to the composition of linear
transformations. If <span class="math inline">\(A\)</span> transforms
vectors in <span class="math inline">\(\mathbb{R}^n\)</span> and <span
class="math inline">\(B\)</span> transforms vectors in <span
class="math inline">\(\mathbb{R}^p\)</span>, then <span
class="math inline">\(AB\)</span> represents applying <span
class="math inline">\(B\)</span> first, then <span
class="math inline">\(A\)</span>. This makes matrices the algebraic
language of transformations. 矩阵乘法对应于线性变换的复合。如果 <span
class="math inline">\(A\)</span> 变换 <span
class="math inline">\(\mathbb{R}^n\)</span> 中的向量， <span
class="math inline">\(B\)</span> 变换 <span
class="math inline">\(\mathbb{R}^p\)</span> 中的向量，那么 <span
class="math inline">\(AB\)</span> 表示先应用 <span
class="math inline">\(B\)</span> ，然后再应用 <span
class="math inline">\(A\)</span> 。这使得矩阵成为变换的代数语言。</p>
<h3 id="notation-4">Notation</h3>
<p>符号</p>
<ul>
<li>Matrix sum: <span class="math inline">\(A+B\)</span>. 矩阵和： <span
class="math inline">\(A+B\)</span> 。</li>
<li>Scalar multiple: <span class="math inline">\(cA\)</span>. 标量倍数：
<span class="math inline">\(cA\)</span> 。</li>
<li>Product: <span class="math inline">\(AB\)</span>, defined only when
the number of columns of <span class="math inline">\(A\)</span> equals
the number of rows of <span class="math inline">\(B\)</span>. 乘积：
<span class="math inline">\(AB\)</span> ，仅当 <span
class="math inline">\(A\)</span> 的列数等于 <span
class="math inline">\(B\)</span> 的行数时才定义。</li>
</ul>
<h3 id="why-this-matters-4">Why this matters</h3>
<p>为什么这很重要</p>
<p>Matrix multiplication is the core mechanism of linear algebra: it
encodes how transformations combine, how systems of equations are
solved, and how data flows in modern algorithms. Addition and scalar
multiplication make matrices into a vector space, while multiplication
gives them an algebraic structure rich enough to model geometry,
computation, and networks.
矩阵乘法是线性代数的核心机制：它编码了变换的组合方式、方程组的求解方式以及现代算法中数据流动的方式。加法和标量乘法将矩阵转化为向量空间，而乘法则赋予矩阵丰富的代数结构，使其能够对几何、计算和网络进行建模。</p>
<h3 id="exercises-2.2">Exercises 2.2</h3>
<p>练习 2.2</p>
<ol type="1">
<li>Compute <span class="math inline">\(A+B\)</span> for 计算 <span
class="math inline">\(A+B\)</span></li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 3 \\-1 &amp; 0 \end{bmatrix}, \quad B =
\begin{bmatrix} 4 &amp; -2 \\5 &amp; 7 \end{bmatrix}.
\]</span></p>
<ol start="2" type="1">
<li>Find 3A where 查找 3A</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; -4 \\2 &amp; 6 \end{bmatrix}.
\]</span></p>
<ol start="3" type="1">
<li>Multiply 乘</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 0 &amp; 2 \\-1 &amp; 3 &amp; 1
\end{bmatrix}, \quad B = \begin{bmatrix} 2 &amp; 1 \\0 &amp; -1 \\3
&amp; 4 \end{bmatrix}.
\]</span></p>
<ol start="4" type="1">
<li>Verify with an explicit example that <span class="math inline">\(AB
\neq BA\)</span>. 通过明确的例子来验证 <span class="math inline">\(AB
\neq BA\)</span> 。</li>
<li>Prove that matrix multiplication is distributive: <span
class="math inline">\(A(B+C) = AB + AC\)</span>. 证明矩阵乘法是分配的：
<span class="math inline">\(A(B+C) = AB + AC\)</span> 。</li>
</ol>
<h2 id="transpose-and-inverse">2.3 Transpose and Inverse</h2>
<p>2.3 转置和逆</p>
<p>Two special operations on matrices-the transpose and the inverse-give
rise to deep algebraic and geometric properties. The transpose
rearranges a matrix by flipping it across its main diagonal, while the
inverse, when it exists, acts as the undo operation for matrix
multiplication.
矩阵的两种特殊运算——转置和逆——引出了深刻的代数和几何性质。转置通过沿矩阵主对角线翻转来重新排列矩阵，而逆（如果存在）则充当矩阵乘法的撤消操作。</p>
<h3 id="the-transpose">The Transpose</h3>
<p>转置</p>
<p>The transpose of an <span class="math inline">\(m \times n\)</span>
matrix <span class="math inline">\(A = [a_{ij}]\)</span> is the <span
class="math inline">\(n \times m\)</span> matrix <span
class="math inline">\(A^T = [a_{ji}]\)</span>, obtained by swapping rows
and columns. <span class="math inline">\(m \times n\)</span> 矩阵 <span
class="math inline">\(A = [a_{ij}]\)</span> 的转置是通过交换行和列获得的
<span class="math inline">\(n \times m\)</span> 矩阵 <span
class="math inline">\(A^T = [a_{ji}]\)</span> 。</p>
<p>Formally, 正式地，</p>
<p><span class="math display">\[
(A^T)\_{ij} = a\_{ji}.
\]</span></p>
<p>Example 2.3.1. If 例 2.3.1. 如果</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 4 &amp; -2 \\0 &amp; 3 &amp; 5\end{bmatrix},
\]</span></p>
<p>then 然后</p>
<p><span class="math display">\[
A^T = \begin{bmatrix}1 &amp; 0 \\4 &amp; 3 \\-2 &amp; 5\end{bmatrix}.
\]</span></p>
<p>Properties of the Transpose. 转置的属性。</p>
<ol type="1">
<li><span class="math inline">\((A^T)^T = A\)</span>.</li>
<li><span class="math inline">\((A+B)^T = A^T + B^T\)</span>.</li>
<li><span class="math inline">\((cA)^T = cA^T\)</span>, for scalar <span
class="math inline">\(c\)</span>. <span class="math inline">\((cA)^T =
cA^T\)</span> ，对于标量 <span class="math inline">\(c\)</span> 。</li>
<li><span class="math inline">\((AB)^T = B^T A^T\)</span>.</li>
</ol>
<p>The last rule is crucial: the order reverses.
最后一条规则至关重要：顺序反转。</p>
<h3 id="the-inverse">The Inverse</h3>
<p>逆向</p>
<p>A square matrix <span class="math inline">\(A \in \mathbb{R}^{n
\times n}\)</span> is said to be invertible (or nonsingular) if there
exists another matrix <span class="math inline">\(A^{-1}\)</span> such
that 如果存在另一个矩阵 <span class="math inline">\(A^{-1}\)</span>
满足以下条件，则称方阵 <span class="math inline">\(A \in \mathbb{R}^{n
\times n}\)</span> 可逆（或非奇异）</p>
<p><span class="math display">\[
AA^{-1} = A^{-1}A = I_n,
\]</span></p>
<p>where <span class="math inline">\(I_n\)</span> is the <span
class="math inline">\(n \times n\)</span> identity matrix. In this case,
<span class="math inline">\(A^{-1}\)</span> is called the inverse of
<span class="math inline">\(A\)</span>. 其中𝐼 𝑛 I n ​ 是 <span
class="math inline">\(n \times n\)</span> 单位矩阵。在这种情况下， <span
class="math inline">\(A^{-1}\)</span> 被称为 <span
class="math inline">\(A\)</span> 的逆。</p>
<p>Not every matrix is invertible. A necessary condition is that <span
class="math inline">\(\det(A) \neq 0\)</span>, a fact that will be
developed in Chapter 6. 并非所有矩阵都是可逆的。必要条件是 <span
class="math inline">\(\det(A) \neq 0\)</span> ，我们将在第 6
章中进一步阐述。</p>
<p>Example 2.3.2. Let 例 2.3.2. 设</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\3 &amp; 4\end{bmatrix}.
\]</span></p>
<p>Its determinant is <span class="math inline">\(\det(A) = (1)(4) -
(2)(3) = -2 \neq 0\)</span>. The inverse is 它的行列式是 <span
class="math inline">\(\det(A) = (1)(4) - (2)(3) = -2 \neq 0\)</span>
。逆是</p>
<p><span class="math display">\[
A^{-1} = \frac{1}{\det(A)} \begin{bmatrix}4 &amp; -2 \\-3 &amp;
1\end{bmatrix} =\begin{bmatrix}-2 &amp; 1 \\1.5 &amp; -0.5\end{bmatrix}.
\]</span></p>
<p>Verification: 确认：</p>
<p><span class="math display">\[
AA^{-1} = \begin{bmatrix}1 &amp; 2 \\3 &amp;
4\end{bmatrix}\begin{bmatrix}-2 &amp; 1 \\1.5 &amp; -0.5\end{bmatrix}
=\begin{bmatrix}1 &amp; 0 \\0 &amp; 1\end{bmatrix}.
\]</span></p>
<h3 id="geometric-meaning-1">Geometric Meaning</h3>
<p>几何意义</p>
<ul>
<li>The transpose corresponds to reflecting a linear transformation
across the diagonal. For vectors, it switches between row and column
forms.
转置对应于沿对角线反映线性变换。对于向量，它在行和列形式之间切换。</li>
<li>The inverse, when it exists, corresponds to reversing a linear
transformation. For example, if <span class="math inline">\(A\)</span>
scales and rotates vectors, <span class="math inline">\(A^{-1}\)</span>
rescales and rotates them back.
如果存在逆变换，则它对应于线性变换的逆变换。例如，如果 <span
class="math inline">\(A\)</span> 缩放并旋转了矢量，则 <span
class="math inline">\(A^{-1}\)</span> 会将其重新缩放并旋转回去。</li>
</ul>
<h3 id="notation-5">Notation</h3>
<p>符号</p>
<ul>
<li>Transpose: <span class="math inline">\(A^T\)</span>. 转置： <span
class="math inline">\(A^T\)</span> 。</li>
<li>Inverse: <span class="math inline">\(A^{-1}\)</span>, defined only
for invertible square matrices. 逆： <span
class="math inline">\(A^{-1}\)</span> ，仅为可逆方阵定义。</li>
<li>Identity: <span class="math inline">\(I_n\)</span>, acts as the
multiplicative identity. 身份：𝐼 𝑛 I n ​ ，充当乘法恒等式。</li>
</ul>
<h3 id="why-this-matters-5">Why this matters</h3>
<p>为什么这很重要</p>
<p>The transpose allows us to define symmetric and orthogonal matrices,
central to geometry and numerical methods. The inverse underlies the
solution of linear systems, encoding the idea of undoing a
transformation. Together, these operations set the stage for
determinants, eigenvalues, and orthogonalization.
转置使我们能够定义对称矩阵和正交矩阵，这是几何和数值方法的核心。逆矩阵是线性系统解的基础，它蕴含着撤销变换的思想。这些运算共同为行列式、特征值和正交化奠定了基础。</p>
<h3 id="exercises-2.3">Exercises 2.3</h3>
<p>练习 2.3</p>
<ol type="1">
<li>Compute the transpose of 计算转置</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; -1 &amp; 3 \\ 0 &amp; 4 &amp; 5
\end{bmatrix}.
\]</span></p>
<ol start="2" type="1">
<li>Verify that <span class="math inline">\((AB)^T = B^T A^T\)</span>
for 验证 <span class="math inline">\((AB)^T = B^T A^T\)</span></li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\0 &amp; 1 \end{bmatrix}, \quad B =
\begin{bmatrix}3 &amp; 4 \\5 &amp; 6 \end{bmatrix}.
\]</span></p>
<ol start="3" type="1">
<li>Determine whether 确定是否</li>
</ol>
<p><span class="math display">\[
C = \begin{bmatrix}2 &amp; 1 \\4 &amp; 2 \end{bmatrix}
\]</span></p>
<p>is invertible. If so, find <span
class="math inline">\(C^{-1}\)</span>. 可逆。如果可逆，则求 <span
class="math inline">\(C^{-1}\)</span> 。</p>
<ol start="4" type="1">
<li>Find the inverse of 求逆</li>
</ol>
<p><span class="math display">\[
D = \begin{bmatrix}0 &amp; 1 \\-1 &amp; 0 \end{bmatrix},
\]</span></p>
<p>and explain its geometric action on vectors in the plane.
并解释其对平面向量的几何作用。</p>
<ol start="5" type="1">
<li>Prove that if <span class="math inline">\(A\)</span> is invertible,
then so is <span class="math inline">\(A^T\)</span>, and <span
class="math inline">\((A^T)^{-1} = (A^{-1})^T\)</span>. 证明如果 <span
class="math inline">\(A\)</span> 可逆，则 <span
class="math inline">\(A^T\)</span> 和 <span
class="math inline">\((A^T)^{-1} = (A^{-1})^T\)</span> 也可逆。</li>
</ol>
<h2 id="special-matrices">2.4 Special Matrices</h2>
<p>2.4 特殊矩阵</p>
<p>Certain matrices occur so frequently in theory and applications that
they are given special names. Recognizing their properties allows us to
simplify computations and understand the structure of linear
transformations more clearly.
某些矩阵在理论和应用中出现频率很高，因此被赋予了特殊的名称。了解它们的性质可以简化计算，并更清楚地理解线性变换的结构。</p>
<h3 id="the-identity-matrix">The Identity Matrix</h3>
<p>身份矩阵</p>
<p>The identity matrix <span class="math inline">\(I_n\)</span> is the
<span class="math inline">\(n \times n\)</span> matrix with ones on the
diagonal and zeros elsewhere: 单位矩阵𝐼 𝑛 I n ​ 是 <span
class="math inline">\(n \times n\)</span> 矩阵，对角线上为 1，其他位置为
0：</p>
<p><span class="math display">\[
I_n = \begin{bmatrix}1 &amp; 0 &amp; \cdots &amp; 0 \\0 &amp; 1 &amp;
\cdots &amp; 0 \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\0 &amp;
0 &amp; \cdots &amp; 1\end{bmatrix}.
\]</span></p>
<p>It acts as the multiplicative identity: 它充当乘法恒等式：</p>
<p><span class="math display">\[
AI_n = I_nA = A, \quad \text{for all } A \in \mathbb{R}^{n \times n}.
\]</span></p>
<p>Geometrically, <span class="math inline">\(I_n\)</span> represents
the transformation that leaves every vector unchanged. 从几何学上讲，𝐼 𝑛
I n ​ 表示保持每个向量不变的变换。</p>
<h3 id="diagonal-matrices">Diagonal Matrices</h3>
<p>对角矩阵</p>
<p>A diagonal matrix has all off-diagonal entries zero:
对角矩阵的所有非对角元素均为零：</p>
<p><span class="math display">\[
D = \begin{bmatrix}d_{11} &amp; 0 &amp; \cdots &amp; 0 \\0 &amp; d_{22}
&amp; \cdots &amp; 0 \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\0
&amp; 0 &amp; \cdots &amp; d_{nn}\end{bmatrix}.
\]</span></p>
<p>Multiplication by a diagonal matrix scales each coordinate
independently: 与对角矩阵相乘可独立缩放每个坐标：</p>
<p><span class="math display">\[
D\mathbf{x} = (d_{11}x_1, d_{22}x_2, \dots, d_{nn}x_n).
\]</span></p>
<p>Example 2.4.1. Let 例 2.4.1. 设</p>
<p><span class="math display">\[
D = \begin{bmatrix} 2 &amp; 0 &amp; 0 \\0 &amp; 3 &amp; 0 \\0 &amp; 0
&amp; -1 \end{bmatrix}, \quad\mathbf{x} = \begin{bmatrix}1 \\4 \\-2
\end{bmatrix}.
\]</span></p>
<p>Then 然后</p>
<p><span class="math display">\[
D\mathbf{x} = \begin{bmatrix}2 \\12 \\2 \end{bmatrix}.
\]</span></p>
<h3 id="permutation-matrices">Permutation Matrices</h3>
<p>置换矩阵</p>
<p>A permutation matrix is obtained by permuting the rows of the
identity matrix. Multiplying a vector by a permutation matrix reorders
its coordinates.
置换矩阵是通过对单位矩阵的行进行置换而得到的。将向量乘以置换矩阵会重新排序其坐标。</p>
<p>Example 2.4.2. Let 例 2.4.2. 设</p>
<p><span class="math display">\[
P = \begin{bmatrix}0 &amp; 1 &amp; 0 \\1 &amp; 0 &amp; 0 \\0 &amp; 0
&amp; 1\end{bmatrix}.
\]</span></p>
<p>Then 然后</p>
<p><span class="math display">\[
P\begin{bmatrix}a \\b \\c \end{bmatrix} =\begin{bmatrix} b \\a \\c
\end{bmatrix}.
\]</span></p>
<p>Thus, <span class="math inline">\(P\)</span> swaps the first two
coordinates. 因此， <span class="math inline">\(P\)</span>
交换前两个坐标。</p>
<p>Permutation matrices are always invertible; their inverses are simply
their transposes. 置换矩阵总是可逆的；它们的逆只是它们的转置。</p>
<h3 id="symmetric-and-skew-symmetric-matrices">Symmetric and
Skew-Symmetric Matrices</h3>
<p>对称矩阵和斜对称矩阵</p>
<p>A matrix is symmetric if 如果矩阵是对称的</p>
<p><span class="math display">\[
A^T = A,
\]</span></p>
<p>and skew-symmetric if Symmetric matrices appear in quadratic forms
and optimization, while skew-symmetric matrices describe rotations and
cross products in geometry.
如果对称矩阵出现在二次形式和优化中，则为斜对称，而斜对称矩阵描述几何中的旋转和叉积。</p>
<h3 id="orthogonal-matrices">Orthogonal Matrices</h3>
<p>正交矩阵</p>
<p>A square matrix <span class="math inline">\(Q\)</span> is orthogonal
if 方阵 <span class="math inline">\(Q\)</span> 是正交的，如果</p>
<p><span class="math display">\[
Q^T Q = QQ^T = I.
\]</span></p>
<p>Equivalently, the rows (and columns) of <span
class="math inline">\(Q\)</span> form an orthonormal set. Orthogonal
matrices preserve lengths and angles; they represent rotations and
reflections. 等价地， <span class="math inline">\(Q\)</span>
的行（和列）构成一个正交集。正交矩阵保留长度和角度；它们表示旋转和反射。</p>
<p>Example 2.4.3. The rotation matrix in the plane: 例2.4.3.
平面内的旋转矩阵:</p>
<p><span class="math display">\[
R(\theta) = \begin{bmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta
&amp; \cos\theta\end{bmatrix}
\]</span></p>
<p>is orthogonal, since 是正交的，因为</p>
<p><span class="math display">\[
R(\theta)^T R(\theta) = I_2.
\]</span></p>
<h3 id="why-this-matters-6">Why this matters</h3>
<p>为什么这很重要</p>
<p>Special matrices serve as the building blocks of linear algebra.
Identity matrices define the neutral element, diagonal matrices simplify
computations, permutation matrices reorder data, symmetric and
orthogonal matrices describe fundamental geometric structures. Much of
modern applied mathematics reduces complex problems to operations
involving these simple forms.
特殊矩阵是线性代数的基石。单位矩阵定义中性元素，对角矩阵简化计算，置换矩阵重新排序数据，对称矩阵和正交矩阵描述基本几何结构。许多现代应用数学将复杂问题简化为涉及这些简单形式的运算。</p>
<h3 id="exercises-2.4">Exercises 2.4</h3>
<p>练习 2.4</p>
<ol type="1">
<li>Show that the product of two diagonal matrices is diagonal, and
compute an example.
证明两个对角矩阵的乘积是对角的，并计算一个例子。</li>
<li>Find the permutation matrix that cycles <span
class="math inline">\((a,b,c)\)</span> into <span
class="math inline">\((b,c,a)\)</span>. 找到将 <span
class="math inline">\((a,b,c)\)</span> 循环到 <span
class="math inline">\((b,c,a)\)</span> 的置换矩阵。</li>
<li>Prove that every permutation matrix is invertible and its inverse is
its transpose. 证明每个置换矩阵都是可逆的，并且它的逆是它的转置。</li>
<li>Verify that 验证</li>
</ol>
<p><span class="math display">\[
Q = \begin{bmatrix}0 &amp; 1 \\-1 &amp; 0 \end{bmatrix}
\]</span></p>
<p>is orthogonal. What geometric transformation does it represent? 5.
Determine whether 是正交的。它代表什么几何变换？5. 判断</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 3 \\3 &amp; 2 \end{bmatrix}, \quad B =
\begin{bmatrix}0 &amp; 5 \\-5 &amp; 0 \end{bmatrix}
\]</span></p>
<p>are symmetric, skew-symmetric, or neither.
是对称的、斜对称的，或者都不是。</p>
<h1 id="chapter-3.-systems-of-linear-equations">Chapter 3. Systems of
Linear Equations</h1>
<p>第 3 章线性方程组</p>
<h2 id="linear-systems-and-solutions">3.1 Linear Systems and
Solutions</h2>
<p>3.1 线性系统及其解</p>
<p>One of the central motivations for linear algebra is solving systems
of linear equations. These systems arise naturally in science,
engineering, and data analysis whenever multiple constraints interact.
Matrices provide a compact language for expressing and solving them.
线性代数的核心动机之一是求解线性方程组。在科学、工程和数据分析领域，当多个约束相互作用时，这类方程组自然而然地出现。矩阵提供了一种简洁的语言来表达和求解它们。</p>
<h3 id="linear-systems">Linear Systems</h3>
<p>线性系统</p>
<p>A linear system consists of equations where each unknown appears only
to the first power and with no products between variables. A general
system of <span class="math inline">\(m\)</span> equations in <span
class="math inline">\(n\)</span> unknowns can be written as:
线性系统由方程组成，其中每个未知数仅出现一次方，并且之间没有乘积
变量。包含 <span class="math inline">\(n\)</span> 个未知数的 <span
class="math inline">\(m\)</span> 个方程的一般系统可以写成：</p>
<p><span class="math display">\[
\begin{aligned}a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &amp;= b_1,
\\a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &amp;= b_2, \\&amp;\vdots
\\a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &amp;= b_m.\end{aligned}
\]</span></p>
<p>Here the coefficients <span class="math inline">\(a_{ij}\)</span> and
constants <span class="math inline">\(b_i\)</span> are scalars, and the
unknowns are <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>.
这里系数𝑎 𝑖 𝑗 a 伊奇 ​ 和常数𝑏 𝑖 b i ​ 是标量，未知数是𝑥 1 , 𝑥 2 , … , 𝑥 𝑛
x 1 ​ ，x 2 ​ ，…，x n ​ .</p>
<h3 id="matrix-form">Matrix Form</h3>
<p>矩阵形式</p>
<p>The system can be expressed compactly as:
该系统可以简洁地表示为：</p>
<p><span class="math display">\[
A\mathbf{x} = \mathbf{b},
\]</span></p>
<p>where 在哪里</p>
<ul>
<li><span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>
is the coefficient matrix <span class="math inline">\([a_{ij}]\)</span>,
<span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>
是系数矩阵 <span class="math inline">\([a_{ij}]\)</span> ，</li>
<li><span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> is
the column vector of unknowns, <span class="math inline">\(\mathbf{x}
\in \mathbb{R}^n\)</span> 是未知数的列向量，</li>
<li><span class="math inline">\(\mathbf{b} \in \mathbb{R}^m\)</span> is
the column vector of constants. <span class="math inline">\(\mathbf{b}
\in \mathbb{R}^m\)</span> 是常数列向量。</li>
</ul>
<p>This formulation turns the problem of solving equations into
analyzing the action of a matrix.
这个公式将解方程的问题转化为分析矩阵的作用。</p>
<p>Example 3.1.1. The system 例 3.1.1. 系统</p>
<p><span class="math display">\[
\begin{cases}x + 2y = 5, \\3x - y = 4\end{cases}
\]</span></p>
<p>can be written as 可以写成</p>
<p><span class="math display">\[
\begin{bmatrix} 1 &amp; 2 \\ 3 &amp; -1 \end{bmatrix}\begin{bmatrix} x
\\ y \end{bmatrix}=\begin{bmatrix} 5 \\ 4 \end{bmatrix}.
\]</span></p>
<h3 id="types-of-solutions">Types of Solutions</h3>
<p>解决方案类型</p>
<p>A linear system may have: 线性系统可能有：</p>
<ol type="1">
<li>No solution (inconsistent): The equations conflict. Example:
无解（不一致）：方程式相互矛盾。例如：</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y = 1 \\x + y = 2\end{cases}
\]</span></p>
<p>This system has no solution. 这个系统没有解决方案。</p>
<ol start="2" type="1">
<li>Exactly one solution (unique): The system’s equations intersect at a
single point. 只有一个解（唯一）：系统方程在一个点相交。 Example: The
following coefficient matrix: 例如：以下系数矩阵：</li>
</ol>
<p><span class="math display">\[
\begin{bmatrix}1 &amp; 2 \\3 &amp; -1\end{bmatrix}
\]</span></p>
<p>has a unique solution. 有一个独特的解决方案。</p>
<ol start="3" type="1">
<li>Infinitely many solutions: The equations describe overlapping
constraints (e.g., multiple equations representing the same line or
plane).
无数个解：方程描述重叠的约束（例如，表示同一条线或平面的多个方程）。</li>
</ol>
<p>The nature of the solution depends on the rank of <span
class="math inline">\(A\)</span> and its relation to the augmented
matrix <span class="math inline">\((A|\mathbf{b})\)</span>, which we
will study later. 解的性质取决于 <span class="math inline">\(A\)</span>
的秩及其与增广矩阵 <span class="math inline">\((A|\mathbf{b})\)</span>
的关系，我们稍后会研究。</p>
<h3 id="geometric-interpretation-1">Geometric Interpretation</h3>
<p>几何解释</p>
<ul>
<li>In <span class="math inline">\(\mathbb{R}^2\)</span>, each linear
equation represents a line. Solving a system means finding intersection
points of lines. 在 <span class="math inline">\(\mathbb{R}^2\)</span>
中，每个线性方程代表一条直线。求解方程组意味着找到直线的交点。</li>
<li>In <span class="math inline">\(\mathbb{R}^3\)</span>, each equation
represents a plane. A system may have no solution (parallel planes), one
solution (a unique intersection point), or infinitely many (a line of
intersection). 在 <span class="math inline">\(\mathbb{R}^3\)</span>
中，每个方程代表一个平面。一个方程组可能没有解（平行平面），可能有一个解（唯一的交点），也可能有无数个解（一条交线）。</li>
<li>In higher dimensions, the picture generalizes: solutions form
intersections of hyperplanes.
在更高维度中，该图概括为：解决方案形成超平面的交点。</li>
</ul>
<h3 id="why-this-matters-7">Why this matters</h3>
<p>为什么这很重要</p>
<p>Linear systems are the practical foundation of linear algebra. They
appear in balancing chemical reactions, circuit analysis, least-squares
regression, optimization, and computer graphics. Understanding how to
represent and classify their solutions is the first step toward
systematic solution methods like Gaussian elimination.
线性系统是线性代数的实践基础。它们出现在平衡化学反应、电路分析、最小二乘回归、优化和计算机图形学中。了解如何表示和分类它们的解是迈向高斯消元法等系统求解方法的第一步。</p>
<h3 id="exercises-3.1">Exercises 3.1</h3>
<p>练习3.1</p>
<ol type="1">
<li>Write the following system in matrix form:
将以下系统写成矩阵形式：</li>
</ol>
<p><span class="math display">\[
\begin{cases}2x + 3y - z = 7, \\x - y + 4z = 1, \\3x + 2y + z =
5\end{cases}
\]</span></p>
<ol start="2" type="1">
<li>Determine whether the system 确定系统是否</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y = 1, \\2x + 2y = 2\end{cases}
\]</span></p>
<p>has no solution, one solution, or infinitely many solutions.
有无解、有一个解或有无数个解。</p>
<ol start="3" type="1">
<li>Geometrically interpret the system 几何解释系统</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y = 3, \\x - y = 1\end{cases}
\]</span></p>
<p>in the plane. 在飞机上。</p>
<ol start="4" type="1">
<li>Solve the system 解决系统</li>
</ol>
<p><span class="math display">\[
\begin{cases}2x + y = 1, \\x - y = 4\end{cases}
\]</span></p>
<p>and check your solution. 并检查您的解决方案。</p>
<ol start="5" type="1">
<li>In <span class="math inline">\(\mathbb{R}^3\)</span>, describe the
solution set of 在 <span class="math inline">\(\mathbb{R}^3\)</span>
中，描述</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y + z = 0, \\2x + 2y + 2z = 0\end{cases}
\]</span></p>
<p>What geometric object does it represent? 它代表什么几何对象？</p>
<h2 id="gaussian-elimination">3.2 Gaussian Elimination</h2>
<p>3.2 高斯消元法</p>
<p>To solve linear systems efficiently, we use Gaussian elimination: a
systematic method of transforming a system into a simpler equivalent one
whose solutions are easier to see. The method relies on elementary row
operations that preserve the solution set.
为了高效地求解线性方程组，我们使用高斯消元法：这是一种将方程组转化为更简单、更易解的等效方程的系统方法。该方法依赖于保留解集的基本行运算。</p>
<h3 id="elementary-row-operations">Elementary Row Operations</h3>
<p>初等行运算</p>
<p>On an augmented matrix <span
class="math inline">\((A|\mathbf{b})\)</span>, we are allowed three
operations: 对于增广矩阵 <span
class="math inline">\((A|\mathbf{b})\)</span>
，我们可以进行三种运算：</p>
<ol type="1">
<li>Row swapping: interchange two rows. 换行：交换两行。</li>
<li>Row scaling: multiply a row by a nonzero scalar.
行缩放：将一行乘以非零标量。</li>
<li>Row replacement: replace one row by itself plus a multiple of
another row. 行替换：用一行本身加上另一行的倍数来替换一行。</li>
</ol>
<p>These operations correspond to re-expressing equations in different
but equivalent forms. 这些运算对应于以不同但等效的形式重新表达方程。</p>
<h3 id="row-echelon-form">Row Echelon Form</h3>
<p>行梯队形式</p>
<p>A matrix is in row echelon form (REF) if:
如果满足以下条件，则矩阵为行阶梯形矩阵（REF）：</p>
<ol type="1">
<li>All nonzero rows are above any zero rows.
所有非零行均位于任何零行之上。</li>
<li>Each leading entry (the first nonzero number from the left in a row)
is to the right of the leading entry in the row above.
每个前导条目（一行中从左边开始的第一个非零数字）位于上一行前导条目的右侧。</li>
<li>All entries below a leading entry are zero.
前导条目下面的所有条目都为零。</li>
</ol>
<p>Further, if each leading entry is 1 and is the only nonzero entry in
its column, the matrix is in reduced row echelon form (RREF).
此外，如果每个前导项都是
1，并且是其列中唯一的非零项，则矩阵为简化行阶梯形式 (RREF)。</p>
<h3 id="algorithm-of-gaussian-elimination">Algorithm of Gaussian
Elimination</h3>
<p>高斯消元法</p>
<ol type="1">
<li>Write the augmented matrix for the system. 写出系统的增广矩阵。</li>
<li>Use row operations to create zeros below each pivot (the leading
entry in a row).
使用行运算在每个枢轴（一行中的前导条目）下方创建零。</li>
<li>Continue column by column until the matrix is in echelon form.
继续逐列进行，直到矩阵呈阶梯形式。</li>
<li>Solve by back substitution: starting from the last pivot equation
and working upward.
通过反向代入来求解：从最后一个枢轴方程开始向上求解。</li>
</ol>
<p>If we continue to RREF, the solution can be read off directly.
如果我们继续 RREF，则可以直接读出解决方案。</p>
<h3 id="example">Example</h3>
<p>例子</p>
<p>Example 3.2.1. Solve 例 3.2.1. 求解</p>
<p><span class="math display">\[
\begin{cases}x + 2y - z = 3, \\2x + y + z = 7, \\3x - y + 2z =
4.\end{cases}
\]</span></p>
<p>Step 1. Augmented matrix 步骤1.增广矩阵</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 2 &amp; -1 &amp; 3 \\2 &amp; 1 &amp; 1
&amp; 7 \\3 &amp; -1 &amp; 2 &amp; 4\end{array}\right].
\]</span></p>
<p>Step 2. Eliminate below the first pivot 步骤 2.
消除第一个枢轴以下</p>
<p>Subtract 2 times row 1 from row 2, and 3 times row 1 from row 3: 从第
2 行减去第 1 行的 2 倍，从第 3 行减去第 1 行的 3 倍：</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 2 &amp; -1 &amp; 3 \\0 &amp; -3 &amp;
3 &amp; 1 \\0 &amp; -7 &amp; 5 &amp; -5\end{array}\right].
\]</span></p>
<p>Step 3. Pivot in column 2 步骤 3. 在第 2 列中进行透视</p>
<p>Divide row 2 by -3: 将第 2 行除以 -3：</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 2 &amp; -1 &amp; 3 \\0 &amp; 1 &amp;
-1 &amp; -\tfrac{1}{3} \\0 &amp; -7 &amp; 5 &amp; -5\end{array}\right].
\]</span></p>
<p>Add 7 times row 2 to row 3: 将第 2 行的 7 倍加到第 3 行：</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 2 &amp; -1 &amp; 3 \\0 &amp; 1 &amp;
-1 &amp; -\tfrac{1}{3} \\0 &amp; 0 &amp; -2 &amp;
-\tfrac{22}{3}\end{array}\right].
\]</span></p>
<p>Step 4. Pivot in column 3 步骤 4. 在第 3 列中进行透视</p>
<p>Divide row 3 by -2: 将第 3 行除以 -2：</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 2 &amp; -1 &amp; 3 \\0 &amp; 1 &amp;
-1 &amp; -\tfrac{1}{3} \\0 &amp; 0 &amp; 1 &amp;
\tfrac{11}{3}\end{array}\right].
\]</span></p>
<p>Step 5. Back substitution 步骤 5. 回代</p>
<p>From the last row: 从最后一行开始：</p>
<p><span class="math display">\[
z = \tfrac{11}{3}.
\]</span></p>
<p>Second row: 第二行：</p>
<p><span class="math display">\[
y - z = -\tfrac{1}{3} \implies y = -\tfrac{1}{3} + \tfrac{11}{3} =
\tfrac{10}{3}.
\]</span></p>
<p>First row: 第一行：</p>
<p><span class="math display">\[
x + 2y - z = 3 \implies x + 2\cdot\tfrac{10}{3} - \tfrac{11}{3} = 3.
\]</span></p>
<p>So 所以</p>
<p><span class="math display">\[
x + \tfrac{20}{3} - \tfrac{11}{3} = 3 \implies x + 3 = 3 \implies x = 0.
\]</span></p>
<p>Solution: 解决方案：</p>
<p><span class="math display">\[
(x,y,z) = \big(0, \tfrac{10}{3}, \tfrac{11}{3}\big).
\]</span></p>
<h3 id="why-this-matters-8">Why this matters</h3>
<p>为什么这很重要</p>
<p>Gaussian elimination is the foundation of computational linear
algebra. It reduces complex systems to a form where solutions are
visible, and it forms the basis for algorithms used in numerical
analysis, scientific computing, and machine learning.
高斯消元法是计算线性代数的基础。它将复杂系统简化为可见解的形式，并构成数值分析、科学计算和机器学习中使用的算法的基础。</p>
<h3 id="exercises-3.2">Exercises 3.2</h3>
<p>练习 3.2</p>
<ol type="1">
<li>Solve by Gaussian elimination: 通过高斯消元法求解：</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y = 2, \\2x - y = 0.\end{cases}
\]</span></p>
<ol start="2" type="1">
<li>Reduce the following augmented matrix to REF: 将以下增广矩阵简化为
REF：</li>
</ol>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 1 &amp; 1 &amp; 6 \\2 &amp; -1 &amp; 3
&amp; 14 \\1 &amp; 4 &amp; -2 &amp; -2\end{array}\right].
\]</span></p>
<ol start="3" type="1">
<li>Show that Gaussian elimination always produces either:
证明高斯消元法总是产生以下结果：</li>
</ol>
<ul>
<li>a unique solution, 一个独特的解决方案，</li>
<li>infinitely many solutions, or 无穷多个解，或者</li>
<li>a contradiction (no solution). 矛盾（无解）。</li>
</ul>
<ol start="4" type="1">
<li>Use Gaussian elimination to find all solutions of
使用高斯消元法找到所有解</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y + z = 0, \\2x + y + z = 1.\end{cases}
\]</span></p>
<ol start="5" type="1">
<li>Explain why pivoting (choosing the largest available pivot element)
is useful in numerical computation.
解释为什么枢轴旋转（选择最大的可用枢轴元素）在数值计算中很有用。</li>
</ol>
<h2 id="rank-and-consistency">3.3 Rank and Consistency</h2>
<p>3.3 等级和一致性</p>
<p>Gaussian elimination not only provides solutions but also reveals the
structure of a linear system. Two key ideas are the rank of a matrix and
the consistency of a system. Rank measures the amount of independent
information in the equations, while consistency determines whether the
system has at least one solution.
高斯消元法不仅能提供解，还能揭示线性系统的结构。两个关键概念是矩阵的秩和系统的一致性。秩衡量方程中独立信息的数量，而一致性则决定系统是否至少有一个解。</p>
<h3 id="rank-of-a-matrix">Rank of a Matrix</h3>
<p>矩阵的秩</p>
<p>The rank of a matrix is the number of leading pivots in its row
echelon form. Equivalently, it is the maximum number of linearly
independent rows or columns.
矩阵的秩是其行阶梯形中前导主元的个数。换句话说，它是线性无关的行或列的最大数量。</p>
<p>Formally, 正式地，</p>
<p><span class="math display">\[
\text{rank}(A) = \dim(\text{row space of } A) = \dim(\text{column space
of } A).
\]</span></p>
<p>The rank tells us the effective dimension of the space spanned by the
rows (or columns). 秩告诉我们行（或列）所跨越的空间的有效维度。</p>
<p>Example 3.3.1. For 例 3.3.1. 对于</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 &amp; 3 \\2 &amp; 4 &amp; 6 \\3 &amp; 6
&amp; 9\end{bmatrix},
\]</span></p>
<p>row reduction gives 行减少给出</p>
<p><span class="math display">\[
\begin{bmatrix}1 &amp; 2 &amp; 3 \\0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp;
0\end{bmatrix}.
\]</span></p>
<p>Thus, <span class="math inline">\(\text{rank}(A) = 1\)</span>, since
all rows are multiples of the first. 因此， <span
class="math inline">\(\text{rank}(A) = 1\)</span>
，因为所有行都是第一行的倍数。</p>
<h3 id="consistency-of-linear-systems">Consistency of Linear
Systems</h3>
<p>线性系统的一致性</p>
<p>Consider the system <span class="math inline">\(A\mathbf{x} =
\mathbf{b}\)</span>. The system is consistent (has at least one
solution) if and only if 考虑系统 <span
class="math inline">\(A\mathbf{x} = \mathbf{b}\)</span>
。该系统是一致的（至少有一个解），当且仅当</p>
<p><span class="math display">\[
\text{rank}(A) = \text{rank}(A|\mathbf{b}),
\]</span></p>
<p>where <span class="math inline">\((A|\mathbf{b})\)</span> is the
augmented matrix. If the ranks differ, the system is inconsistent. 其中
<span class="math inline">\((A|\mathbf{b})\)</span>
是增广矩阵。如果秩不同，则系统不一致。</p>
<ul>
<li>If <span class="math inline">\(\text{rank}(A) =
\text{rank}(A|\mathbf{b}) = n\)</span> (number of unknowns), the system
has a unique solution. 如果 <span class="math inline">\(\text{rank}(A) =
\text{rank}(A|\mathbf{b}) = n\)</span>
（未知数），则系统有一个唯一的解。</li>
<li>If <span class="math inline">\(\text{rank}(A) =
\text{rank}(A|\mathbf{b}) &lt; n\)</span>, the system has infinitely
many solutions. 如果 <span class="math inline">\(\text{rank}(A) =
\text{rank}(A|\mathbf{b}) &lt; n\)</span> ，则系统有无数个解。</li>
</ul>
<h3 id="example-1">Example</h3>
<p>例子</p>
<p>Example 3.3.2. Consider 例 3.3.2. 考虑</p>
<p><span class="math display">\[
\begin{cases}x + y + z = 1, \\2x + 2y + 2z = 2, \\x + y + z =
3.\end{cases}
\]</span></p>
<p>The augmented matrix is 增广矩阵是</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 1 &amp; 1 &amp; 1 \\2 &amp; 2 &amp; 2
&amp; 2 \\1 &amp; 1 &amp; 1 &amp; 3\end{array}\right].
\]</span></p>
<p>Row reduction gives 行减少给出</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 1 &amp; 1 &amp; 1 \\0 &amp; 0 &amp; 0
&amp; 0 \\0 &amp; 0 &amp; 0 &amp; 2\end{array}\right].
\]</span></p>
<p>Here, <span class="math inline">\(\text{rank}(A) = 1\)</span>, but
<span class="math inline">\(\text{rank}(A|\mathbf{b}) = 2\)</span>.
Since the ranks differ, the system is inconsistent: no solution exists.
这里， <span class="math inline">\(\text{rank}(A) = 1\)</span> ，但
<span class="math inline">\(\text{rank}(A|\mathbf{b}) = 2\)</span>
。由于秩不同，系统不一致：不存在解。</p>
<h3 id="example-with-infinite-solutions">Example with Infinite
Solutions</h3>
<p>无限解的例子</p>
<p>Example 3.3.3. For 例 3.3.3. 对于</p>
<p><span class="math display">\[
\begin{cases}x + y = 2, \\2x + 2y = 4,\end{cases}
\]</span></p>
<p>the augmented matrix reduces to 增广矩阵简化为</p>
<p><span class="math display">\[
\left[\begin{array}{cc|c}1 &amp; 1 &amp; 2 \\0 &amp; 0 &amp;
0\end{array}\right].
\]</span></p>
<p>Here, <span class="math inline">\(\text{rank}(A) =
\text{rank}(A|\mathbf{b}) = 1 &lt; 2\)</span>. Thus, infinitely many
solutions exist, forming a line. 这里， <span
class="math inline">\(\text{rank}(A) = \text{rank}(A|\mathbf{b}) = 1
&lt; 2\)</span> 。因此，存在无数个解，形成一条线。</p>
<h3 id="why-this-matters-9">Why this matters</h3>
<p>为什么这很重要</p>
<p>Rank is a measure of independence: it tells us how many truly
distinct equations or directions are present. Consistency explains when
equations align versus when they contradict. These concepts connect
linear systems to vector spaces and prepare for the ideas of dimension,
basis, and the Rank–Nullity Theorem.
秩是独立性的度量：它告诉我们有多少个真正不同的方程或方向。一致性解释了方程何时一致，何时矛盾。这些概念将线性系统与向量空间联系起来，并为维度、基和秩零定理的概念做好准备。</p>
<h3 id="exercises-3.3">Exercises 3.3</h3>
<p>练习 3.3</p>
<ol type="1">
<li>Compute the rank of 计算</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 &amp; 1 \\0 &amp; 1 &amp; -1 \\2 &amp; 5
&amp; -1\end{bmatrix}.
\]</span></p>
<ol start="2" type="1">
<li>Determine whether the system 确定系统</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + y + z = 1, \\2x + 3y + z = 2, \\3x + 5y + 2z =
3\end{cases}
\]</span></p>
<p>is consistent. 是一致的。</p>
<ol start="3" type="1">
<li><p>Show that the rank of the identity matrix <span
class="math inline">\(I_n\)</span> is <span
class="math inline">\(n\)</span>. 证明单位矩阵𝐼的秩 𝑛 I n ​ 是 <span
class="math inline">\(n\)</span> 。</p></li>
<li><p>Give an example of a system in <span
class="math inline">\(\mathbb{R}^3\)</span> with infinitely many
solutions, and explain why it satisfies the rank condition. 给出 <span
class="math inline">\(\mathbb{R}^3\)</span>
中具有无穷多个解的系统的例子，并解释它为什么满足秩条件。</p></li>
<li><p>Prove that for any matrix <span class="math inline">\(A \in
\mathbb{R}^{m \times n}\)</span>, <span
class="math inline">\(\text{rank}(A) \leq \min(m,n).\)</span>
证明对于任意矩阵 <span class="math inline">\(A \in \mathbb{R}^{m \times
n}\)</span> ， <span class="math inline">\(\text{rank}(A) \leq
\min(m,n).\)</span></p></li>
</ol>
<h2 id="homogeneous-systems">3.4 Homogeneous Systems</h2>
<p>3.4 均质系统</p>
<p>A homogeneous system is a linear system in which all constant terms
are zero: 齐次系统是所有常数项都为零的线性系统：</p>
<p><span class="math display">\[
A\mathbf{x} = \mathbf{0},
\]</span></p>
<p>where <span class="math inline">\(A \in \mathbb{R}^{m \times
n}\)</span>, and <span class="math inline">\(\mathbf{0}\)</span> is the
zero vector in <span class="math inline">\(\mathbb{R}^m\)</span>. 其中
<span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> ，且
<span class="math inline">\(\mathbf{0}\)</span> 是 <span
class="math inline">\(\mathbb{R}^m\)</span> 中的零向量。</p>
<h3 id="the-trivial-solution">The Trivial Solution</h3>
<p>简单的解决方案</p>
<p>Every homogeneous system has at least one solution:
每个同质系统至少有一个解：</p>
<p><span class="math display">\[
\mathbf{x} = \mathbf{0}.
\]</span></p>
<p>This is called the trivial solution. The interesting question is
whether <em>nontrivial solutions</em> (nonzero vectors) exist.
这被称为平凡解。有趣的问题是是否存在<em>非平凡解</em> （非零向量）。</p>
<h3 id="existence-of-nontrivial-solutions">Existence of Nontrivial
Solutions</h3>
<p>非平凡解的存在性</p>
<p>Nontrivial solutions exist precisely when the number of unknowns
exceeds the rank of the coefficient matrix:
当未知数的数量超过系数矩阵的秩时，就会存在非平凡解：</p>
<p><span class="math display">\[
\text{rank}(A) &lt; n.
\]</span></p>
<p>In this case, there are infinitely many solutions, forming a subspace
of <span class="math inline">\(\mathbb{R}^n\)</span>. The dimension of
this solution space is 在这种情况下，有无穷多个解，形成一个 <span
class="math inline">\(\mathbb{R}^n\)</span>
的子空间。这个解空间的维度是</p>
<p><span class="math display">\[
\dim(\text{null}(A)) = n - \text{rank}(A),
\]</span></p>
<p>where null(A) is the set of all solutions to <span
class="math inline">\(A\mathbf{x} = 0\)</span>. This set is called the
null space or kernel of <span class="math inline">\(A\)</span>. 其中
null(A) 是 <span class="math inline">\(A\mathbf{x} = 0\)</span>
所有解的集合。该集合称为 <span class="math inline">\(A\)</span>
的零空间或零核。</p>
<h3 id="example-2">Example</h3>
<p>例子</p>
<p>Example 3.4.1. Consider 例 3.4.1. 考虑</p>
<p><span class="math display">\[
\begin{cases}x + y + z = 0, \\2x + y - z = 0.\end{cases}
\]</span></p>
<p>The augmented matrix is 增广矩阵是</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 1 &amp; 1 &amp; 0 \\2 &amp; 1 &amp; -1
&amp; 0\end{array}\right].
\]</span></p>
<p>Row reduction: 行减少：</p>
<p><span class="math display">\[
\left[\begin{array}{ccc|c}1 &amp; 1 &amp; 1 &amp; 0 \\0 &amp; -1 &amp;
-3 &amp; 0\end{array}\right]\quad\to\quad\left[\begin{array}{ccc|c}1
&amp; 1 &amp; 1 &amp; 0 \\0 &amp; 1 &amp; 3 &amp; 0\end{array}\right].
\]</span></p>
<p>So the system is equivalent to: 因此该系统等同于：</p>
<p><span class="math display">\[
\begin{cases}x + y + z = 0, \\y + 3z = 0.\end{cases}
\]</span></p>
<p>From the second equation, <span class="math inline">\(y =
-3z\)</span>. Substituting into the first: <span class="math inline">\(x
- 3z + z = 0 \implies x = 2z.\)</span> 从第二个方程得出 <span
class="math inline">\(y = -3z\)</span> 。代入第一个方程： <span
class="math inline">\(x - 3z + z = 0 \implies x = 2z.\)</span></p>
<p>Thus solutions are: 因此解决方案是：</p>
<p><span class="math display">\[
(x,y,z) = z(2, -3, 1), \quad z \in \mathbb{R}.
\]</span></p>
<p>The null space is the line spanned by the vector <span
class="math inline">\((2, -3, 1)\)</span>. 零空间是向量 <span
class="math inline">\((2, -3, 1)\)</span> 所跨越的线。</p>
<h3 id="geometric-interpretation-2">Geometric Interpretation</h3>
<p>几何解释</p>
<p>The solution set of a homogeneous system is always a subspace of
<span class="math inline">\(\mathbb{R}^n\)</span>. 同质系统的解集始终是
<span class="math inline">\(\mathbb{R}^n\)</span> 的子空间。</p>
<ul>
<li>If <span class="math inline">\(\text{rank}(A) = n\)</span>, the only
solution is the zero vector. 如果为 <span
class="math inline">\(\text{rank}(A) = n\)</span>
，则唯一的解就是零向量。</li>
<li>If <span class="math inline">\(\text{rank}(A) = n-1\)</span>, the
solution set is a line through the origin. 如果为 <span
class="math inline">\(\text{rank}(A) = n-1\)</span>
，则解集是一条过原点的线。</li>
<li>If <span class="math inline">\(\text{rank}(A) = n-2\)</span>, the
solution set is a plane through the origin. 如果为 <span
class="math inline">\(\text{rank}(A) = n-2\)</span>
，则解集是通过原点的平面。</li>
</ul>
<p>More generally, the null space has dimension <span
class="math inline">\(n - \text{rank}(A)\)</span>, known as the nullity.
更一般地，零空间的维度为 <span class="math inline">\(n -
\text{rank}(A)\)</span> ，称为零度。</p>
<h3 id="why-this-matters-10">Why this matters</h3>
<p>为什么这很重要</p>
<p>Homogeneous systems are central to understanding vector spaces,
subspaces, and dimension. They lead directly to the concepts of kernel,
null space, and linear dependence. In applications, homogeneous systems
appear in equilibrium problems, eigenvalue equations, and computer
graphics transformations.
齐次系统是理解向量空间、子空间和维度的核心。它们直接引出核、零空间和线性相关性的概念。在实际应用中，齐次系统出现在平衡问题、特征值方程和计算机图形变换中。</p>
<h3 id="exercises-3.4">Exercises 3.4</h3>
<p>练习 3.4</p>
<ol type="1">
<li>Solve the homogeneous system 解决均质系统</li>
</ol>
<p><span class="math display">\[
\begin{cases}x + 2y - z = 0, \\2x + 4y - 2z = 0.\end{cases}
\]</span></p>
<p>What is the dimension of its solution space?
其解空间的维数是多少？</p>
<ol start="2" type="1">
<li>Find all solutions of 找到所有解决方案</li>
</ol>
<p><span class="math display">\[
\begin{cases}x - y + z = 0, \\2x + y - z = 0.\end{cases}
\]</span></p>
<ol start="3" type="1">
<li><p>Show that the solution set of any homogeneous system is a
subspace of <span class="math inline">\(\mathbb{R}^n\)</span>.
证明任何同质系统的解集都是 <span
class="math inline">\(\mathbb{R}^n\)</span> 的子空间。</p></li>
<li><p>Suppose <span class="math inline">\(A\)</span> is a <span
class="math inline">\(3 \\times 3\)</span>matrix with<span
class="math inline">\(\\text{rank}(A) = 2\)</span>. What is the
dimension of the null space of <span class="math inline">\(A\)</span>?
假设 <span class="math inline">\(A\)</span> 是 $3 \times 3 <span
class="math inline">\(matrix with\)</span> \text{rank}(A) = 2 $. What is
the dimension of the null space of $ A$？</p></li>
<li><p>For 为了</p></li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 2 &amp; -1 \\ 0 &amp; 1 &amp; 3
\end{bmatrix},
\]</span></p>
<p>compute a basis for the null space of <span
class="math inline">\(A\)</span>. 计算 <span
class="math inline">\(A\)</span> 的零空间的基础。</p>
<h1 id="chapter-4.-vector-spaces">Chapter 4. Vector Spaces</h1>
<p>第 4 章 向量空间</p>
<h2 id="definition-of-a-vector-space">4.1 Definition of a Vector
Space</h2>
<p>4.1 向量空间的定义</p>
<p>Up to now we have studied vectors and matrices concretely in <span
class="math inline">\(\mathbb{R}^n\)</span>. The next step is to move
beyond coordinates and define vector spaces in full generality. A vector
space is an abstract setting where the familiar rules of addition and
scalar multiplication hold, regardless of whether the elements are
geometric vectors, polynomials, functions, or other objects.
到目前为止，我们已经在 <span class="math inline">\(\mathbb{R}^n\)</span>
中具体学习了向量和矩阵。下一步是超越坐标，全面定义向量空间。向量空间是一个抽象的场景，其中熟悉的加法和标量乘法规则始终成立，无论元素是几何向量、多项式、函数还是其他对象。</p>
<h3 id="formal-definition-1">Formal Definition</h3>
<p>正式定义</p>
<p>A vector space over the real numbers <span
class="math inline">\(\mathbb{R}\)</span> is a set <span
class="math inline">\(V\)</span> equipped with two operations: 实数
<span class="math inline">\(\mathbb{R}\)</span>
上的向量空间是具有两个运算的集合 <span class="math inline">\(V\)</span>
：</p>
<ol type="1">
<li>Vector addition: For any <span class="math inline">\(\mathbf{u},
\mathbf{v} \in V\)</span>, there is a vector <span
class="math inline">\(\mathbf{u} + \mathbf{v} \in V\)</span>.
向量加法：对于任何 <span class="math inline">\(\mathbf{u}, \mathbf{v}
\in V\)</span> ，都有向量 <span class="math inline">\(\mathbf{u} +
\mathbf{v} \in V\)</span> 。</li>
<li>Scalar multiplication: For any scalar <span class="math inline">\(c
\in \mathbb{R}\)</span> and any <span class="math inline">\(\mathbf{v}
\in V\)</span>, there is a vector <span
class="math inline">\(c\mathbf{v} \in V\)</span>. 标量乘法：对于任何标量
<span class="math inline">\(c \in \mathbb{R}\)</span> 和任何 <span
class="math inline">\(\mathbf{v} \in V\)</span> ，都有一个向量 <span
class="math inline">\(c\mathbf{v} \in V\)</span> 。</li>
</ol>
<p>These operations must satisfy the following axioms (for all <span
class="math inline">\(\mathbf{u}, \mathbf{v}, \mathbf{w} \in V\)</span>
and all scalars <span class="math inline">\(a,b \in
\mathbb{R}\)</span>): 这些运算必须满足以下公理（对于所有 <span
class="math inline">\(\mathbf{u}, \mathbf{v}, \mathbf{w} \in V\)</span>
和所有标量 <span class="math inline">\(a,b \in \mathbb{R}\)</span>
）：</p>
<ol type="1">
<li>Commutativity of addition: <span class="math inline">\(\mathbf{u} +
\mathbf{v} = \mathbf{v} + \mathbf{u}\)</span>. 加法的交换性： <span
class="math inline">\(\mathbf{u} + \mathbf{v} = \mathbf{v} +
\mathbf{u}\)</span> 。</li>
<li>Associativity of addition: <span class="math inline">\((\mathbf{u} +
\mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} +
\mathbf{w})\)</span>. 加法的结合性： <span
class="math inline">\((\mathbf{u} + \mathbf{v}) + \mathbf{w} =
\mathbf{u} + (\mathbf{v} + \mathbf{w})\)</span> 。</li>
<li>Additive identity: There exists a zero vector <span
class="math inline">\(\mathbf{0} \in V\)</span> such that <span
class="math inline">\(\mathbf{v} + \mathbf{0} = \mathbf{v}\)</span>.
加法恒等式：存在零向量 <span class="math inline">\(\mathbf{0} \in
V\)</span> 使得 <span class="math inline">\(\mathbf{v} + \mathbf{0} =
\mathbf{v}\)</span> 。</li>
<li>Additive inverses: For each <span class="math inline">\(\mathbf{v}
\in V\)</span>, there exists <span class="math inline">\((-\mathbf{v}
\in V\)</span> such that <span class="math inline">\(\mathbf{v} +
(-\mathbf{v}) = \mathbf{0}\)</span>. 加法逆元：对于每个 <span
class="math inline">\(\mathbf{v} \in V\)</span> ，存在 <span
class="math inline">\((-\mathbf{v} \in V\)</span> 使得 <span
class="math inline">\(\mathbf{v} + (-\mathbf{v}) = \mathbf{0}\)</span>
。</li>
<li>Compatibility of scalar multiplication: <span
class="math inline">\(a(b\mathbf{v}) = (ab)\mathbf{v}\)</span>.
标量乘法的兼容性： <span class="math inline">\(a(b\mathbf{v}) =
(ab)\mathbf{v}\)</span> 。</li>
<li>Identity element of scalars: 1⋅v=v. 标量的标识元： 1⋅v=v 。</li>
<li>Distributivity over vector addition: <span
class="math inline">\(a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} +
a\mathbf{v}\)</span>. 向量加法的分配律： <span
class="math inline">\(a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} +
a\mathbf{v}\)</span> 。</li>
<li>Distributivity over scalar addition: <span
class="math inline">\((a+b)\mathbf{v} = a\mathbf{v} +
b\mathbf{v}\)</span>. 标量加法的分配律： <span
class="math inline">\((a+b)\mathbf{v} = a\mathbf{v} +
b\mathbf{v}\)</span> 。</li>
</ol>
<p>If a set <span class="math inline">\(V\)</span> with operations
satisfies all eight axioms, we call it a vector space. 如果一个集合
<span class="math inline">\(V\)</span>
满足所有八个公理，我们称它为向量空间。</p>
<h3 id="examples-1">Examples</h3>
<p>示例</p>
<p>Example 4.1.1. Standard Euclidean space <span
class="math inline">\(\mathbb{R}^n\)</span> with ordinary addition and
scalar multiplication is a vector space. This is the model case from
which the axioms are abstracted. 例 4.1.1. 标准欧几里得空间 <span
class="math inline">\(\mathbb{R}^n\)</span>
进行普通的加法和标量乘法运算后，是一个向量空间。这是抽象出公理的典型例子。</p>
<p>Example 4.1.2. Polynomials The set of all polynomials with real
coefficients, denoted <span
class="math inline">\(\mathbb{R}[x]\)</span>, forms a vector space.
Addition and scalar multiplication are defined term by term. 例 4.1.2.
多项式 所有实系数多项式的集合，记为 <span
class="math inline">\(\mathbb{R}[x]\)</span>
，构成一个向量空间。加法和标量乘法是逐项定义的。</p>
<p>Example 4.1.3. Functions The set of all real-valued functions on an
interval, e.g. <span class="math inline">\(f: [0,1] \to
\mathbb{R}\)</span>, forms a vector space, since functions can be added
and scaled pointwise. 例 4.1.3. 函数 区间上的所有实值函数的集合，例如
<span class="math inline">\(f: [0,1] \to \mathbb{R}\)</span>
，形成一个向量空间，因为函数可以逐点添加和缩放。</p>
<h3 id="non-examples">Non-Examples</h3>
<p>非示例</p>
<p>Not every set with operations qualifies. For instance, the set of
positive real numbers under usual addition is not a vector space,
because additive inverses (negative numbers) are missing. The axioms
must all hold.
并非所有包含运算的集合都符合条件。例如，通常加法运算下的正实数集不是向量空间，因为缺少加法逆元（负数）。公理必须全部成立。</p>
<h3 id="geometric-interpretation-3">Geometric Interpretation</h3>
<p>几何解释</p>
<p>In familiar cases like <span
class="math inline">\(\mathbb{R}^2\)</span> or <span
class="math inline">\(\mathbb{R}^3\)</span>, vector spaces provide the
stage for geometry: vectors can be added, scaled, and combined to form
lines, planes, and higher-dimensional structures. In abstract settings
like function spaces, the same algebraic rules let us apply geometric
intuition to infinite-dimensional problems. 在像 <span
class="math inline">\(\mathbb{R}^2\)</span> 或 <span
class="math inline">\(\mathbb{R}^3\)</span>
这样常见的情形下，向量空间为几何学提供了舞台：向量可以相加、缩放和组合，从而形成线、平面和更高维度的结构。在像函数空间这样的抽象环境中，同样的代数规则让我们能够将几何直觉应用于无限维问题。</p>
<h3 id="why-this-matters-11">Why this matters</h3>
<p>为什么这很重要</p>
<p>The concept of vector space unifies seemingly different mathematical
objects under a single framework. Whether dealing with forces in
physics, signals in engineering, or data in machine learning, the common
language of vector spaces allows us to use the same techniques
everywhere.
向量空间的概念将看似不同的数学对象统一在一个框架下。无论是处理物理学中的力、工程学中的信号，还是机器学习中的数据，向量空间的通用语言使我们能够在任何地方使用相同的技术。</p>
<h3 id="exercises-4.1">Exercises 4.1</h3>
<p>练习4.1</p>
<ol type="1">
<li>Verify that <span class="math inline">\(\mathbb{R}^2\)</span> with
standard addition and scalar multiplication satisfies all eight vector
space axioms. 验证 <span class="math inline">\(\mathbb{R}^2\)</span>
通过标准加法和标量乘法满足所有八个向量空间公理。</li>
<li>Show that the set of integers <span
class="math inline">\(\mathbb{Z}\)</span> with ordinary operations is
not a vector space over <span class="math inline">\(\mathbb{R}\)</span>.
Which axiom fails? 证明：具有普通运算的整数集 <span
class="math inline">\(\mathbb{Z}\)</span> 不是 <span
class="math inline">\(\mathbb{R}\)</span>
上的向量空间。哪条公理不成立？</li>
<li>Consider the set of all polynomials of degree at most 3. Show it
forms a vector space over <span
class="math inline">\(\mathbb{R}\)</span>. What is its dimension?
考虑所有次数最多为3的多项式的集合。证明它构成一个 <span
class="math inline">\(\mathbb{R}\)</span>
上的向量空间。它的维度是多少？</li>
<li>Give an example of a vector space where the vectors are not
geometric objects. 给出一个向量空间的例子，其中的向量不是几何对象。</li>
<li>Prove that in any vector space, the zero vector is unique.
证明在任何向量空间中，零向量都是唯一的。</li>
</ol>
<h2 id="subspaces">4.2 Subspaces</h2>
<p>4.2 子空间</p>
<p>A subspace is a smaller vector space living inside a larger one. Just
as lines and planes naturally sit inside three-dimensional space,
subspaces generalize these ideas to higher dimensions and more abstract
settings.
子空间是位于较大向量空间中的较小向量空间。正如线和平面自然地存在于三维空间中一样，子空间将这些概念推广到更高维度和更抽象的场景。</p>
<h3 id="definition-1">Definition</h3>
<p>定义</p>
<p>Let <span class="math inline">\(V\)</span> be a vector space. A
subset <span class="math inline">\(W \subseteq V\)</span> is called a
subspace of <span class="math inline">\(V\)</span> if: 令 <span
class="math inline">\(V\)</span> 为向量空间。若满足以下条件，则子集
<span class="math inline">\(W \subseteq V\)</span> 称为 <span
class="math inline">\(V\)</span> 的子空间：</p>
<ol type="1">
<li><span class="math inline">\(\mathbf{0} \in W\)</span> (contains the
zero vector), <span class="math inline">\(\mathbf{0} \in W\)</span>
（包含零向量），</li>
<li>For all <span class="math inline">\(\mathbf{u}, \mathbf{v} \in
W\)</span>, the sum <span class="math inline">\(\mathbf{u} + \mathbf{v}
\in W\)</span> (closed under addition), 对于所有 <span
class="math inline">\(\mathbf{u}, \mathbf{v} \in W\)</span> ，总和为
<span class="math inline">\(\mathbf{u} + \mathbf{v} \in W\)</span>
（加法闭包），</li>
<li>For all scalars <span class="math inline">\(c \in
\mathbb{R}\)</span> and vectors <span class="math inline">\(\mathbf{v}
\in W\)</span>, the product <span class="math inline">\(c\mathbf{v} \in
W\)</span> (closed under scalar multiplication). 对于所有标量 <span
class="math inline">\(c \in \mathbb{R}\)</span> 和向量 <span
class="math inline">\(\mathbf{v} \in W\)</span> ，乘积 <span
class="math inline">\(c\mathbf{v} \in W\)</span>
（在标量乘法下封闭）。</li>
</ol>
<p>If these hold, then <span class="math inline">\(W\)</span> is itself
a vector space with the inherited operations. 如果这些成立，那么 <span
class="math inline">\(W\)</span> 本身就是具有继承操作的向量空间。</p>
<h3 id="examples-2">Examples</h3>
<p>示例</p>
<p>Example 4.2.1. Line through the origin in <span
class="math inline">\(\mathbb{R}^2\)</span> The set 例 4.2.1. 穿过 <span
class="math inline">\(\mathbb{R}^2\)</span> 中的原点的线 该套装</p>
<p><span class="math display">\[
W = \{ (t, 2t) \mid t \in \mathbb{R} \}
\]</span></p>
<p>is a subspace of <span class="math inline">\(\mathbb{R}^2\)</span>.
It contains the zero vector, is closed under addition, and is closed
under scalar multiplication. 是 <span
class="math inline">\(\mathbb{R}^2\)</span>
的一个子空间。它包含零向量，在加法运算下封闭，在标量乘法运算下封闭。</p>
<p>Example 4.2.2. The x–y plane in <span
class="math inline">\(\mathbb{R}^3\)</span> The set 例 4.2.2. <span
class="math inline">\(\mathbb{R}^3\)</span> 中的 x-y 平面 该套装</p>
<p><span class="math display">\[
W = \{ (x, y, 0) \mid x,y \in \mathbb{R} \}
\]</span></p>
<p>is a subspace of <span class="math inline">\(\mathbb{R}^3\)</span>.
It is the collection of all vectors lying in the plane through the
origin parallel to the x–y plane. 是 <span
class="math inline">\(\mathbb{R}^3\)</span>
的一个子空间。它是位于通过原点并平行于 x-y
平面的平面内的所有向量的集合。</p>
<p>Example 4.2.3. Null space of a matrix For a matrix <span
class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, the null
space 例 4.2.3. 矩阵的零空间 对于矩阵 <span class="math inline">\(A \in
\mathbb{R}^{m \times n}\)</span> ，零空间</p>
<p><span class="math display">\[
\{ \mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = \mathbf{0} \}
\]</span></p>
<p>is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>.
This subspace represents all solutions to the homogeneous system. 是
<span class="math inline">\(\mathbb{R}^n\)</span>
的一个子空间。该子空间表示齐次系统的所有解。</p>
<h3 id="non-examples-1">Non-Examples</h3>
<p>非示例</p>
<p>Not every subset is a subspace. 并非每个子集都是子空间。</p>
<ul>
<li>The set <span class="math inline">\({ (x,y) \in \mathbb{R}^2 \mid x
\geq 0 }\)</span> is not a subspace: it is not closed under scalar
multiplication (a negative scalar breaks the condition). 集合 <span
class="math inline">\({ (x,y) \in \mathbb{R}^2 \mid x \geq 0 }\)</span>
不是子空间：它在标量乘法下不封闭（负标量会破坏该条件）。</li>
<li>Any line in <span class="math inline">\(\mathbb{R}^2\)</span> that
does not pass through the origin is not a subspace, because it does not
contain <span class="math inline">\(\mathbf{0}\)</span>. <span
class="math inline">\(\mathbb{R}^2\)</span>
中任何不经过原点的线都不是子空间，因为它不包含 <span
class="math inline">\(\mathbf{0}\)</span> 。</li>
</ul>
<h3 id="geometric-interpretation-4">Geometric Interpretation</h3>
<p>几何解释</p>
<p>Subspaces are the linear structures inside vector spaces.
子空间是向量空间内的线性结构。</p>
<ul>
<li>In <span class="math inline">\(\mathbb{R}^2\)</span>, the subspaces
are: the zero vector, any line through the origin, or the entire plane.
在 <span class="math inline">\(\mathbb{R}^2\)</span>
中，子空间是：零向量、过原点的任意直线或整个平面。</li>
<li>In <span class="math inline">\(\mathbb{R}^3\)</span>, the subspaces
are: the zero vector, any line through the origin, any plane through the
origin, or the entire space. 在 <span
class="math inline">\(\mathbb{R}^3\)</span>
中，子空间是：零向量、过原点的任意直线、过原点的任意平面或整个空间。</li>
<li>In higher dimensions, the same principle applies: subspaces are the
flat linear pieces through the origin.
在更高的维度中，同样的原理适用：子空间是通过原点的平坦线性部分。</li>
</ul>
<h3 id="why-this-matters-12">Why this matters</h3>
<p>为什么这很重要</p>
<p>Subspaces capture the essential structure of linear problems. Column
spaces, row spaces, and null spaces are all subspaces. Much of linear
algebra consists of understanding how these subspaces intersect, span,
and complement each other.
子空间捕捉了线性问题的本质结构。列空间、行空间和零空间都是子空间。线性代数的大部分内容都在于理解这些子空间如何相互交叉、延伸和互补。</p>
<h3 id="exercises-4.2">Exercises 4.2</h3>
<p>练习 4.2</p>
<ol type="1">
<li>Prove that the set <span class="math inline">\(W = { (x,0) \mid x
\in \mathbb{R} } \subseteq \mathbb{R}^2\)</span> is a subspace. 证明集合
<span class="math inline">\(W = { (x,0) \mid x \in \mathbb{R} }
\subseteq \mathbb{R}^2\)</span> 是一个子空间。</li>
<li>Show that the line <span class="math inline">\({ (1+t, 2t) \mid t
\in \mathbb{R} }\)</span> is not a subspace of <span
class="math inline">\(\mathbb{R}^2\)</span>. Which condition fails?
证明行 <span class="math inline">\({ (1+t, 2t) \mid t \in \mathbb{R}
}\)</span> 不是 <span class="math inline">\(\mathbb{R}^2\)</span>
的子空间。哪个条件不成立？</li>
<li>Determine whether the set of all vectors <span
class="math inline">\((x,y,z) \in \mathbb{R}^3\)</span> satisfying <span
class="math inline">\(x+y+z=0\)</span> is a subspace. 确定满足 <span
class="math inline">\(x+y+z=0\)</span> 的所有向量 <span
class="math inline">\((x,y,z) \in \mathbb{R}^3\)</span>
的集合是否为子空间。</li>
<li>For the matrix 对于矩阵</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 &amp; 3 \\4 &amp; 5 &amp; 6\end{bmatrix}
\]</span></p>
<p>Describe the null space of <span class="math inline">\(A\)</span> as
a subspace of <span class="math inline">\(\mathbb{R}^3\)</span>. 将
<span class="math inline">\(A\)</span> 的零空间描述为 <span
class="math inline">\(\mathbb{R}^3\)</span> 的子空间。</p>
<ol start="5" type="1">
<li>List all possible subspaces of <span
class="math inline">\(\mathbb{R}^2\)</span>. 列出 <span
class="math inline">\(\mathbb{R}^2\)</span> 所有可能的子空间。</li>
</ol>
<h2 id="span-basis-dimension">4.3 Span, Basis, Dimension</h2>
<p>4.3 跨度、基、维度</p>
<p>The ideas of span, basis, and dimension provide the language for
describing the size and structure of subspaces. Together, they tell us
how a vector space is generated, how many building blocks it requires,
and how those blocks can be chosen.
跨度、基和维数的概念提供了描述子空间大小和结构的语言。它们共同告诉我们向量空间是如何生成的，它需要多少个构建块，以及如何选择这些构建块。</p>
<h3 id="span">Span</h3>
<p>跨度</p>
<p>Given a set of vectors <span class="math inline">\({\mathbf{v}_1,
\mathbf{v}_2, \dots, \mathbf{v}_k} \subseteq V\)</span>, the span is the
collection of all linear combinations: 给定一组向量 <span
class="math inline">\({\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k}
\subseteq V\)</span> ，跨度是所有线性组合的集合：</p>
<p><span class="math display">\[
\text{span}\{\mathbf{v}_1, \dots, \mathbf{v}_k\} = \{ c_1\mathbf{v}_1 +
\cdots + c_k\mathbf{v}_k \mid c_i \in \mathbb{R} \}.
\]</span></p>
<p>The span is always a subspace of <span
class="math inline">\(V\)</span>, namely the smallest subspace
containing those vectors. 跨度始终是 <span
class="math inline">\(V\)</span>
的子空间，即包含这些向量的最小子空间。</p>
<p>Example 4.3.1. In <span class="math inline">\(\mathbb{R}^2\)</span>,
$ = {(x,0) x },$ the x-axis. Similarly, <span
class="math inline">\(\text{span}\{(1,0),(0,1)\} =
\mathbb{R}^2.\)</span> 例 4.3.1。 在 <span
class="math inline">\(\mathbb{R}^2\)</span> 中， $ = {(x,0) x },$ x
轴。同样， <span class="math inline">\(\text{span}\{(1,0),(0,1)\} =
\mathbb{R}^2.\)</span></p>
<h3 id="basis">Basis</h3>
<p>基础</p>
<p>A basis of a vector space <span class="math inline">\(V\)</span> is a
set of vectors that: 向量空间 <span class="math inline">\(V\)</span>
的基是一组向量，其：</p>
<ol type="1">
<li>Span <span class="math inline">\(V\)</span>. 跨度 <span
class="math inline">\(V\)</span> 。</li>
<li>Are linearly independent (no vector in the set is a linear
combination of the others).
是线性独立的（集合中没有向量是其他向量的线性组合）。</li>
</ol>
<p>If either condition fails, the set is not a basis.
如果任一条件不成立，则该集合不作为基础。</p>
<p>Example 4.3.2. In <span class="math inline">\(\mathbb{R}^3\)</span>,
the standard unit vectors 例 4.3.2。 在 <span
class="math inline">\(\mathbb{R}^3\)</span> 中，标准单位向量</p>
<p><span class="math display">\[
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3
= (0,0,1)
\]</span></p>
<p>form a basis. Every vector <span
class="math inline">\((x,y,z)\)</span> can be uniquely written as
构成基础。每个向量 <span class="math inline">\((x,y,z)\)</span>
都可以唯一地写成</p>
<p><span class="math display">\[
x\mathbf{e}_1 + y\mathbf{e}_2 + z\mathbf{e}_3.
\]</span></p>
<h3 id="dimension">Dimension</h3>
<p>方面</p>
<p>The dimension of a vector space <span
class="math inline">\(V\)</span>, written <span
class="math inline">\(\dim(V)\)</span>, is the number of vectors in any
basis of <span class="math inline">\(V\)</span>. This number is
well-defined: all bases of a vector space have the same cardinality.
向量空间 <span class="math inline">\(V\)</span> 的维数，记作 <span
class="math inline">\(\dim(V)\)</span> ，是任意 <span
class="math inline">\(V\)</span>
的基中向量的数量。这个维数定义明确：向量空间的所有基都具有相同的基数。</p>
<p>Examples 4.3.3. 示例 4.3.3。</p>
<ul>
<li><span class="math inline">\(\dim(\mathbb{R}^2) = 2\)</span>, with
basis <span class="math inline">\((1,0), (0,1)\)</span>. <span
class="math inline">\(\dim(\mathbb{R}^2) = 2\)</span> ，依据是 <span
class="math inline">\((1,0), (0,1)\)</span> 。</li>
<li><span class="math inline">\(\dim(\mathbb{R}^3) = 3\)</span>, with
basis <span class="math inline">\((1,0,0), (0,1,0), (0,0,1)\)</span>.
<span class="math inline">\(\dim(\mathbb{R}^3) = 3\)</span> ，依据是
<span class="math inline">\((1,0,0), (0,1,0), (0,0,1)\)</span> 。</li>
<li>The set of polynomials of degree at most 3 has dimension 4, with
basis <span class="math inline">\((1, x, x^2, x^3)\)</span>. 次数最多为
3 的多项式集的维度为 4，基为 <span class="math inline">\((1, x, x^2,
x^3)\)</span> 。</li>
</ul>
<h3 id="geometric-interpretation-5">Geometric Interpretation</h3>
<p>几何解释</p>
<ul>
<li>The span is like the reach of a set of vectors.
跨度就像一组向量的范围。</li>
<li>A basis is the minimal set of directions needed to reach everything
in the space. 基础是到达空间中所有事物所需的最小方向集。</li>
<li>The dimension is the count of those independent directions.
维度是这些独立方向的数量。</li>
</ul>
<p>Lines, planes, and higher-dimensional flats can all be described in
terms of span, basis, and dimension.
线、平面和高维平面都可以用跨度、基和维度来描述。</p>
<h3 id="why-this-matters-13">Why this matters</h3>
<p>为什么这很重要</p>
<p>These concepts classify vector spaces and subspaces in terms of size
and structure. Many theorems in linear algebra-such as the Rank–Nullity
Theorem-are consequences of understanding span, basis, and dimension. In
practical terms, bases are how we encode data in coordinates, and
dimension tells us how much freedom a system truly has.
这些概念根据大小和结构对向量空间和子空间进行分类。线性代数中的许多定理，例如秩零定理，都是理解跨度、基和维数的结果。实际上，基是我们在坐标系中编码数据的方式，而维数则告诉我们一个系统真正拥有多少自由度。</p>
<h3 id="exercises-4.3">Exercises 4.3</h3>
<p>练习 4.3</p>
<ol type="1">
<li>Show that <span class="math inline">\((1,0,0)\)</span>, <span
class="math inline">\((0,1,0)\)</span>, <span
class="math inline">\((1,1,0)\)</span> span the <span
class="math inline">\(xy\)</span>-plane in <span
class="math inline">\(\mathbb{R}^3\)</span>. Are they a basis? 证明
<span class="math inline">\((1,0,0)\)</span> , <span
class="math inline">\((0,1,0)\)</span> , <span
class="math inline">\((1,1,0)\)</span> 在 <span
class="math inline">\(\mathbb{R}^3\)</span> 中跨越 <span
class="math inline">\(xy\)</span> -平面。它们是基吗？</li>
<li>Find a basis for the line <span class="math inline">\(\{(2t,-3t,t) :
t \in \mathbb{R}\}\)</span> in <span
class="math inline">\(\mathbb{R}^3\)</span>. 找出 <span
class="math inline">\(\mathbb{R}^3\)</span> 中第 <span
class="math inline">\(\{(2t,-3t,t) : t \in \mathbb{R}\}\)</span>
行的依据。</li>
<li>Determine the dimension of the subspace of <span
class="math inline">\(\mathbb{R}^3\)</span> defined by <span
class="math inline">\(x+y+z=0\)</span>. 确定由 <span
class="math inline">\(x+y+z=0\)</span> 定义的 <span
class="math inline">\(\mathbb{R}^3\)</span> 子空间的维数。</li>
<li>Prove that any two different bases of <span
class="math inline">\(\mathbb{R}^n\)</span> must contain exactly <span
class="math inline">\(n\)</span> vectors. 证明 <span
class="math inline">\(\mathbb{R}^n\)</span> 的任意两个不同基必定包含恰好
<span class="math inline">\(n\)</span> 个向量。</li>
<li>Give a basis for the set of polynomials of degree <span
class="math inline">\(\leq 2\)</span>. What is its dimension? 给出次数为
<span class="math inline">\(\leq 2\)</span>
的多项式集的基。它的维数是多少？</li>
</ol>
<h2 id="coordinates">4.4 Coordinates</h2>
<p>4.4 坐标</p>
<p>Once a basis for a vector space is chosen, every vector can be
expressed uniquely as a linear combination of the basis vectors. The
coefficients in this combination are called the coordinates of the
vector relative to that basis. Coordinates allow us to move between the
abstract world of vector spaces and the concrete world of numbers.
一旦选定了向量空间的基，每个向量都可以唯一地表示为基向量的线性组合。该组合中的系数称为向量相对于该基的坐标。坐标使我们能够在向量空间的抽象世界和具体的数字世界之间移动。</p>
<h3 id="coordinates-relative-to-a-basis">Coordinates Relative to a
Basis</h3>
<p>相对于基坐标</p>
<p>Let <span class="math inline">\(V\)</span> be a vector space, and let
令 <span class="math inline">\(V\)</span> 为向量空间，</p>
<p><span class="math display">\[
\mathcal{B} = \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}
\]</span></p>
<p>be an ordered basis for <span class="math inline">\(V\)</span>. Every
vector <span class="math inline">\(\mathbf{u} \in V\)</span> can be
written uniquely as 是 <span class="math inline">\(V\)</span>
的有序基。每个向量 <span class="math inline">\(\mathbf{u} \in V\)</span>
都可以唯一地写成</p>
<p><span class="math display">\[
\mathbf{u} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n
\mathbf{v}_n.
\]</span></p>
<p>The scalars <span class="math inline">\((c_1, c_2, \dots,
c_n)\)</span> are the coordinates of <span
class="math inline">\(\mathbf{u}\)</span> relative to <span
class="math inline">\(\mathcal{B}\)</span>, written 标量 <span
class="math inline">\((c_1, c_2, \dots, c_n)\)</span> 是 <span
class="math inline">\(\mathbf{u}\)</span> 相对于 <span
class="math inline">\(\mathcal{B}\)</span> 的坐标，写为</p>
<p><span class="math display">\[
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n
\end{bmatrix}.
\]</span></p>
<h3 id="example-in-mathbbr2">Example in <span
class="math inline">\(\mathbb{R}^2\)</span></h3>
<p><span class="math inline">\(\mathbb{R}^2\)</span> 中的示例</p>
<p>Example 4.4.1. Let the basis be 例 4.4.1. 设基础为</p>
<p><span class="math display">\[
\mathcal{B} = \{ (1,1), (1,-1) \}.
\]</span></p>
<p>To find the coordinates of <span class="math inline">\(\mathbf{u} =
(3,1)\)</span> relative to <span
class="math inline">\(\mathcal{B}\)</span>, solve 要查找 <span
class="math inline">\(\mathbf{u} = (3,1)\)</span> 相对于 <span
class="math inline">\(\mathcal{B}\)</span> 的坐标，请求解</p>
<p><span class="math display">\[
(3,1) = c_1(1,1) + c_2(1,-1).
\]</span></p>
<p>This gives the system 这使得系统</p>
<p><span class="math display">\[
\begin{cases}c_1 + c_2 = 3, \\c_1 - c_2 = 1.\end{cases}
\]</span></p>
<p>Adding: <span class="math inline">\(2c\_1 = 4 \\implies c\_1 =
2\)</span>. Then <span class="math inline">\(c\_2 = 1\)</span>.
添加：$2c_1 = 4 \implies c_1 = 2 $. Then $ c_2 = 1$。</p>
<p>So, 所以，</p>
<p><span class="math display">\[
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}.
\]</span></p>
<h3 id="standard-coordinates">Standard Coordinates</h3>
<p>标准坐标</p>
<p>In <span class="math inline">\(\mathbb{R}^n\)</span>, the standard
basis is 在 <span class="math inline">\(\mathbb{R}^n\)</span>
中，标准依据是</p>
<p><span class="math display">\[
\mathbf{e}_1 = (1,0,\dots,0), \quad \mathbf{e}_2 = (0,1,0,\dots,0),
\dots, \mathbf{e}_n = (0,\dots,0,1).
\]</span></p>
<p>Relative to this basis, the coordinates of a vector are simply its
entries. Thus, column vectors are coordinate representations by default.
相对于此基，向量的坐标仅仅是它的元素。因此，列向量默认为坐标表示。</p>
<h3 id="change-of-basis">Change of Basis</h3>
<p>基础变更</p>
<p>If <span class="math inline">\(\mathcal{B} = {\mathbf{v}_1, \dots,
\mathbf{v}_n}\)</span> is a basis of <span
class="math inline">\(\mathbb{R}^n\)</span>, the change of basis matrix
is 如果𝐵 = 𝑣 1 , … , 𝑣 𝑛 B=v 1 ​ ，…，v n ​ 是 <span
class="math inline">\(\mathbb{R}^n\)</span> 的基，基矩阵的变化是</p>
<p><span class="math display">\[
P = \begin{bmatrix} \mathbf{v}_1 &amp; \mathbf{v}_2 &amp; \cdots &amp;
\mathbf{v}_n \end{bmatrix},
\]</span></p>
<p>with basis vectors as columns. For any vector <span
class="math inline">\(\mathbf{u}\)</span>, 以基向量为列。对于任意向量
<span class="math inline">\(\mathbf{u}\)</span> ，</p>
<p><span class="math display">\[
\mathbf{u} = P [\mathbf{u}]_{\mathcal{B}}, \qquad
[\mathbf{u}]_{\mathcal{B}} = P^{-1}\mathbf{u}.
\]</span></p>
<p>Thus, switching between bases reduces to matrix multiplication.
因此，基数之间的切换就简化为矩阵乘法。</p>
<h3 id="geometric-interpretation-6">Geometric Interpretation</h3>
<p>几何解释</p>
<p>Coordinates are the address of a vector relative to a chosen set of
directions. Different bases are like different coordinate systems:
Cartesian, rotated, skewed, or scaled. The same vector may look very
different numerically depending on the basis, but its geometric identity
is unchanged.
坐标是向量相对于一组选定方向的地址。不同的基就像不同的坐标系：笛卡尔坐标系、旋转坐标系、倾斜坐标系或缩放坐标系。同一个向量在不同基上可能呈现出截然不同的数值，但其几何恒等式保持不变。</p>
<h3 id="why-this-matters-14">Why this matters</h3>
<p>为什么这很重要</p>
<p>Coordinates turn abstract vectors into concrete numerical data.
Changing basis is the algebraic language for rotations of axes,
diagonalization of matrices, and principal component analysis in data
science. Mastery of coordinates is essential for moving fluidly between
geometry, algebra, and computation.
坐标将抽象向量转化为具体的数值数据。变换基是数据科学中轴旋转、矩阵对角化和主成分分析的代数语言。掌握坐标系对于在几何、代数和计算之间流畅切换至关重要。</p>
<h3 id="exercises-4.4">Exercises 4.4</h3>
<p>练习 4.4</p>
<ol type="1">
<li>Express <span class="math inline">\((4,2)\)</span> in terms of the
basis <span class="math inline">\((1,1), (1,-1)\)</span>. 根据基础 <span
class="math inline">\((1,1), (1,-1)\)</span> 表达 <span
class="math inline">\((4,2)\)</span> 。</li>
<li>Find the coordinates of <span class="math inline">\((1,2,3)\)</span>
relative to the standard basis of <span
class="math inline">\(\mathbb{R}^3\)</span>. 找出 <span
class="math inline">\((1,2,3)\)</span> 相对于 <span
class="math inline">\(\mathbb{R}^3\)</span> 标准基的坐标。</li>
<li>If <span class="math inline">\(\mathcal{B} = \{(2,0),
(0,3)\}\)</span>, compute <span class="math inline">\([ (4,6)
]_{\mathcal{B}}\)</span>. 如果 <span class="math inline">\(\mathcal{B} =
\{(2,0), (0,3)\}\)</span> ，则计算 [ ( 4 , 6 ) ] 𝐵 [(4,6)] B ​ .</li>
<li>Construct the change of basis matrix from the standard basis of
<span class="math inline">\(\mathbb{R}^2\)</span> to <span
class="math inline">\(\mathcal{B} = \{(1,1), (1,-1)\}\)</span>.
构建从标准基 <span class="math inline">\(\mathbb{R}^2\)</span> 到 <span
class="math inline">\(\mathcal{B} = \{(1,1), (1,-1)\}\)</span>
的基变换矩阵。</li>
<li>Prove that coordinate representation with respect to a basis is
unique. 证明关于基的坐标表示是唯一的。</li>
</ol>
<h1 id="chapter-5.-linear-transformations">Chapter 5. Linear
Transformations</h1>
<p>第五章线性变换</p>
<h2 id="functions-that-preserve-linearity">5.1 Functions that Preserve
Linearity</h2>
<p>5.1 保持线性的函数</p>
<p>A central theme of linear algebra is understanding linear
transformations: functions between vector spaces that preserve their
algebraic structure. These transformations generalize the idea of matrix
multiplication and capture the essence of linear behavior.
线性代数的核心主题是理解线性变换：向量空间之间保持其代数结构的函数。这些变换推广了矩阵乘法的概念，并抓住了线性行为的本质。</p>
<h3 id="definition-2">Definition</h3>
<p>定义</p>
<p>Let <span class="math inline">\(V\)</span> and <span
class="math inline">\(W\)</span> be vector spaces over <span
class="math inline">\(\mathbb{R}\)</span>. A function 令 <span
class="math inline">\(V\)</span> 和 <span
class="math inline">\(W\)</span> 为 <span
class="math inline">\(\mathbb{R}\)</span> 上的向量空间。函数</p>
<p><span class="math display">\[
T : V \to W
\]</span></p>
<p>is called a linear transformation (or linear map) if for all vectors
<span class="math inline">\(\mathbf{u}, \mathbf{v} \in V\)</span> and
all scalars <span class="math inline">\(c \in \mathbb{R}\)</span>:
如果对于所有向量 <span class="math inline">\(\mathbf{u}, \mathbf{v} \in
V\)</span> 和所有标量 <span class="math inline">\(c \in
\mathbb{R}\)</span> ，则称为线性变换（或线性映射）：</p>
<ol type="1">
<li>Additivity: 加性：</li>
</ol>
<p><span class="math display">\[
T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}),
\]</span></p>
<ol start="2" type="1">
<li>Homogeneity: 同质性：</li>
</ol>
<p><span class="math display">\[
T(c\mathbf{u}) = cT(\mathbf{u}).
\]</span></p>
<p>If both conditions hold, then <span class="math inline">\(T\)</span>
automatically respects linear combinations: 如果两个条件都成立，则 <span
class="math inline">\(T\)</span> 自动遵循线性组合：</p>
<p><span class="math display">\[
T(c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k) = c_1 T(\mathbf{v}_1) +
\cdots + c_k T(\mathbf{v}_k).
\]</span></p>
<h3 id="examples-3">Examples</h3>
<p>示例</p>
<p>Example 5.1.1. Scaling in <span
class="math inline">\(\mathbb{R}^2\)</span>. Let <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span> be
defined by 例 5.1.1. 缩放 <span
class="math inline">\(\mathbb{R}^2\)</span> 。令 <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span>
定义为</p>
<p><span class="math display">\[
T(x,y) = (2x, 2y).
\]</span></p>
<p>This doubles the length of every vector, preserving direction. It is
linear. 这会使每个向量的长度加倍，同时保持方向不变。它是线性的。</p>
<p>Example 5.1.2. Rotation. 例 5.1.2. 旋转。</p>
<p>Let <span class="math inline">\(R_\theta: \mathbb{R}^2 \to
\mathbb{R}^2\)</span> be 令 <span class="math inline">\(R_\theta:
\mathbb{R}^2 \to \mathbb{R}^2\)</span> 为</p>
<p><span class="math display">\[
R_\theta(x,y) = (x\cos\theta - y\sin\theta, \; x\sin\theta +
y\cos\theta).
\]</span></p>
<p>This rotates vectors by angle <span
class="math inline">\(\theta\)</span>. It satisfies additivity and
homogeneity, hence is linear. 这将向量旋转角度 <span
class="math inline">\(\theta\)</span>
。它满足可加性和齐次性，因此是线性的。</p>
<p>Example 5.1.3. Differentiation. 例 5.1.3. 区分。</p>
<p>Let <span class="math inline">\(D: \mathbb{R}[x] \to
\mathbb{R}[x]\)</span> be differentiation: <span
class="math inline">\(D(p(x)) = p&#39;(x)\)</span>. 令 <span
class="math inline">\(D: \mathbb{R}[x] \to \mathbb{R}[x]\)</span>
为微分： <span class="math inline">\(D(p(x)) = p&#39;(x)\)</span> 。</p>
<p>Since derivatives respect addition and scalar multiples,
differentiation is a linear transformation.
由于导数尊重加法和标量倍数，因此微分是一种线性变换。</p>
<h3 id="non-example">Non-Example</h3>
<p>非示例</p>
<p>The map <span class="math inline">\(S:\mathbb{R}^2 \to
\mathbb{R}^2\)</span> defined by 地图 <span
class="math inline">\(S:\mathbb{R}^2 \to \mathbb{R}^2\)</span>
定义为</p>
<p><span class="math display">\[
S(x,y) = (x^2, y^2)
\]</span></p>
<p>is not linear, because <span class="math inline">\(S(\mathbf{u} +
\mathbf{v}) \neq S(\mathbf{u}) + S(\mathbf{v})\)</span> in general.
不是线性的，因为一般来说 <span class="math inline">\(S(\mathbf{u} +
\mathbf{v}) \neq S(\mathbf{u}) + S(\mathbf{v})\)</span> 。</p>
<h3 id="geometric-interpretation-7">Geometric Interpretation</h3>
<p>几何解释</p>
<p>Linear transformations are exactly those that preserve the origin,
lines through the origin, and proportions along those lines. They
include familiar operations: scaling, rotations, reflections, shears,
and projections. Nonlinear transformations bend or curve space, breaking
these properties.
线性变换正是那些保留原点、过原点的直线以及沿这些直线的比例的变换。它们包括我们熟悉的操作：缩放、旋转、反射、剪切和投影。非线性变换会弯曲空间，从而破坏这些属性。</p>
<h3 id="why-this-matters-15">Why this matters</h3>
<p>为什么这很重要</p>
<p>Linear transformations unify geometry, algebra, and computation. They
explain how matrices act on vectors, how data can be rotated or
projected, and how systems evolve under linear rules. Much of linear
algebra is devoted to understanding these transformations, their
representations, and their invariants.
线性变换统一了几何、代数和计算。它解释了矩阵如何作用于向量，数据如何旋转或投影，以及系统如何在线性规则下演化。线性代数的大部分内容致力于理解这些变换、它们的表示及其不变量。</p>
<h3 id="exercises-5.1">Exercises 5.1</h3>
<p>练习 5.1</p>
<ol type="1">
<li>Verify that <span class="math inline">\(T(x,y) = (3x-y, 2y)\)</span>
is a linear transformation on <span
class="math inline">\(\mathbb{R}^2\)</span>. 验证 <span
class="math inline">\(T(x,y) = (3x-y, 2y)\)</span> 是否是 <span
class="math inline">\(\mathbb{R}^2\)</span> 的线性变换。</li>
<li>Show that <span class="math inline">\(T(x,y) = (x+1, y)\)</span> is
not linear. Which axiom fails? 证明 <span class="math inline">\(T(x,y) =
(x+1, y)\)</span> 不是线性的。哪条公理不成立？</li>
<li>Prove that if <span class="math inline">\(T\)</span> and <span
class="math inline">\(S\)</span> are linear transformations, then so is
<span class="math inline">\(T+S\)</span>. 证明如果 <span
class="math inline">\(T\)</span> 和 <span
class="math inline">\(S\)</span> 是线性变换，那么 <span
class="math inline">\(T+S\)</span> 也是线性变换。</li>
<li>Give an example of a linear transformation from <span
class="math inline">\(\mathbb{R}^3\)</span> to <span
class="math inline">\(\mathbb{R}^2\)</span>. 给出一个从 <span
class="math inline">\(\mathbb{R}^3\)</span> 到 <span
class="math inline">\(\mathbb{R}^2\)</span> 的线性变换的例子。</li>
<li>Let <span class="math inline">\(T:\mathbb{R}[x] \to
\mathbb{R}[x]\)</span> be integration: 令 <span
class="math inline">\(T:\mathbb{R}[x] \to \mathbb{R}[x]\)</span>
为积分：</li>
</ol>
<p><span class="math display">\[
T(p(x)) = \int_0^x p(t)\\,dt.
\]</span></p>
<p>Prove that <span class="math inline">\(T\)</span> is a linear
transformation. 证明 <span class="math inline">\(T\)</span>
是线性变换。</p>
<h2 id="matrix-representation-of-linear-maps">5.2 Matrix Representation
of Linear Maps</h2>
<p>5.2 线性映射的矩阵表示</p>
<p>Every linear transformation between finite-dimensional vector spaces
can be represented by a matrix. This correspondence is one of the
central insights of linear algebra: it lets us use the tools of matrix
arithmetic to study abstract transformations.
有限维向量空间之间的所有线性变换都可以用矩阵表示。这种对应关系是线性代数的核心洞见之一：它让我们能够利用矩阵运算工具来研究抽象的变换。</p>
<h3 id="from-linear-map-to-matrix">From Linear Map to Matrix</h3>
<p>从线性映射到矩阵</p>
<p>Let <span class="math inline">\(T: \mathbb{R}^n \to
\mathbb{R}^m\)</span> be a linear transformation. Choose the standard
basis <span class="math inline">\(\{ \mathbf{e}_1, \dots, \mathbf{e}_n
\}\)</span> of <span class="math inline">\(\mathbb{R}^n\)</span>, where
<span class="math inline">\(\mathbf{e}_i\)</span> has a 1 in the <span
class="math inline">\(i\)</span>-th position and 0 elsewhere. 令 <span
class="math inline">\(T: \mathbb{R}^n \to \mathbb{R}^m\)</span>
为线性变换。选取 <span class="math inline">\(\mathbb{R}^n\)</span>
的标准基 <span class="math inline">\(\{ \mathbf{e}_1, \dots,
\mathbf{e}_n \}\)</span> ，其中 𝑒 𝑖 e i ​ 第 <span
class="math inline">\(i\)</span> 个位置为 1，其他地方为 0。</p>
<p>The action of <span class="math inline">\(T\)</span> on each basis
vector determines the entire transformation: <span
class="math inline">\(T\)</span> 对每个基向量的作用决定了整个变换：</p>
<p><span class="math display">\[
T(\mathbf{e}\_j) = \begin{bmatrix}a_{1j} \\a_{2j} \\\vdots \\a_{mj}
\end{bmatrix}.
\]</span></p>
<p>Placing these outputs as columns gives the matrix of <span
class="math inline">\(T\)</span>: 将这些输出作为列放置，得到矩阵 <span
class="math inline">\(T\)</span> ：</p>
<p><span class="math display">\[
[T] = A = \begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}
\\a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\\vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\a_{m1} &amp; a_{m2} &amp; \cdots &amp;
a_{mn}\end{bmatrix}.
\]</span></p>
<p>Then for any vector <span class="math inline">\(\mathbf{x} \in
\mathbb{R}^n\)</span>: 然后对于任意向量 <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> ：</p>
<p><span class="math display">\[
T(\mathbf{x}) = A\mathbf{x}.
\]</span></p>
<h3 id="examples-4">Examples</h3>
<p>示例</p>
<p>Example 5.2.1. Scaling in <span
class="math inline">\(\mathbb{R}^2\)</span>. Let <span
class="math inline">\(T(x,y) = (2x, 3y)\)</span>. Then 例 5.2.1. 缩放
<span class="math inline">\(\mathbb{R}^2\)</span> 。设 <span
class="math inline">\(T(x,y) = (2x, 3y)\)</span> 。然后</p>
<p><span class="math display">\[
T(\mathbf{e}_1) = (2,0), \quad T(\mathbf{e}_2) = (0,3).
\]</span></p>
<p>So the matrix is 所以矩阵是</p>
<p><span class="math display">\[
[T] = \begin{bmatrix}2 &amp; 0 \\0 &amp; 3\end{bmatrix}.
\]</span></p>
<p>Example 5.2.2. Rotation in the plane. The rotation transformation
<span class="math inline">\(R_\theta(x,y) = (x\cos\theta - y\sin\theta,
\; x\sin\theta + y\cos\theta)\)</span> has matrix 例5.2.2. 平面旋转。
旋转变换 <span class="math inline">\(R_\theta(x,y) = (x\cos\theta -
y\sin\theta, \; x\sin\theta + y\cos\theta)\)</span> 具有矩阵</p>
<p><span class="math display">\[
[R_\theta] = \begin{bmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta
&amp; \cos\theta\end{bmatrix}.
\]</span></p>
<p>Example 5.2.3. Projection onto the x-axis. The map <span
class="math inline">\(P(x,y) = (x,0)\)</span> corresponds to 例 5.2.3.
投影到 x 轴。 地图 <span class="math inline">\(P(x,y) = (x,0)\)</span>
对应于</p>
<p><span class="math display">\[
[P] = \begin{bmatrix}1 &amp; 0 \\0 &amp; 0\end{bmatrix}.
\]</span></p>
<h3 id="change-of-basis-1">Change of Basis</h3>
<p>基础变更</p>
<p>Matrix representations depend on the chosen basis. If <span
class="math inline">\(\mathcal{B}\)</span> and <span
class="math inline">\(\mathcal{C}\)</span> are bases of <span
class="math inline">\(\mathbb{R}^n\)</span> and <span
class="math inline">\(\mathbb{R}^m\)</span>, then the matrix of <span
class="math inline">\(T: \mathbb{R}^n \to \mathbb{R}^m\)</span> with
respect to these bases is obtained by expressing <span
class="math inline">\(T(\mathbf{v}_j)\)</span> in terms of <span
class="math inline">\(\mathcal{C}\)</span> for each <span
class="math inline">\(\mathbf{v}_j \in \mathcal{B}\)</span>. Changing
bases corresponds to conjugating the matrix by the appropriate
change-of-basis matrices. 矩阵表示取决于所选的基。如果 <span
class="math inline">\(\mathcal{B}\)</span> 和 <span
class="math inline">\(\mathcal{C}\)</span> 是 <span
class="math inline">\(\mathbb{R}^n\)</span> 的基 和 <span
class="math inline">\(\mathbb{R}^m\)</span> ，则 <span
class="math inline">\(T: \mathbb{R}^n \to \mathbb{R}^m\)</span>
关于这些基的矩阵，可以通过将 <span
class="math inline">\(T(\mathbf{v}_j)\)</span> 表示为 <span
class="math inline">\(\mathcal{C}\)</span> 来获得，其中 <span
class="math inline">\(\mathbf{v}_j \in \mathcal{B}\)</span> 表示为 <span
class="math inline">\(T(\mathbf{v}_j)\)</span>。改变基相当于将矩阵与适当的基变换矩阵共轭。</p>
<h3 id="geometric-interpretation-8">Geometric Interpretation</h3>
<p>几何解释</p>
<p>Matrices are not just convenient notation-they <em>are</em> linear
maps once a basis is fixed. Every rotation, reflection, projection,
shear, or scaling corresponds to multiplying by a specific matrix. Thus,
studying linear transformations reduces to studying their matrices.
矩阵不仅仅是方便的符号——一旦基确定，它们<em>就是</em>线性映射。所有旋转、反射、投影、剪切或缩放都对应于乘以一个特定的矩阵。因此，研究线性变换可以归结为研究它们的矩阵。</p>
<h3 id="why-this-matters-16">Why this matters</h3>
<p>为什么这很重要</p>
<p>Matrix representations make linear transformations computable. They
connect abstract definitions to explicit calculations, enabling
algorithms for solving systems, finding eigenvalues, and performing
decompositions. Applications from graphics to machine learning depend on
this translation.
矩阵表示使线性变换可计算。它们将抽象定义与明确的计算联系起来，从而支持求解系统、查找特征值和执行分解的算法。从图形到机器学习等各种应用都依赖于这种转换。</p>
<h3 id="exercises-5.2">Exercises 5.2</h3>
<p>练习 5.2</p>
<ol type="1">
<li>Find the matrix representation of <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span>, <span
class="math inline">\(T(x,y) = (x+y, x-y)\)</span>. 找到 <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span> , <span
class="math inline">\(T(x,y) = (x+y, x-y)\)</span> 的矩阵表示。</li>
<li>Determine the matrix of the linear transformation <span
class="math inline">\(T:\mathbb{R}^3 \to \mathbb{R}^2\)</span>, <span
class="math inline">\(T(x,y,z) = (x+z, y-2z)\)</span>. 确定线性变换矩阵
<span class="math inline">\(T:\mathbb{R}^3 \to \mathbb{R}^2\)</span> ，
<span class="math inline">\(T(x,y,z) = (x+z, y-2z)\)</span> 。</li>
<li>What matrix represents reflection across the line <span
class="math inline">\(y=x\)</span> in <span
class="math inline">\(\mathbb{R}^2\)</span>? 哪个矩阵表示 <span
class="math inline">\(\mathbb{R}^2\)</span> 中沿线 <span
class="math inline">\(y=x\)</span> 的反射？</li>
<li>Show that the matrix of the identity transformation on <span
class="math inline">\(\mathbb{R}^n\)</span> is <span
class="math inline">\(I_n\)</span>. 证明 <span
class="math inline">\(\mathbb{R}^n\)</span> 上的恒等变换矩阵是 𝐼 𝑛 I n ​
.</li>
<li>For the differentiation map <span
class="math inline">\(D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]\)</span>,
where <span class="math inline">\(\mathbb{R}_k[x]\)</span> is the space
of polynomials of degree at most <span class="math inline">\(k\)</span>,
find the matrix of <span class="math inline">\(D\)</span> relative to
the bases <span class="math inline">\(\{1,x,x^2\}\)</span> and <span
class="math inline">\(\{1,x\}\)</span>. 对于微分映射 <span
class="math inline">\(D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]\)</span>
，其中 <span class="math inline">\(\mathbb{R}_k[x]\)</span> 是次数最多为
<span class="math inline">\(k\)</span> 的多项式空间，求出 <span
class="math inline">\(D\)</span> 相对于基数 <span
class="math inline">\(\{1,x,x^2\}\)</span> 和 <span
class="math inline">\(\{1,x\}\)</span> 的矩阵。</li>
</ol>
<h2 id="kernel-and-image">5.3 Kernel and Image</h2>
<p>5.3 内核和镜像</p>
<p>To understand a linear transformation deeply, we must examine what it
kills and what it produces. These ideas are captured by the kernel and
the image, two fundamental subspaces associated with any linear map.
要深入理解线性变换，我们必须考察它消除了什么，又产生了什么。这些概念可以通过核和像来理解，它们是任何线性映射都相关的两个基本子空间。</p>
<h3 id="the-kernel">The Kernel</h3>
<p>内核</p>
<p>The kernel (or null space) of a linear transformation <span
class="math inline">\(T: V \to W\)</span> is the set of all vectors in
<span class="math inline">\(V\)</span> that map to the zero vector in
<span class="math inline">\(W\)</span>: 线性变换 <span
class="math inline">\(T: V \to W\)</span> 的核（或零空间）是 <span
class="math inline">\(V\)</span> 中映射到 <span
class="math inline">\(W\)</span> 中的零向量的所有向量的集合：</p>
<p><span class="math display">\[
\ker(T) = \{ \mathbf{v} \in V \mid T(\mathbf{v}) = \mathbf{0} \}.
\]</span></p>
<p>The kernel is always a subspace of <span
class="math inline">\(V\)</span>. It measures the degeneracy of the
transformation-directions that collapse to nothing. 核始终是 <span
class="math inline">\(V\)</span>
的子空间。它衡量的是坍缩为零的变换方向的退化程度。</p>
<p>Example 5.3.1. Let <span class="math inline">\(T:\mathbb{R}^3 \to
\mathbb{R}^2\)</span> be defined by 例 5.3.1。 让 <span
class="math inline">\(T:\mathbb{R}^3 \to \mathbb{R}^2\)</span>
定义为</p>
<p><span class="math display">\[
T(x,y,z) = (x+y, y+z).
\]</span></p>
<p>In matrix form, 以矩阵形式，</p>
<p><span class="math display">\[
[T] = \begin{bmatrix}1 &amp; 1 &amp; 0 \\0 &amp; 1 &amp; 1\end{bmatrix}.
\]</span></p>
<p>To find the kernel, solve 要找到内核，请解决</p>
<p><span class="math display">\[
\begin{bmatrix}1 &amp; 1 &amp; 0 \\0 &amp; 1 &amp;
1\end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix}= \begin{bmatrix}
0 \\ 0 \end{bmatrix}.
\]</span></p>
<p>This gives the equations <span class="math inline">\(x + y =
0\)</span>, <span class="math inline">\(y + z = 0\)</span>. Hence <span
class="math inline">\(x = -y, z = -y\)</span>. The kernel is
由此得到方程 <span class="math inline">\(x + y = 0\)</span> ， <span
class="math inline">\(y + z = 0\)</span> 。因此 <span
class="math inline">\(x = -y, z = -y\)</span> 。核函数为</p>
<p><span class="math display">\[
\ker(T) = \{ (-t, t, -t) \mid t \in \mathbb{R} \},
\]</span></p>
<p>a line in <span class="math inline">\(\mathbb{R}^3\)</span>. <span
class="math inline">\(\mathbb{R}^3\)</span> 中的一行。</p>
<h3 id="the-image">The Image</h3>
<p>图像</p>
<p>The image (or range) of a linear transformation <span
class="math inline">\(T: V \to W\)</span> is the set of all outputs:
线性变换 <span class="math inline">\(T: V \to W\)</span>
的图像（或范围）是所有输出的集合：</p>
<p><span class="math display">\[
\text{im}(T) = \{ T(\mathbf{v}) \mid \mathbf{v} \in V \} \subseteq W.
\]</span></p>
<p>Equivalently, it is the span of the columns of the representing
matrix. The image is always a subspace of <span
class="math inline">\(W\)</span>.
等效地，它是表示矩阵的列的跨度。图像始终是 <span
class="math inline">\(W\)</span> 的子空间。</p>
<p>Example 5.3.2. For the same transformation as above, 例 5.3.2.
对于与上述相同的变换，</p>
<p><span class="math display">\[
[T] = \begin{bmatrix}1 &amp; 1 &amp; 0 \\0 &amp; 1 &amp; 1\end{bmatrix},
\]</span></p>
<p>the columns are <span class="math inline">\((1,0)\)</span>, <span
class="math inline">\((1,1)\)</span>, and <span
class="math inline">\((0,1)\)</span>. Since <span
class="math inline">\((1,1) = (1,0) + (0,1)\)</span>, the image is 列为
<span class="math inline">\((1,0)\)</span> 、 <span
class="math inline">\((1,1)\)</span> 和 <span
class="math inline">\((0,1)\)</span> 。由于 <span
class="math inline">\((1,1) = (1,0) + (0,1)\)</span> ，因此图像为</p>
<p><span class="math display">\[
\text{im}(T) = \text{span}\{ (1,0), (0,1) \} = \mathbb{R}^2.
\]</span></p>
<h3 id="dimension-formula-ranknullity-theorem">Dimension Formula
(Rank–Nullity Theorem)</h3>
<p>维度公式（秩-零度定理）</p>
<p>For a linear transformation <span class="math inline">\(T: V \to
W\)</span> with <span class="math inline">\(V\)</span>
finite-dimensional, 对于线性变换 <span class="math inline">\(T: V \to
W\)</span> 且 <span class="math inline">\(V\)</span> 为有限维，</p>
<p><span class="math display">\[
\dim(\ker(T)) + \dim(\text{im}(T)) = \dim(V).
\]</span></p>
<p>This fundamental result connects the lost directions (kernel) with
the achieved directions (image).
这个基本结果将丢失的方向（内核）与实现的方向（图像）联系起来。</p>
<h3 id="geometric-interpretation-9">Geometric Interpretation</h3>
<p>几何解释</p>
<ul>
<li>The kernel describes how the transformation flattens space (e.g.,
projecting a 3D object onto a plane).
内核描述了变换如何使空间变平坦（例如，将 3D 对象投影到平面上）。</li>
<li>The image describes the target subspace reached by the
transformation. 该图像描述了变换所达到的目标子空间。</li>
<li>The rank–nullity theorem quantifies the tradeoff: the more
dimensions collapse, the fewer remain in the image.
秩零定理量化了这种权衡：维度崩溃得越多，图像中剩余的维度就越少。</li>
</ul>
<h3 id="why-this-matters-17">Why this matters</h3>
<p>为什么这很重要</p>
<p>Kernel and image capture the essence of a linear map. They classify
transformations, explain when systems have unique or infinite solutions,
and form the backbone of important results like the Rank–Nullity
Theorem, diagonalization, and spectral theory.
核和图像捕捉了线性映射的本质。它们对变换进行分类，解释系统何时具有唯一或无限解，并构成秩零定理、对角化和谱理论等重要结果的支柱。</p>
<h3 id="exercises-5.3">Exercises 5.3</h3>
<p>练习 5.3</p>
<ol type="1">
<li>Find the kernel and image of <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span>, <span
class="math inline">\(T(x,y) = (x-y, x+y)\)</span>. 查找 <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span> 、 <span
class="math inline">\(T(x,y) = (x-y, x+y)\)</span> 的核和图像。</li>
<li>Let 让</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; 1 &amp; 4 \end{bmatrix}
\]</span></p>
<p>Find bases for <span class="math inline">\(\ker(A)\)</span> and <span
class="math inline">\(\text{im}(A)\)</span>. 3. For the projection map
<span class="math inline">\(P(x,y,z) = (x,y,0)\)</span>, describe the
kernel and image. 4. Prove that <span
class="math inline">\(\ker(T)\)</span> and <span
class="math inline">\(\text{im}(T)\)</span> are always subspaces. 5.
Verify the Rank–Nullity Theorem for the transformation in Example 5.3.1.
找到 <span class="math inline">\(\ker(A)\)</span> 和 <span
class="math inline">\(\text{im}(A)\)</span> 的基。3. 对于投影图 <span
class="math inline">\(P(x,y,z) = (x,y,0)\)</span> ，描述其核和图像。4.
证明 <span class="math inline">\(\ker(T)\)</span> 和 <span
class="math inline">\(\text{im}(T)\)</span> 始终是子空间。5.
验证示例5.3.1中变换的秩零性定理。</p>
<h2 id="change-of-basis-2">5.4 Change of Basis</h2>
<p>5.4 基础变更</p>
<p>Linear transformations can look very different depending on the
coordinate system we use. The process of rewriting vectors and
transformations relative to a new basis is called a change of basis.
This concept lies at the heart of diagonalization, orthogonalization,
and many computational techniques.
根据我们使用的坐标系，线性变换看起来可能非常不同。相对于新的基重写向量和变换的过程称为基变换。这个概念是对角化、正交化以及许多计算技术的核心。</p>
<h3 id="coordinate-change">Coordinate Change</h3>
<p>坐标变换</p>
<p>Suppose <span class="math inline">\(V\)</span> is an <span
class="math inline">\(n\)</span>-dimensional vector space, and let <span
class="math inline">\(\mathcal{B} = \{\mathbf{v}_1, \dots,
\mathbf{v}_n\}\)</span> be a basis. Every vector <span
class="math inline">\(\mathbf{x} \in V\)</span> has a coordinate vector
<span class="math inline">\([\mathbf{x}]_{\mathcal{B}} \in
\mathbb{R}^n\)</span>. 假设 <span class="math inline">\(V\)</span>
是一个 <span class="math inline">\(n\)</span> 维向量空间，设 <span
class="math inline">\(\mathcal{B} = \{\mathbf{v}_1, \dots,
\mathbf{v}_n\}\)</span> 为基。每个向量 <span
class="math inline">\(\mathbf{x} \in V\)</span> 都有一个坐标向量 <span
class="math inline">\([\mathbf{x}]_{\mathcal{B}} \in
\mathbb{R}^n\)</span> 。</p>
<p>If <span class="math inline">\(P\)</span> is the change-of-basis
matrix from <span class="math inline">\(\mathcal{B}\)</span> to the
standard basis, then 如果 <span class="math inline">\(P\)</span> 是从
<span class="math inline">\(\mathcal{B}\)</span>
到标准基的基变换矩阵，则</p>
<p><span class="math display">\[
\mathbf{x} = P [\mathbf{x}]_{\mathcal{B}}.
\]</span></p>
<p>Equivalently, 等价地，</p>
<p><span class="math display">\[
[\mathbf{x}]_{\mathcal{B}} = P^{-1} \mathbf{x}.
\]</span></p>
<p>Here, <span class="math inline">\(P\)</span> has the basis vectors of
<span class="math inline">\(\mathcal{B}\)</span> as its columns: 这里，
<span class="math inline">\(P\)</span> 以 <span
class="math inline">\(\mathcal{B}\)</span> 的基向量作为其列：</p>
<p><span class="math display">\[
P = \begin{bmatrix}\mathbf{v}_1 &amp; \mathbf{v}_2 &amp; \cdots &amp;
\mathbf{v}_n\end{bmatrix}.
\]</span></p>
<h3 id="transformation-of-matrices">Transformation of Matrices</h3>
<p>矩阵变换</p>
<p>Let <span class="math inline">\(T: V \to V\)</span> be a linear
transformation. Suppose its matrix in the standard basis is <span
class="math inline">\(A\)</span>. In the basis <span
class="math inline">\(\mathcal{B}\)</span>, the representing matrix
becomes 令 <span class="math inline">\(T: V \to V\)</span>
为线性变换。假设其在标准基中的矩阵为 <span
class="math inline">\(A\)</span> 。在基 <span
class="math inline">\(\mathcal{B}\)</span> 中，表示矩阵变为</p>
<p><span class="math display">\[
[T]_{\mathcal{B}} = P^{-1} A P.
\]</span></p>
<p>Thus, changing basis corresponds to a similarity transformation of
the matrix. 因此，改变基础对应于矩阵的相似变换。</p>
<h3 id="example-3">Example</h3>
<p>例子</p>
<p>Example 5.4.1. Let <span class="math inline">\(T:\mathbb{R}^2 \to
\mathbb{R}^2\)</span> be given by 例 5.4.1。 令 <span
class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span> 为</p>
<p><span class="math display">\[
T(x,y) = (3x + y, x + y).
\]</span></p>
<p>In the standard basis, its matrix is 在标准基础上，其矩阵为</p>
<p><span class="math display">\[
A = \begin{bmatrix}3 &amp; 1 \\1 &amp; 1\end{bmatrix}.
\]</span></p>
<p>Now consider the basis <span class="math inline">\(\mathcal{B} = \{
(1,1), (1,-1) \}\)</span>. The change-of-basis matrix is 现在考虑基
<span class="math inline">\(\mathcal{B} = \{ (1,1), (1,-1) \}\)</span>
。基变换矩阵为</p>
<p><span class="math display">\[
P = \begin{bmatrix}1 &amp; 1 \\1 &amp; -1\end{bmatrix}.
\]</span></p>
<p>Then 然后</p>
<p><span class="math display">\[
[T]_{\mathcal{B}} = P^{-1} A P.
\]</span></p>
<p>Computing gives 计算得出</p>
<p><span class="math display">\[
[T]_{\mathcal{B}} =\begin{bmatrix}4 &amp; 0 \\0 &amp; 0\end{bmatrix}.
\]</span></p>
<p>In this new basis, the transformation is diagonal: one direction is
scaled by 4, the other collapsed to 0.
在这个新的基础上，变换是对角的：一个方向缩放 4，另一个方向折叠为 0。</p>
<h3 id="geometric-interpretation-10">Geometric Interpretation</h3>
<p>几何解释</p>
<p>Change of basis is like rotating or skewing your coordinate grid. The
underlying transformation does not change, but its description in
numbers becomes simpler or more complicated depending on the basis.
Finding a basis that simplifies a transformation (often a diagonal
basis) is a key theme in linear algebra.
基变换就像旋转或倾斜坐标网格。底层变换本身不会改变，但其数值描述会根据基的变化而变得更简单或更复杂。寻找能够简化变换的基（通常是对角基）是线性代数的一个关键主题。</p>
<h3 id="why-this-matters-18">Why this matters</h3>
<p>为什么这很重要</p>
<p>Change of basis connects the abstract notion of similarity to
practical computation. It is the tool that allows us to diagonalize
matrices, compute eigenvalues, and simplify complex transformations. In
applications, it corresponds to choosing a more natural coordinate
system-whether in geometry, physics, or machine learning.
基变换将相似性的抽象概念与实际计算联系起来。它使我们能够对矩阵进行对角化、计算特征值并简化复杂的变换。在实际应用中，它相当于选择一个更自然的坐标系——无论是在几何、物理还是机器学习领域。</p>
<h3 id="exercises-5.4">Exercises 5.4</h3>
<p>练习 5.4</p>
<ol type="1">
<li>Let 让</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 1 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<p>Compute its representation in the basis <span
class="math inline">\(\{(1,0),(1,1)\}\)</span>. 2. Find the
change-of-basis matrix from the standard basis of <span
class="math inline">\(\mathbb{R}^2\)</span> to <span
class="math inline">\(\{(2,1),(1,1)\}\)</span>. 3. Prove that similar
matrices (related by <span class="math inline">\(P^{-1}AP\)</span>)
represent the same linear transformation under different bases. 4.
Diagonalize the matrix 计算其在基 <span
class="math inline">\(\{(1,0),(1,1)\}\)</span> 中的表示。2. 求出从 <span
class="math inline">\(\mathbb{R}^2\)</span> 到 <span
class="math inline">\(\{(2,1),(1,1)\}\)</span> 的标准基变换矩阵。3.
证明相似的矩阵（由 <span class="math inline">\(P^{-1}AP\)</span>
关联）在不同基下表示相同的线性变换。4. 对角化矩阵</p>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}
\]</span></p>
<p>in the basis <span class="math inline">\(\{(1,1),(1,-1)\}\)</span>.
5. In <span class="math inline">\(\mathbb{R}^3\)</span>, let <span
class="math inline">\(\mathcal{B} =
\{(1,0,0),(1,1,0),(1,1,1)\}\)</span>. Construct the change-of-basis
matrix <span class="math inline">\(P\)</span> and compute <span
class="math inline">\(P^{-1}\)</span>. 在基 <span
class="math inline">\(\{(1,1),(1,-1)\}\)</span> 中。5. 在 <span
class="math inline">\(\mathbb{R}^3\)</span> 中，令 <span
class="math inline">\(\mathcal{B} = \{(1,0,0),(1,1,0),(1,1,1)\}\)</span>
。构造基变换矩阵 <span class="math inline">\(P\)</span> 并计算 <span
class="math inline">\(P^{-1}\)</span> 。</p>
<h1 id="chapter-6.-determinants">Chapter 6. Determinants</h1>
<p>第六章 决定因素</p>
<h2 id="motivation-and-geometric-meaning">6.1 Motivation and Geometric
Meaning</h2>
<p>6.1 动机和几何意义</p>
<p>Determinants are numerical values associated with square matrices. At
first they may appear as a complicated formula, but their importance
comes from what they measure: determinants encode scaling, orientation,
and invertibility of linear transformations. They bridge algebra and
geometry.
行列式是与方阵相关的数值。乍一看，它们可能看起来像一个复杂的公式，但它们的重要性在于它们所测量的内容：行列式编码了线性变换的缩放、方向和可逆性。它们连接了代数和几何。</p>
<h3 id="determinants-of-22-matrices">Determinants of 2×2 Matrices</h3>
<p>2×2 矩阵的行列式</p>
<p>For a 2×2 matrix 对于 2×2 矩阵</p>
<p><span class="math display">\[
A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix},
\]</span></p>
<p>the determinant is defined as 行列式定义为</p>
<p><span class="math display">\[
\det(A) = ad - bc.
\]</span></p>
<p>Geometric meaning: If <span class="math inline">\(A\)</span>
represents a linear transformation of the plane, then <span
class="math inline">\(|\det(A)|\)</span> is the area scaling factor. For
example, if <span class="math inline">\(\det(A) = 2\)</span>, areas of
shapes are doubled. If <span class="math inline">\(\det(A) = 0\)</span>,
the transformation collapses the plane to a line: all area is lost.
几何含义：如果 <span class="math inline">\(A\)</span>
表示平面的线性变换，则 <span class="math inline">\(|\det(A)|\)</span>
是面积缩放因子。例如，如果 <span class="math inline">\(\det(A) =
2\)</span> ，形状的面积将加倍。如果 <span class="math inline">\(\det(A)
= 0\)</span> ，变换将平面折叠成一条线：所有面积都将丢失。</p>
<h3 id="determinants-of-33-matrices">Determinants of 3×3 Matrices</h3>
<p>3×3 矩阵的行列式</p>
<p>For 为了</p>
<p><span class="math display">\[
A = \begin{bmatrix}a &amp; b &amp; c \\d &amp; e &amp; f \\g &amp; h
&amp; i\end{bmatrix},
\]</span></p>
<p>the determinant can be computed as 行列式可以计算为</p>
<p><span class="math display">\[
\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).
\]</span></p>
<p>Geometric meaning: In <span
class="math inline">\(\mathbb{R}^3\)</span>, <span
class="math inline">\(|\det(A)|\)</span> is the volume scaling factor.
If <span class="math inline">\(\det(A) &lt; 0\)</span>, orientation is
reversed (a handedness flip), such as turning a right-handed coordinate
system into a left-handed one. 几何含义：在 <span
class="math inline">\(\mathbb{R}^3\)</span> 中， <span
class="math inline">\(|\det(A)|\)</span> 是体积缩放因子。如果为 <span
class="math inline">\(\det(A) &lt; 0\)</span>
，则方向反转（即手性翻转），例如将右手坐标系转换为左手坐标系。</p>
<h3 id="general-case">General Case</h3>
<p>一般情况</p>
<p>For <span class="math inline">\(A \in \mathbb{R}^{n \times
n}\)</span>, the determinant is a scalar that measures how the linear
transformation given by <span class="math inline">\(A\)</span> scales
n-dimensional volume. 对于 <span class="math inline">\(A \in
\mathbb{R}^{n \times n}\)</span> ，行列式是一个标量，它衡量 <span
class="math inline">\(A\)</span> 给出的线性变换如何缩放 n 维体积。</p>
<ul>
<li>If <span class="math inline">\(\det(A) = 0\)</span>: the
transformation squashes space into a lower dimension, so <span
class="math inline">\(A\)</span> is not invertible. 如果 <span
class="math inline">\(\det(A) = 0\)</span>
：变换将空间压缩到较低维度，因此 <span class="math inline">\(A\)</span>
不可逆。</li>
<li>If <span class="math inline">\(\det(A) &gt; 0\)</span>: volume is
scaled by <span class="math inline">\(\det(A)\)</span>, orientation
preserved. 如果是 <span class="math inline">\(\det(A) &gt; 0\)</span>
：体积按 <span class="math inline">\(\det(A)\)</span>
缩放，方向保持不变。</li>
<li>If <span class="math inline">\(\det(A) &lt; 0\)</span>: volume is
scaled by <span class="math inline">\(|\det(A)|\)</span>, orientation
reversed. 如果是 <span class="math inline">\(\det(A) &lt; 0\)</span>
：体积按 <span class="math inline">\(|\det(A)|\)</span>
缩放，方向反转。</li>
</ul>
<h3 id="visual-examples">Visual Examples</h3>
<p>视觉示例</p>
<ol type="1">
<li><p>Shear in <span class="math inline">\(\mathbb{R}^2\)</span>: <span
class="math inline">\(A = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1
\end{bmatrix}\)</span>. Then <span class="math inline">\(\det(A) =
1\)</span>. The transformation slants the unit square into a
parallelogram but preserves area. <span
class="math inline">\(\mathbb{R}^2\)</span> 处的剪切： <span
class="math inline">\(A = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1
\end{bmatrix}\)</span> 。然后是 <span class="math inline">\(\det(A) =
1\)</span> 。变换将单位正方形倾斜为平行四边形，但保留面积。</p></li>
<li><p>Projection in <span class="math inline">\(\mathbb{R}^2\)</span>:
<span class="math inline">\(A = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0
\end{bmatrix}\)</span>. Then <span class="math inline">\(\det(A) =
0\)</span>. The unit square collapses into a line segment: area
vanishes. <span class="math inline">\(\mathbb{R}^2\)</span> 中的投影：
<span class="math inline">\(A = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0
\end{bmatrix}\)</span> 。然后 <span class="math inline">\(\det(A) =
0\)</span> 。单位正方形坍缩成一条线段：面积消失。</p></li>
<li><p>Rotation in <span class="math inline">\(\mathbb{R}^2\)</span>:
<span class="math inline">\(R_\theta = \begin{bmatrix} \cos\theta &amp;
-\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}\)</span>. Then
<span class="math inline">\(\det(R_\theta) = 1\)</span>. Rotations
preserve area and orientation. <span
class="math inline">\(\mathbb{R}^2\)</span> 中的旋转： <span
class="math inline">\(R_\theta = \begin{bmatrix} \cos\theta &amp;
-\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}\)</span> 。然后
<span class="math inline">\(\det(R_\theta) = 1\)</span>
。旋转保留面积和方向。</p></li>
</ol>
<h3 id="why-this-matters-19">Why this matters</h3>
<p>为什么这很重要</p>
<p>The determinant is not just a formula-it is a measure of
transformation. It tells us whether a matrix is invertible, how it
distorts space, and whether it flips orientation. This geometric insight
makes the determinant indispensable in analysis, geometry, and applied
mathematics.
行列式不仅仅是一个公式，它还是一种变换的度量。它告诉我们一个矩阵是否可逆，它如何扭曲空间，以及它是否会翻转方向。这种几何学上的洞察力使得行列式在分析、几何和应用数学中不可或缺。</p>
<h3 id="exercises-6.1">Exercises 6.1</h3>
<p>练习 6.1</p>
<ol type="1">
<li>Compute the determinant of 计算行列式</li>
</ol>
<p><span class="math display">\[
\begin{bmatrix} 2 &amp; 3 \\ 1 &amp; 4 \end{bmatrix}
\]</span></p>
<p>What area scaling factor does it represent? 2. Find the determinant
of the shear matrix 它代表什么面积比例因子？2. 求剪切矩阵的行列式</p>
<p><span class="math display">\[
\begin{bmatrix} 1 &amp; 2 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<blockquote>
<p>下面这段公式解析错误，我改成源码模式了</p>
</blockquote>
<pre><code>What happens to the area of the unit square? 3. For the $3 \\times 3matrix \[100020003\] Compute the determinant. How does it scale volume in\\mathbb{R}^3$?4. Show that any rotation matrix in $\\mathbb{R}^2 has determinant \\1. 5. Give an example of a \\2 \\times 2$matrix with determinant$\-1$. What geometric action does it represent?
单位正方形的面积会发生什么变化？ 3. 对于 $3 \\times 3 矩阵 \[100020003\] 计算行列式。\\mathbb{R}^3 $?4. Show that any rotation matrix in $ \\mathbb{R}^2 的行列式为 \\ 1 ，它如何缩放体积 ？5. 举一个 \\ 2 \\times 2 $matrix with determinant$ -1$ 的例子 。它代表什么几何作用？</code></pre>
<h2 id="properties-of-determinants">6.2 Properties of Determinants</h2>
<p>6.2 行列式的性质</p>
<p>Beyond their geometric meaning, determinants satisfy a collection of
algebraic rules that make them powerful tools in linear algebra. These
properties allow us to compute efficiently, test invertibility, and
understand how determinants behave under matrix operations.
除了几何意义之外，行列式还满足一系列代数规则，使其成为线性代数中强大的工具。这些性质使我们能够高效计算、测试可逆性，并理解行列式在矩​​阵运算下的行为。</p>
<h3 id="basic-properties">Basic Properties</h3>
<p>基本属性</p>
<p>Let <span class="math inline">\(A, B \in \mathbb{R}^{n \times
n}\)</span>, and let <span class="math inline">\(c \in
\mathbb{R}\)</span>. Then: 令 <span class="math inline">\(A, B \in
\mathbb{R}^{n \times n}\)</span> ，令 <span class="math inline">\(c \in
\mathbb{R}\)</span> 。然后：</p>
<ol type="1">
<li>Identity: 身份：</li>
</ol>
<p><span class="math display">\[
\det(I_n) = 1.
\]</span></p>
<ol start="2" type="1">
<li>Triangular matrices: If <span class="math inline">\(A\)</span> is
upper or lower triangular, then 三角矩阵： 如果 <span
class="math inline">\(A\)</span> 是上三角或下三角，则</li>
</ol>
<p><span class="math display">\[
\det(A) = a_{11} a_{22} \cdots a_{nn}.
\]</span></p>
<ol start="3" type="1">
<li><p>Row/column swap: Interchanging two rows (or columns) multiplies
the determinant by <span class="math inline">\(-1\)</span>. 行/列交换：
交换两行（或列）将行列式乘以 <span class="math inline">\(-1\)</span>
。</p></li>
<li><p>Row/column scaling: Multiplying a row (or column) by a scalar
<span class="math inline">\(c\)</span> multiplies the determinant by
<span class="math inline">\(c\)</span>. 行/列缩放： 将行（或列）乘以标量
<span class="math inline">\(c\)</span> 会将行列式乘以 <span
class="math inline">\(c\)</span> 。</p></li>
<li><p>Row/column addition: Adding a multiple of one row to another does
not change the determinant.
行/列加法：将一行的倍数添加到另一行不会改变行列式。</p></li>
<li><p>Transpose: 转置：</p></li>
</ol>
<p><span class="math display">\[
\det(A^T) = \det(A).
\]</span></p>
<ol start="7" type="1">
<li>Multiplicativity: 乘法性：</li>
</ol>
<p><span class="math display">\[
\det(AB) = \det(A)\det(B).
\]</span></p>
<ol start="8" type="1">
<li>Invertibility: <span class="math inline">\(A\)</span> is invertible
if and only if <span class="math inline">\(\det(A) \neq 0\)</span>.
可逆性： 当且仅当 <span class="math inline">\(\det(A) \neq 0\)</span>
时， <span class="math inline">\(A\)</span> 才是可逆的。</li>
</ol>
<h3 id="example-computations">Example Computations</h3>
<p>计算示例</p>
<p>Example 6.2.1. For 例 6.2.1. 对于</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 0 &amp; 0 \\1 &amp; 3 &amp; 0 \\-1 &amp; 4
&amp; 5\end{bmatrix},
\]</span></p>
<p><span class="math inline">\(A\)</span> is lower triangular, so <span
class="math inline">\(A\)</span> 是下三角，所以</p>
<p><span class="math display">\[
\det(A) = 2 \cdot 3 \cdot 5 = 30.
\]</span></p>
<p>Example 6.2.2. Let 例 6.2.2. 设</p>
<p><span class="math display">\[
B = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}, \quad C =
\begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}.
\]</span></p>
<p>Then 然后</p>
<p><span class="math display">\[
\det(B) = 1\cdot 4 - 2\cdot 3 = -2, \quad \det(C) = -1.
\]</span></p>
<p>Since <span class="math inline">\(CB\)</span> is obtained by swapping
rows of <span class="math inline">\(B\)</span>, 由于 <span
class="math inline">\(CB\)</span> 是通过交换 <span
class="math inline">\(B\)</span> 的行获得的，</p>
<p><span class="math display">\[
\det(CB) = -\det(B) = 2.
\]</span></p>
<p>This matches the multiplicativity rule: <span
class="math inline">\(\det(CB) = \det(C)\det(B) = (-1)(-2) = 2.\)</span>
这符合乘法规则： <span class="math inline">\(\det(CB) = \det(C)\det(B) =
(-1)(-2) = 2.\)</span></p>
<h3 id="geometric-insights">Geometric Insights</h3>
<p>几何洞察</p>
<ul>
<li>Row swaps: flipping orientation of space.
行交换：翻转空间的方向。</li>
<li>Scaling a row: stretching space in one direction.
缩放一行：朝一个方向拉伸空间。</li>
<li>Row replacement: sliding hyperplanes without altering volume.
行替换：滑动超平面而不改变体积。</li>
<li>Multiplicativity: performing two transformations multiplies their
scaling factors. 乘法性：执行两个变换会将它们的比例因子相乘。</li>
</ul>
<p>These properties make determinants both computationally manageable
and geometrically interpretable.
这些性质使得行列式既易于计算管理，又易于几何解释。</p>
<h3 id="why-this-matters-20">Why this matters</h3>
<p>为什么这很重要</p>
<p>Determinant properties connect computation with geometry and theory.
They explain why Gaussian elimination works, why invertibility is
equivalent to nonzero determinant, and why determinants naturally arise
in areas like volume computation, eigenvalue theory, and differential
equations.
行列式的性质将计算与几何和理论联系起来。它们解释了高斯消元法为何有效，可逆性为何等价于非零行列式，以及行列式为何自然地出现在体积计算、特征值理论和微分方程等领域。</p>
<h3 id="exercises-6.2">Exercises 6.2</h3>
<p>练习 6.2</p>
<ol type="1">
<li>Compute the determinant of 计算行列式</li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; 1 &amp; 4 \\ 0 &amp; 0
&amp; 2 \end{bmatrix}.
\]</span></p>
<ol start="2" type="1">
<li><p>Show that if two rows of a square matrix are identical, then its
determinant is zero. 证明如果方阵的两行相同，则其行列式为零。</p></li>
<li><p>Verify <span class="math inline">\(\det(A^T) = \det(A)\)</span>
for 验证 <span class="math inline">\(\det(A^T) =
\det(A)\)</span></p></li>
</ol>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; -1 \\ 3 &amp; 4 \end{bmatrix}.
\]</span></p>
<ol start="4" type="1">
<li>If <span class="math inline">\(A\)</span> is invertible, prove that
如果 <span class="math inline">\(A\)</span> 可逆，则证明</li>
</ol>
<p><span class="math display">\[
\det(A^{-1}) = \frac{1}{\det(A)}.
\]</span></p>
<ol start="5" type="1">
<li>Suppose <span class="math inline">\(A\)</span> is a <span
class="math inline">\(3\\times 3\)</span>matrix with<span
class="math inline">\(\\det(A) = 5\)</span>. What is <span
class="math inline">\(\\det(2A)\)</span>? 假设 <span
class="math inline">\(A\)</span> 是 $3\times 3 <span
class="math inline">\(matrix with\)</span> \det(A) = 5 $. What is $
\det(2A)$？</li>
</ol>
<h2 id="cofactor-expansion">6.3 Cofactor Expansion</h2>
<p>6.3 辅因子展开</p>
<p>While determinants of small matrices can be computed directly from
formulas, larger matrices require a systematic method. The cofactor
expansion (also known as Laplace expansion) provides a recursive way to
compute determinants by breaking them into smaller ones.
虽然小矩阵的行列式可以直接通过公式计算，但较大的矩阵则需要系统的方法。余因子展开式（也称为拉普拉斯展开式）通过将行列式分解为更小的矩阵，提供了一种递归计算行列式的方法。</p>
<h3 id="minors-and-cofactors">Minors and Cofactors</h3>
<p>小式和辅因子</p>
<p>For an <span class="math inline">\(n \times n\)</span> matrix <span
class="math inline">\(A = [a_{ij}]\)</span>: 对于 <span
class="math inline">\(n \times n\)</span> 矩阵 <span
class="math inline">\(A = [a_{ij}]\)</span> ：</p>
<ul>
<li>The minor <span class="math inline">\(M_{ij}\)</span> is the
determinant of the <span class="math inline">\((n-1) \times
(n-1)\)</span> matrix obtained by deleting the <span
class="math inline">\(i\)</span>-th row and <span
class="math inline">\(j\)</span> -th column of <span
class="math inline">\(A\)</span>. 小调𝑀 𝑖 𝑗 M 伊奇 ​ 是删除第 <span
class="math inline">\(i\)</span> 行和 <span
class="math inline">\(j\)</span> 后得到的 <span
class="math inline">\((n-1) \times (n-1)\)</span> 矩阵的行列式 <span
class="math inline">\(A\)</span> 的第列。</li>
<li>The cofactor <span class="math inline">\(C_{ij}\)</span> is defined
by 辅因子𝐶 𝑖 𝑗 C 伊奇 ​ 定义为</li>
</ul>
<p><span class="math display">\[
C_{ij} = (-1)^{i+j} M_{ij}.
\]</span></p>
<p>The sign factor <span class="math inline">\((-1)^{i+j}\)</span>
alternates in a checkerboard pattern: 符号因子 <span
class="math inline">\((-1)^{i+j}\)</span> 以棋盘格图案交替出现：</p>
<p><span class="math display">\[
\begin{bmatrix}+ &amp; - &amp; + &amp; - &amp; \cdots \\- &amp; + &amp;
- &amp; + &amp; \cdots \\+ &amp; - &amp; + &amp; - &amp; \cdots \\\vdots
&amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots\end{bmatrix}.
\]</span></p>
<h3 id="cofactor-expansion-formula">Cofactor Expansion Formula</h3>
<p>辅因式展开公式</p>
<p>The determinant of <span class="math inline">\(A\)</span> can be
computed by expanding along any row or any column: <span
class="math inline">\(A\)</span>
的行列式可以通过沿任意行或任意列展开来计算：</p>
<p><span class="math display">\[
\det(A) = \sum_{j=1}^n a_{ij} C_{ij} \quad \text{(expansion along row
\(i\))},
\]</span></p>
<p><span class="math display">\[
\det(A) = \sum_{i=1}^n a_{ij} C_{ij} \quad \text{(expansion along column
\(j\))}.
\]</span></p>
<h3 id="example-4">Example</h3>
<p>例子</p>
<p>Example 6.3.1. Compute 例 6.3.1. 计算</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 &amp; 3 \\0 &amp; 4 &amp; 5 \\1 &amp; 0
&amp; 6\end{bmatrix}.
\]</span></p>
<p>Expand along the first row: 沿第一行展开：</p>
<p><span class="math display">\[
\det(A) = 1 \cdot C_{11} + 2 \cdot C_{12} + 3 \cdot C_{13}.
\]</span></p>
<ul>
<li>For <span class="math inline">\(C_{11}\)</span>: 对于𝐶 11 C 11 ​
:</li>
</ul>
<p><span class="math display">\[
M_{11} = \det \begin{bmatrix} 4 &amp; 5 \\ 0 &amp; 6 \end{bmatrix} = 24
\]</span></p>
<p>so <span class="math inline">\(C_{11} = (+1)(24) = 24\)</span>. 所以
<span class="math inline">\(C_{11} = (+1)(24) = 24\)</span> 。</p>
<ul>
<li>For <span class="math inline">\(C_{12}\)</span>: 对于𝐶 12 C 12 ​
:</li>
</ul>
<p><span class="math display">\[
M_{12} = \det \begin{bmatrix} 0 &amp; 5 \\ 1 &amp; 6 \end{bmatrix} = 0 -
5 = -5
\]</span></p>
<p>so <span class="math inline">\(C_{12} = (-1)(-5) = 5\)</span>. 所以
<span class="math inline">\(C_{12} = (-1)(-5) = 5\)</span> 。</p>
<ul>
<li>For <span class="math inline">\(C_{13}\)</span>: 对于𝐶 13 C 13 ​
:</li>
</ul>
<p><span class="math display">\[
M_{13} = \det \begin{bmatrix} 0 &amp; 4 \\ 1 &amp; 0 \end{bmatrix} = 0 -
4 = -4
\]</span></p>
<p>so <span class="math inline">\(C_{13} = (+1)(-4) = -4\)</span>. 所以
<span class="math inline">\(C_{13} = (+1)(-4) = -4\)</span> 。</p>
<p>Thus, 因此，</p>
<p><span class="math display">\[
\det(A) = 1(24) + 2(5) + 3(-4) = 24 + 10 - 12 = 22.
\]</span></p>
<h3 id="properties-of-cofactor-expansion">Properties of Cofactor
Expansion</h3>
<p>辅因子展开的性质</p>
<ol type="1">
<li>Expansion along any row or column yields the same result.
沿任意行或列扩展都会产生相同的结果。</li>
<li>The cofactor expansion provides a recursive definition of
determinant: a determinant of size <span
class="math inline">\(n\)</span> is expressed in terms of determinants
of size <span class="math inline">\(n-1\)</span>.
余因子展开提供了行列式的递归定义：大小为 <span
class="math inline">\(n\)</span> 的行列式可以用大小为 <span
class="math inline">\(n-1\)</span> 的行列式来表示。</li>
<li>Cofactors are fundamental in constructing the adjugate matrix, which
gives a formula for inverses:
余因子是构造伴随矩阵的基础，它给出了逆的公式：</li>
</ol>
<p><span class="math display">\[
A^{-1} = \frac{1}{\det(A)} \, \text{adj}(A), \quad \text{where adj}(A) =
[C_{ji}].
\]</span></p>
<h3 id="geometric-interpretation-11">Geometric Interpretation</h3>
<p>几何解释</p>
<p>Cofactor expansion breaks down the determinant into contributions
from sub-volumes defined by fixing one row or column at a time. Each
cofactor measures how that row/column influences the overall volume
scaling.
余因子展开将行列式分解为由每次固定一行或一列定义的子体积的贡献。每个余因子衡量该行/列对整体体积缩放的影响。</p>
<h3 id="why-this-matters-21">Why this matters</h3>
<p>为什么这很重要</p>
<p>Cofactor expansion generalizes the small-matrix formulas and provides
a conceptual definition of determinants. While not the most efficient
way to compute determinants for large matrices, it is essential for
theory, proofs, and connections to adjugates, Cramer’s rule, and
classical geometry.
余因子展开式推广了小矩阵公式，并提供了行列式的概念定义。虽然它并非计算大矩阵行列式的最有效方法，但它对于理论、证明以及与伴随项、克莱姆规则和古典几何的联系至关重要。</p>
<h3 id="exercises-6.3">Exercises 6.3</h3>
<p>练习 6.3</p>
<ol type="1">
<li>Compute the determinant of 计算行列式</li>
</ol>
<p><span class="math display">\[
\begin{bmatrix}2 &amp; 0 &amp; 1 \\3 &amp; -1 &amp; 4 \\1 &amp; 2 &amp;
0\end{bmatrix}
\]</span></p>
<p>by cofactor expansion along the first column.
通过沿第一列的余因子展开。</p>
<ol start="2" type="1">
<li><p>Verify that expanding along the second row of Example 6.3.1 gives
the same determinant. 验证沿示例 6.3.1
的第二行展开是否给出相同的行列式。</p></li>
<li><p>Prove that expansion along any row gives the same value.
证明沿任何行展开都会给出相同的值。</p></li>
<li><p>Show that if a row of a matrix is zero, then its determinant is
zero. 证明如果矩阵的某一行是零，那么它的行列式也是零。</p></li>
<li><p>Use cofactor expansion to prove that <span
class="math inline">\(\det(A) = \det(A^T)\)</span>. 使用余因子展开来证明
<span class="math inline">\(\det(A) = \det(A^T)\)</span> 。</p></li>
</ol>
<h2 id="applications-volume-invertibility-test">6.4 Applications
(Volume, Invertibility Test)</h2>
<p>6.4 应用（体积、可逆性测试）</p>
<p>Determinants are not merely algebraic curiosities; they have concrete
geometric and computational uses. Two of the most important applications
are measuring volumes and testing invertibility of matrices.
行列式不仅仅是代数上的奇闻；它们有着具体的几何和计算用途。其中最重要的两个应用是测量体积和检验矩阵的可逆性。</p>
<h3 id="determinants-as-volume-scalers">Determinants as Volume
Scalers</h3>
<p>决定因素作为体积标量</p>
<p>Given vectors <span class="math inline">\(\mathbf{v}_1, \mathbf{v}_2,
\dots, \mathbf{v}_n \in \mathbb{R}^n\)</span>, arrange them as columns
of a matrix: 给定向量 <span class="math inline">\(\mathbf{v}_1,
\mathbf{v}_2, \dots, \mathbf{v}_n \in \mathbb{R}^n\)</span>
，将它们排列为矩阵的列：</p>
<p><span class="math display">\[
A = \begin{bmatrix}| &amp; | &amp; &amp; | \\\mathbf{v}_1 &amp;
\mathbf{v}_2 &amp; \cdots &amp; \mathbf{v}_n \\| &amp; | &amp; &amp;
|\end{bmatrix}.
\]</span></p>
<p>Then <span class="math inline">\(|\det(A)|\)</span> equals the volume
of the parallelepiped spanned by these vectors. 那么 <span
class="math inline">\(|\det(A)|\)</span>
等于这些向量所跨越的平行六面体的体积。</p>
<ul>
<li>In <span class="math inline">\(\mathbb{R}^2\)</span>, <span
class="math inline">\(|\det(A)|\)</span> gives the area of the
parallelogram spanned by <span class="math inline">\(\mathbf{v}_1,
\mathbf{v}_2\)</span>. 在 <span
class="math inline">\(\mathbb{R}^2\)</span> 中， <span
class="math inline">\(|\det(A)|\)</span> 给出由 𝑣 构成的平行四边形的面积
1 , 𝑣 2 v 1 ​ ，v 2 ​ .</li>
<li>In <span class="math inline">\(\mathbb{R}^3\)</span>, <span
class="math inline">\(|\det(A)|\)</span> gives the volume of the
parallelepiped spanned by <span class="math inline">\(\mathbf{v}_1,
\mathbf{v}_2, \mathbf{v}_3\)</span>. 在 <span
class="math inline">\(\mathbb{R}^3\)</span> 中， <span
class="math inline">\(|\det(A)|\)</span> 给出平行六面体的体积，跨度为 𝑣
1 , 𝑣 2 , 𝑣 3 v 1 ​ ，v 2 ​ ，v 3 ​ .</li>
<li>In higher dimensions, it generalizes to <span
class="math inline">\(n\)</span>-dimensional volume (hypervolume).
在更高维度中，它可以推广到 <span class="math inline">\(n\)</span>
维体积（超体积）。</li>
</ul>
<p>Example 6.4.1. Let 例 6.4.1. 设</p>
<p><span class="math display">\[
\mathbf{v}_1 = (1,0,0), \quad \mathbf{v}_2 = (1,1,0), \quad \mathbf{v}_3
= (1,1,1).
\]</span></p>
<p>Then 然后</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 1 &amp; 1 \\0 &amp; 1 &amp; 1 \\0 &amp; 0
&amp; 1\end{bmatrix}, \quad \det(A) = 1.
\]</span></p>
<p>So the parallelepiped has volume 1, even though the vectors are not
orthogonal. 因此，即使向量不正交，平行六面体的体积也是 1 。</p>
<h3 id="invertibility-test">Invertibility Test</h3>
<p>可逆性测试</p>
<p>A square matrix <span class="math inline">\(A\)</span> is invertible
if and only if <span class="math inline">\(\det(A) \neq 0\)</span>. 方阵
<span class="math inline">\(A\)</span> 可逆当且仅当 <span
class="math inline">\(\det(A) \neq 0\)</span> 。</p>
<ul>
<li>If <span class="math inline">\(\det(A) = 0\)</span>: the
transformation collapses space into a lower dimension (area/volume is
zero). No inverse exists. 如果 <span class="math inline">\(\det(A) =
0\)</span>
：变换将空间塌缩至较低维度（面积/体积为零）。不存在逆变换。</li>
<li>If <span class="math inline">\(\det(A) \neq 0\)</span>: the
transformation scales volume by <span
class="math inline">\(|\det(A)|\)</span>, and is reversible. 如果 <span
class="math inline">\(\det(A) \neq 0\)</span> ：变换将体积缩放 <span
class="math inline">\(|\det(A)|\)</span> ，并且是可逆的。</li>
</ul>
<p>Example 6.4.2. The matrix 例 6.4.2. 矩阵</p>
<p><span class="math display">\[
B = \begin{bmatrix} 2 &amp; 4 \\ 1 &amp; 2 \end{bmatrix}
\]</span></p>
<p>has determinant <span class="math inline">\(\det(B) = 2 \cdot 2 - 4
\cdot 1 = 0\)</span>. Thus, <span class="math inline">\(B\)</span> is
not invertible. Geometrically, the two column vectors are collinear,
spanning only a line in <span
class="math inline">\(\mathbb{R}^2\)</span>. 行列式为 <span
class="math inline">\(\det(B) = 2 \cdot 2 - 4 \cdot 1 = 0\)</span>
。因此， <span class="math inline">\(B\)</span>
不可逆。几何上，这两个列向量共线，在 <span
class="math inline">\(\mathbb{R}^2\)</span> 中仅延伸一条线。</p>
<h3 id="cramers-rule">Cramer’s Rule</h3>
<p>克莱默规则</p>
<p>Determinants also provide an explicit formula for solving systems of
linear equations when the matrix is invertible. For <span
class="math inline">\(A\mathbf{x} = \mathbf{b}\)</span> with <span
class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>:
当矩阵可逆时，行列式还提供了求解线性方程组的明确公式。 对于带有 <span
class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> 的 <span
class="math inline">\(A\mathbf{x} = \mathbf{b}\)</span> ：</p>
<p><span class="math display">\[
x_i = \frac{\det(A_i)}{\det(A)},
\]</span></p>
<p>where <span class="math inline">\(A_i\)</span> is obtained by
replacing the <span class="math inline">\(i\)</span>-th column of <span
class="math inline">\(A\)</span> with <span
class="math inline">\(\mathbf{b}\)</span>. While inefficient
computationally, Cramer’s rule highlights the determinant’s role in
solutions and uniqueness. 其中𝐴 𝑖 A i ​ 通过将 <span
class="math inline">\(A\)</span> 的第 <span
class="math inline">\(i\)</span> 列替换为 <span
class="math inline">\(\mathbf{b}\)</span>
得到。克莱姆规则虽然计算效率低下，但它凸显了行列式在解和唯一性方面的作用。</p>
<h3 id="orientation">Orientation</h3>
<p>方向</p>
<p>The sign of <span class="math inline">\(\det(A)\)</span> indicates
whether a transformation preserves or reverses orientation. For example,
a reflection in the plane has determinant <span
class="math inline">\(-1\)</span>, flipping handedness. <span
class="math inline">\(\det(A)\)</span>
的符号表示变换是保持方向还是反转方向。例如，平面上的反射具有行列式 <span
class="math inline">\(-1\)</span> ，即翻转旋向性。</p>
<h3 id="why-this-matters-22">Why this matters</h3>
<p>为什么这很重要</p>
<p>Determinants condense key information: they measure scaling, test
invertibility, and track orientation. These insights are indispensable
in geometry (areas and volumes), analysis (Jacobian determinants in
calculus), and computation ( solving systems and checking singularity).
行列式浓缩了关键信息：它们测量缩放比例、检验可逆性并追踪方向。这些洞见在几何学（面积和体积）、分析学（微积分中的雅可比行列式）和计算学（求解系统和检查奇点）中都不可或缺。</p>
<h3 id="exercises-6.4">Exercises 6.4</h3>
<p>练习 6.4</p>
<ol type="1">
<li><p>Compute the area of the parallelogram spanned by <span
class="math inline">\((2,1)\)</span> and <span
class="math inline">\((1,3)\)</span>. 计算 <span
class="math inline">\((2,1)\)</span> 和 <span
class="math inline">\((1,3)\)</span> 所构成的平行四边形的面积。</p></li>
<li><p>Find the volume of the parallelepiped spanned by <span
class="math inline">\((1,0,0), (1,1,0), (1,1,1)\)</span>. 求出 <span
class="math inline">\((1,0,0), (1,1,0), (1,1,1)\)</span>
所跨度的平行六面体的体积。</p></li>
<li><p>Determine whether the matrix 确定矩阵</p></li>
</ol>
<p><span class="math display">\[
\begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 6 \end{bmatrix}
\]</span></p>
<p>is invertible. Justify using determinants. 4. Use Cramer’s rule to
solve 是可逆的。用行列式证明。4. 使用克莱姆规则求解</p>
<p><span class="math display">\[
\begin{cases}x + y = 3, \\2x - y = 0.\end{cases}
\]</span></p>
<ol start="5" type="1">
<li>Explain geometrically why a determinant of zero implies no inverse
exists. 从几何角度解释为什么行列式为零意味着不存在逆元。</li>
</ol>
<h1 id="chapter-7.-inner-product-spaces">Chapter 7. Inner Product
Spaces</h1>
<p>第七章内积空间</p>
<h2 id="inner-products-and-norms">7.1 Inner Products and Norms</h2>
<p>7.1 内积和范数</p>
<p>To extend the geometric ideas of length, distance, and angle beyond
<span class="math inline">\(\mathbb{R}^2\)</span> and <span
class="math inline">\(\mathbb{R}^3\)</span>, we introduce inner
products. Inner products provide a way of measuring similarity between
vectors, while norms derived from them measure length. These concepts
are the foundation of geometry inside vector spaces.
为了将长度、距离和角度的几何概念扩展到 <span
class="math inline">\(\mathbb{R}^2\)</span> 和 <span
class="math inline">\(\mathbb{R}^3\)</span>
之外，我们引入了内积。内积提供了一种度量向量之间相似性的方法，而由内积导出的范数则用于度量长度。这些概念是向量空间几何的基础。</p>
<h3 id="inner-product">Inner Product</h3>
<p>内积</p>
<p>An inner product on a real vector space <span
class="math inline">\(V\)</span> is a function 实向量空间 <span
class="math inline">\(V\)</span> 上的内积是一个函数</p>
<p><span class="math display">\[
\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}
\]</span></p>
<p>that assigns to each pair of vectors <span
class="math inline">\((\mathbf{u}, \mathbf{v})\)</span> a real number,
subject to the following properties: 为每对向量 <span
class="math inline">\((\mathbf{u}, \mathbf{v})\)</span>
分配一个实数，并遵循以下属性：</p>
<ol type="1">
<li><p>Symmetry: <span class="math inline">\(\langle \mathbf{u},
\mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.\)</span>
对称： <span class="math inline">\(\langle \mathbf{u}, \mathbf{v}
\rangle = \langle \mathbf{v}, \mathbf{u} \rangle.\)</span></p></li>
<li><p>Linearity in the first argument: <span
class="math inline">\(\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v}
\rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle
\mathbf{w}, \mathbf{v} \rangle.\)</span> 第一个参数的线性： <span
class="math inline">\(\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v}
\rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle
\mathbf{w}, \mathbf{v} \rangle.\)</span></p></li>
<li><p>Positive-definiteness: <span class="math inline">\(\langle
\mathbf{v}, \mathbf{v} \rangle \geq 0\)</span>, and equality holds if
and only if <span class="math inline">\(\mathbf{v} =
\mathbf{0}\)</span>. 正定性： <span class="math inline">\(\langle
\mathbf{v}, \mathbf{v} \rangle \geq 0\)</span> ，且仅当 <span
class="math inline">\(\mathbf{v} = \mathbf{0}\)</span>
时等式成立。</p></li>
</ol>
<p>The standard inner product on <span
class="math inline">\(\mathbb{R}^n\)</span> is the dot product: <span
class="math inline">\(\mathbb{R}^n\)</span> 上的标准内积是点积：</p>
<p><span class="math display">\[
\langle \mathbf{u}, \mathbf{v} \rangle = u_1 v_1 + u_2 v_2 + \cdots +
u_n v_n.
\]</span></p>
<h3 id="norms">Norms</h3>
<p>规范</p>
<p>The norm of a vector is its length, defined in terms of the inner
product: 向量的范数是其长度，根据内积定义：</p>
<p><span class="math display">\[
\|\mathbf{v}\| = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}.
\]</span></p>
<p>For the dot product in <span
class="math inline">\(\mathbb{R}^n\)</span>: 对于 <span
class="math inline">\(\mathbb{R}^n\)</span> 中的点积：</p>
<p><span class="math display">\[
\|(x_1, x_2, \dots, x_n)\| = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}.
\]</span></p>
<h3 id="angles-between-vectors-1">Angles Between Vectors</h3>
<p>向量之间的角度</p>
<p>The inner product allows us to define the angle <span
class="math inline">\(\theta\)</span> between two nonzero vectors <span
class="math inline">\(\mathbf{u}, \mathbf{v}\)</span> by
通过内积，我们可以定义两个非零向量 <span
class="math inline">\(\mathbf{u}, \mathbf{v}\)</span> 之间的角度 <span
class="math inline">\(\theta\)</span> ，即</p>
<p><span class="math display">\[
\cos \theta = \frac{\langle \mathbf{u}, \mathbf{v}
\rangle}{\|\mathbf{u}\| \, \|\mathbf{v}\|}.
\]</span></p>
<p>Thus, two vectors are orthogonal if <span
class="math inline">\(\langle \mathbf{u}, \mathbf{v} \rangle =
0\)</span>. 因此，若 <span class="math inline">\(\langle \mathbf{u},
\mathbf{v} \rangle = 0\)</span> ，则两个向量正交。</p>
<h3 id="examples-5">Examples</h3>
<p>示例</p>
<p>Example 7.1.1. In <span class="math inline">\(\mathbb{R}^2\)</span>,
with <span class="math inline">\(\mathbf{u} = (1,2)\)</span>, <span
class="math inline">\(\mathbf{v} = (3,4)\)</span>: 例 7.1.1。 在 <span
class="math inline">\(\mathbb{R}^2\)</span> 中，使用 <span
class="math inline">\(\mathbf{u} = (1,2)\)</span> 、 <span
class="math inline">\(\mathbf{v} = (3,4)\)</span> ：</p>
<p><span class="math display">\[
\langle \mathbf{u}, \mathbf{v} \rangle = 1\cdot 3 + 2\cdot 4 = 11.
\]</span></p>
<p><span class="math display">\[
\|\mathbf{u}\| = \sqrt{1^2 + 2^2} = \sqrt{5}, \quad \|\mathbf{v}\| =
\sqrt{3^2 + 4^2} = 5.
\]</span></p>
<p>So, 所以，</p>
<p><span class="math display">\[
\cos \theta = \frac{11}{\sqrt{5}\cdot 5}.
\]</span></p>
<p>Example 7.1.2. In the function space <span
class="math inline">\(C[0,1]\)</span>, the inner product 例 7.1.2。
在函数空间 <span class="math inline">\(C[0,1]\)</span> 中，内积</p>
<p><span class="math display">\[
\langle f, g \rangle = \int_0^1 f(x) g(x)\, dx
\]</span></p>
<p>defines a length 定义长度</p>
<p><span class="math display">\[
\|f\| = \sqrt{\int_0^1 f(x)^2 dx}.
\]</span></p>
<p>This generalizes geometry to infinite-dimensional spaces.
这将几何学推广到无限维空间。</p>
<h3 id="geometric-interpretation-12">Geometric Interpretation</h3>
<p>几何解释</p>
<ul>
<li>Inner product: measures similarity between vectors.
内积：测量向量之间的相似性。</li>
<li>Norm: length of a vector. 范数：向量的长度。</li>
<li>Angle: measure of alignment between two directions.
角度：两个方向之间的对齐度量。</li>
</ul>
<p>These concepts unify algebraic operations with geometric intuition.
这些概念将代数运算与几何直觉统一起来。</p>
<h3 id="why-this-matters-23">Why this matters</h3>
<p>为什么这很重要</p>
<p>Inner products and norms allow us to extend geometry into abstract
vector spaces. They form the basis of orthogonality, projections,
Fourier series, least squares approximation, and many applications in
physics and machine learning.
内积和范数使我们能够将几何扩展到抽象向量空间。它们构成了正交性、投影、傅里叶级数、最小二乘近似以及物理学和机器学习中许多应用的基础。</p>
<h3 id="exercises-7.1">Exercises 7.1</h3>
<p>练习 7.1</p>
<ol type="1">
<li><p>Compute <span class="math inline">\(\langle (2,-1,3), (1,4,0)
\rangle\)</span>. Then find the angle between them. 计算 <span
class="math inline">\(\langle (2,-1,3), (1,4,0) \rangle\)</span>
。然后求出它们之间的角度。</p></li>
<li><p>Show that <span class="math inline">\(\|(x,y)\| =
\sqrt{x^2+y^2}\)</span> satisfies the properties of a norm. 证明∥ ( 𝑥 ,
𝑦 ) ∥ = 𝑥 2 + 𝑦 2 ∥(x,y)∥= x 2 +y 2 ​ 满足范数的性质。</p></li>
<li><p>In <span class="math inline">\(\mathbb{R}^3\)</span>, verify that
<span class="math inline">\((1,1,0)\)</span> and <span
class="math inline">\((1,-1,0)\)</span> are orthogonal. 在 <span
class="math inline">\(\mathbb{R}^3\)</span> 中，验证 <span
class="math inline">\((1,1,0)\)</span> 和 <span
class="math inline">\((1,-1,0)\)</span> 是否正交。</p></li>
<li><p>In <span class="math inline">\(C[0,1]\)</span>, compute <span
class="math inline">\(\langle f,g \rangle\)</span> for <span
class="math inline">\(f(x)=x\)</span>, <span
class="math inline">\(g(x)=1\)</span>. 在 <span
class="math inline">\(C[0,1]\)</span> 中，计算 <span
class="math inline">\(f(x)=x\)</span> 、 <span
class="math inline">\(g(x)=1\)</span> 的 <span
class="math inline">\(\langle f,g \rangle\)</span> 。</p></li>
<li><p>Prove the Cauchy–Schwarz inequality: 证明柯西-施瓦茨不等式：</p>
<p><span class="math display">\[
|\langle \mathbf{u}, \mathbf{v} \rangle| \leq \|\mathbf{u}\| \,
\|\mathbf{v}\|.
\]</span></p></li>
</ol>
<h2 id="orthogonal-projections">7.2 Orthogonal Projections</h2>
<p>7.2 正交投影</p>
<p>One of the most useful applications of inner products is the notion
of orthogonal projection. Projection allows us to approximate a vector
by another lying in a subspace, minimizing error in the sense of
distance. This idea underpins geometry, statistics, and numerical
analysis.
内积最有用的应用之一是正交投影的概念。投影使我们能够用子空间中的另一个向量来近似一个向量，从而最小化距离方向上的误差。这一思想是几何、统计学和数值分析的基础。</p>
<h3 id="projection-onto-a-line">Projection onto a Line</h3>
<p>投影到线上</p>
<p>Let <span class="math inline">\(\mathbf{u} \in \mathbb{R}^n\)</span>
be a nonzero vector. The line spanned by <span
class="math inline">\(\mathbf{u}\)</span> is 令 <span
class="math inline">\(\mathbf{u} \in \mathbb{R}^n\)</span> 为非零向量。
<span class="math inline">\(\mathbf{u}\)</span> 所构成的线段为</p>
<p><span class="math display">\[
L = \{ c\mathbf{u} \mid c \in \mathbb{R} \}.
\]</span></p>
<p>Given a vector <span class="math inline">\(\mathbf{v}\)</span>, the
projection of <span class="math inline">\(\mathbf{v}\)</span> onto <span
class="math inline">\(\mathbf{u}\)</span> is the vector in <span
class="math inline">\(L\)</span> closest to <span
class="math inline">\(\mathbf{v}\)</span>. Geometrically, it is the
shadow of <span class="math inline">\(\mathbf{v}\)</span> on the line.
给定向量 <span class="math inline">\(\mathbf{v}\)</span> ， <span
class="math inline">\(\mathbf{v}\)</span> 在 <span
class="math inline">\(\mathbf{u}\)</span> 上的投影是 <span
class="math inline">\(L\)</span> 中距离 <span
class="math inline">\(\mathbf{v}\)</span> 最近的向量。从几何学上讲，它是
<span class="math inline">\(\mathbf{v}\)</span> 在线上的阴影。</p>
<p>The formula is 公式是</p>
<p><span class="math display">\[
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\langle \mathbf{v},
\mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle} \,
\mathbf{u}.
\]</span></p>
<p>The error vector <span class="math inline">\(\mathbf{v} -
\text{proj}_{\mathbf{u}}(\mathbf{v})\)</span> is orthogonal to <span
class="math inline">\(\mathbf{u}\)</span>. 误差向量 <span
class="math inline">\(\mathbf{v} -
\text{proj}_{\mathbf{u}}(\mathbf{v})\)</span> 与 <span
class="math inline">\(\mathbf{u}\)</span> 正交。</p>
<h3 id="example-7.2.1">Example 7.2.1</h3>
<p>例 7.2.1</p>
<p>Let <span class="math inline">\(\mathbf{u} = (1,2)\)</span>, <span
class="math inline">\(\mathbf{v} = (3,1)\)</span>. 令 <span
class="math inline">\(\mathbf{u} = (1,2)\)</span> ， <span
class="math inline">\(\mathbf{v} = (3,1)\)</span> 。</p>
<p><span class="math display">\[
\langle \mathbf{v}, \mathbf{u} \rangle = 3\cdot 1 + 1\cdot 2 = 5,
\quad\langle \mathbf{u}, \mathbf{u} \rangle = 1^2 + 2^2 = 5.
\]</span></p>
<p>So 所以</p>
<p><span class="math display">\[
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{5}{5}(1,2) = (1,2).
\]</span></p>
<p>The error vector is <span class="math inline">\((3,1) - (1,2) =
(2,-1)\)</span>, which is orthogonal to <span
class="math inline">\((1,2)\)</span>. 误差向量为 <span
class="math inline">\((3,1) - (1,2) = (2,-1)\)</span> ，与 <span
class="math inline">\((1,2)\)</span> 正交。</p>
<h3 id="projection-onto-a-subspace">Projection onto a Subspace</h3>
<p>投影到子空间</p>
<p>Suppose <span class="math inline">\(W \subseteq \mathbb{R}^n\)</span>
is a subspace with orthonormal basis <span class="math inline">\(\{
\mathbf{w}_1, \dots, \mathbf{w}_k \}\)</span>. The projection of a
vector <span class="math inline">\(\mathbf{v}\)</span> onto <span
class="math inline">\(W\)</span> is 假设 <span class="math inline">\(W
\subseteq \mathbb{R}^n\)</span> 是一个具有正交基 <span
class="math inline">\(\{ \mathbf{w}_1, \dots, \mathbf{w}_k \}\)</span>
的子空间。向量 <span class="math inline">\(\mathbf{v}\)</span> 在 <span
class="math inline">\(W\)</span> 上的投影为</p>
<p><span class="math display">\[
\text{proj}_{W}(\mathbf{v}) = \langle \mathbf{v}, \mathbf{w}_1 \rangle
\mathbf{w}_1 + \cdots + \langle \mathbf{v}, \mathbf{w}_k \rangle
\mathbf{w}_k.
\]</span></p>
<p>This is the unique vector in <span class="math inline">\(W\)</span>
closest to <span class="math inline">\(\mathbf{v}\)</span>. The
difference <span class="math inline">\(\mathbf{v} -
\text{proj}_{W}(\mathbf{v})\)</span> is orthogonal to all of <span
class="math inline">\(W\)</span>. 这是 <span
class="math inline">\(W\)</span> 中与 <span
class="math inline">\(\mathbf{v}\)</span> 最接近的唯一向量。差值 <span
class="math inline">\(\mathbf{v} - \text{proj}_{W}(\mathbf{v})\)</span>
与所有 <span class="math inline">\(W\)</span> 正交。</p>
<h3 id="least-squares-approximation">Least Squares Approximation</h3>
<p>最小二乘近似</p>
<p>Orthogonal projection explains the method of least squares. To solve
an overdetermined system <span class="math inline">\(A\mathbf{x} \approx
\mathbf{b}\)</span>, we seek the <span
class="math inline">\(\mathbf{x}\)</span> that makes <span
class="math inline">\(A\mathbf{x}\)</span> the projection of <span
class="math inline">\(\mathbf{b}\)</span> onto the column space of <span
class="math inline">\(A\)</span>. This gives the normal equations
正交投影解释了最小二乘法。为了解决超定问题 系统 <span
class="math inline">\(A\mathbf{x} \approx \mathbf{b}\)</span> ，我们寻找
<span class="math inline">\(\mathbf{x}\)</span> ，使得 <span
class="math inline">\(A\mathbf{x}\)</span> 成为 <span
class="math inline">\(\mathbf{b}\)</span> 在 <span
class="math inline">\(A\)</span> 的列空间上的投影。这给出了正则方程</p>
<p><span class="math display">\[
A^T A \mathbf{x} = A^T \mathbf{b}.
\]</span></p>
<p>Thus, least squares is just projection in disguise.
因此，最小二乘法只是伪装的投影。</p>
<h3 id="geometric-interpretation-13">Geometric Interpretation</h3>
<p>几何解释</p>
<ul>
<li>Projection finds the closest point in a subspace to a given vector.
投影找到子空间中距离给定向量最近的点。</li>
<li>It minimizes distance (error) in the sense of Euclidean norm.
它按照欧几里得范数的意义最小化距离（误差）。</li>
<li>Orthogonality ensures the error vector points directly away from the
subspace. 正交性确保误差向量直接指向远离子空间的方向。</li>
</ul>
<h3 id="why-this-matters-24">Why this matters</h3>
<p>为什么这很重要</p>
<p>Orthogonal projection is central in both pure and applied
mathematics. It underlies the geometry of subspaces, the theory of
Fourier series, regression in statistics, and approximation methods in
numerical linear algebra. Whenever we fit data with a simpler model,
projection is at work.
正交投影在纯数学和应用数学中都至关重要。它是子空间几何、傅里叶级数理论、统计学中的回归以及数值线性代数中的近似方法的基础。每当我们用更简单的模型拟合数据时，投影就会发挥作用。</p>
<h3 id="exercises-7.2">Exercises 7.2</h3>
<p>练习 7.2</p>
<ol type="1">
<li>Compute the projection of <span class="math inline">\((2,3)\)</span>
onto the vector <span class="math inline">\((1,1)\)</span>. 计算 <span
class="math inline">\((2,3)\)</span> 到向量 <span
class="math inline">\((1,1)\)</span> 的投影。</li>
<li>Show that <span class="math inline">\(\mathbf{v} -
\text{proj}_{\mathbf{u}}(\mathbf{v})\)</span> is orthogonal to <span
class="math inline">\(\mathbf{u}\)</span>. 证明 <span
class="math inline">\(\mathbf{v} -
\text{proj}_{\mathbf{u}}(\mathbf{v})\)</span> 与 <span
class="math inline">\(\mathbf{u}\)</span> 正交。</li>
<li>Let <span class="math inline">\(W = \text{span}\{(1,0,0), (0,1,0)\}
\subseteq \mathbb{R}^3\)</span>. Find the projection of <span
class="math inline">\((1,2,3)\)</span> onto <span
class="math inline">\(W\)</span>. 令 <span class="math inline">\(W =
\text{span}\{(1,0,0), (0,1,0)\} \subseteq \mathbb{R}^3\)</span> 。求
<span class="math inline">\((1,2,3)\)</span> 到 <span
class="math inline">\(W\)</span> 的投影。</li>
<li>Explain why least squares fitting corresponds to projection onto the
column space of <span class="math inline">\(A\)</span>.
解释为什么最小二乘拟合对应于 <span class="math inline">\(A\)</span>
的列空间上的投影。</li>
<li>Prove that projection onto a subspace <span
class="math inline">\(W\)</span> is unique: there is exactly one closest
vector in <span class="math inline">\(W\)</span> to a given <span
class="math inline">\(\mathbf{v}\)</span>. 证明投影到子空间 <span
class="math inline">\(W\)</span> 是唯一的：在 <span
class="math inline">\(W\)</span> 中，有且仅有一个与给定 <span
class="math inline">\(\mathbf{v}\)</span> 最接近的向量。</li>
</ol>
<h2 id="gramschmidt-process">7.3 Gram–Schmidt Process</h2>
<p>7.3 格拉姆-施密特过程</p>
<p>The Gram–Schmidt process is a systematic way to turn any linearly
independent set of vectors into an orthonormal basis. This is especially
useful because orthonormal bases simplify computations: inner products
become simple coordinate comparisons, and projections take clean forms.
格拉姆-施密特过程是一种将任意线性无关的向量集转化为正交基的系统方法。这种方法尤其有用，因为正交基可以简化计算：内积变成了简单的坐标比较，并且投影呈现出清晰的形式。</p>
<h3 id="the-idea">The Idea</h3>
<p>理念</p>
<p>Given a linearly independent set of vectors <span
class="math inline">\(\{\mathbf{v}_1, \mathbf{v}_2, \dots,
\mathbf{v}_n\}\)</span> in an inner product space, we want to construct
an orthonormal set <span class="math inline">\(\{\mathbf{u}_1,
\mathbf{u}_2, \dots, \mathbf{u}_n\}\)</span> that spans the same
subspace. 给定内积空间中一组线性无关的向量 <span
class="math inline">\(\{\mathbf{v}_1, \mathbf{v}_2, \dots,
\mathbf{v}_n\}\)</span> ，我们想要构建一个跨越同一子空间的正交集 <span
class="math inline">\(\{\mathbf{u}_1, \mathbf{u}_2, \dots,
\mathbf{u}_n\}\)</span> 。</p>
<p>We proceed step by step: 我们一步步来：</p>
<ol type="1">
<li>Start with <span class="math inline">\(\mathbf{v}_1\)</span>,
normalize it to get <span class="math inline">\(\mathbf{u}_1\)</span>.
从𝑣开始 1 v 1 ​ ，将其标准化得到𝑢 1 u 1 ​ .</li>
<li>Subtract from <span class="math inline">\(\mathbf{v}_2\)</span> its
projection onto <span class="math inline">\(\mathbf{u}_1\)</span>,
leaving a vector orthogonal to <span
class="math inline">\(\mathbf{u}_1\)</span>. Normalize to get <span
class="math inline">\(\mathbf{u}_2\)</span>. 从𝑣中减去 2 v 2 ​
它在𝑢上的投影 1 u 1 ​ ，留下一个与𝑢正交的向量 1 u 1 ​ . 标准化得到𝑢 2 u 2 ​
.</li>
<li>For each <span class="math inline">\(\mathbf{v}_k\)</span>, subtract
projections onto all previously constructed <span
class="math inline">\(\mathbf{u}_1, \dots, \mathbf{u}_{k-1}\)</span>,
then normalize. 对于每个𝑣 𝑘 v k ​ ，减去所有先前构建的𝑢上的投影 1 , … , 𝑢
𝑘 − 1 u 1 ​ ，…，你 k−1 ​ ，然后标准化。</li>
</ol>
<h3 id="the-algorithm">The Algorithm</h3>
<p>算法</p>
<p>For <span class="math inline">\(k = 1, 2, \dots, n\)</span>: 对于
<span class="math inline">\(k = 1, 2, \dots, n\)</span> ：</p>
<p><span class="math display">\[
\mathbf{w}_k = \mathbf{v}_k - \sum_{j=1}^{k-1} \langle \mathbf{v}_k,
\mathbf{u}_j \rangle \mathbf{u}_j,
\]</span></p>
<p><span class="math display">\[
\mathbf{u}_k = \frac{\mathbf{w}_k}{\|\mathbf{w}_k\|}.
\]</span></p>
<p>The result <span class="math inline">\(\{\mathbf{u}_1, \dots,
\mathbf{u}_n\}\)</span> is an orthonormal basis of the span of the
original vectors. 结果 <span class="math inline">\(\{\mathbf{u}_1,
\dots, \mathbf{u}_n\}\)</span> 是原始向量跨度的正交基。</p>
<h3 id="example-7.3.1">Example 7.3.1</h3>
<p>例 7.3.1</p>
<p>Take <span class="math inline">\(\mathbf{v}_1 = (1,1,0), \
\mathbf{v}_2 = (1,0,1), \ \mathbf{v}_3 = (0,1,1)\)</span> in <span
class="math inline">\(\mathbb{R}^3\)</span>. 在 <span
class="math inline">\(\mathbb{R}^3\)</span> 中乘坐 <span
class="math inline">\(\mathbf{v}_1 = (1,1,0), \ \mathbf{v}_2 = (1,0,1),
\ \mathbf{v}_3 = (0,1,1)\)</span> 。</p>
<ol type="1">
<li>Normalize <span class="math inline">\(\mathbf{v}_1\)</span>: 标准化𝑣
1 v 1 ​ :</li>
</ol>
<p><span class="math display">\[
\mathbf{u}_1 = \frac{1}{\sqrt{2}}(1,1,0).
\]</span></p>
<ol start="2" type="1">
<li>Subtract projection of <span
class="math inline">\(\mathbf{v}_2\)</span> on <span
class="math inline">\(\mathbf{u}_1\)</span>: 减去𝑣的投影 2 v 2 ​ 在𝑢 1 u
1 ​ :</li>
</ol>
<p><span class="math display">\[
\mathbf{w}_2 = \mathbf{v}_2 - \langle \mathbf{v}_2,\mathbf{u}_1 \rangle
\mathbf{u}_1.
\]</span></p>
<p><span class="math display">\[
\langle \mathbf{v}_2,\mathbf{u}_1 \rangle = \frac{1}{\sqrt{2}}(1\cdot 1
+ 0\cdot 1 + 1\cdot 0) = \tfrac{1}{\sqrt{2}}.
\]</span></p>
<p>So 所以</p>
<p><span class="math display">\[
\mathbf{w}_2 = (1,0,1) - \tfrac{1}{\sqrt{2}}\cdot
\tfrac{1}{\sqrt{2}}(1,1,0)= (1,0,1) - \tfrac{1}{2}(1,1,0)=
\left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
\]</span></p>
<p>Normalize: 规范化：</p>
<p><span class="math display">\[
\mathbf{u}_2 = \frac{1}{\sqrt{\tfrac{1}{4}+\tfrac{1}{4}+1}}
\left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)=
\frac{1}{\sqrt{\tfrac{3}{2}}}\left(\tfrac{1}{2}, -\tfrac{1}{2},
1\right).
\]</span></p>
<ol start="3" type="1">
<li>Subtract projections from <span
class="math inline">\(\mathbf{v}_3\)</span>: 从𝑣中减去投影 3 v 3 ​ :</li>
</ol>
<p><span class="math display">\[
\mathbf{w}_3 = \mathbf{v}_3 - \langle \mathbf{v}_3,\mathbf{u}_1 \rangle
\mathbf{u}_1 - \langle \mathbf{v}_3,\mathbf{u}_2 \rangle \mathbf{u}_2.
\]</span></p>
<p>After computing, normalize to obtain <span
class="math inline">\(\mathbf{u}_3\)</span>. 计算后，归一化得到𝑢 3 u 3 ​
.</p>
<p>The result is an orthonormal basis of the span of <span
class="math inline">\(\{\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3\}\)</span>.
结果是 <span
class="math inline">\(\{\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3\}\)</span>
跨度的正交基。</p>
<h3 id="geometric-interpretation-14">Geometric Interpretation</h3>
<p>几何解释</p>
<p>Gram–Schmidt is like straightening out a set of vectors: you start
with the original directions and adjust each new vector to be
perpendicular to all previous ones. Then you scale to unit length. The
process ensures orthogonality while preserving the span.
格拉姆-施密特变换就像拉直一组向量：从原始方向开始，调整每个新向量使其与所有先前的向量垂直。然后缩放到单位长度。这个过程确保了正交性，同时保留了跨度。</p>
<h3 id="why-this-matters-25">Why this matters</h3>
<p>为什么这很重要</p>
<p>Orthonormal bases simplify inner products, projections, and
computations in general. They make coordinate systems easier to work
with and are crucial in numerical methods, QR decomposition, Fourier
analysis, and statistics (orthogonal polynomials, principal component
analysis).
正交基可以简化内积、投影和一般计算。它们使坐标系更易于使用，并且在数值方法、QR
分解、傅里叶分析和统计学（正交多项式、主成分分析）中至关重要。</p>
<h3 id="exercises-7.3">Exercises 7.3</h3>
<p>练习 7.3</p>
<ol type="1">
<li>Apply Gram–Schmidt to <span class="math inline">\((1,0),
(1,1)\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>. 对
<span class="math inline">\(\mathbb{R}^2\)</span> 中的 <span
class="math inline">\((1,0), (1,1)\)</span> 应用 Gram–Schmidt
公式。</li>
<li>Orthogonalize <span class="math inline">\((1,1,1), (1,0,1)\)</span>
in <span class="math inline">\(\mathbb{R}^3\)</span>. 在 <span
class="math inline">\(\mathbb{R}^3\)</span> 中对 <span
class="math inline">\((1,1,1), (1,0,1)\)</span> 进行正交化。</li>
<li>Prove that each step of Gram–Schmidt yields a vector orthogonal to
all previous ones. 证明 Gram-Schmidt
的每一步都会产生一个与所有前面的向量正交的向量。</li>
<li>Show that Gram–Schmidt preserves the span of the original vectors.
证明 Gram–Schmidt 保留了原始向量的跨度。</li>
<li>Explain how Gram–Schmidt leads to the QR decomposition of a matrix.
解释 Gram-Schmidt 如何导致矩阵的 QR 分解。</li>
</ol>
<h2 id="orthonormal-bases">7.4 Orthonormal Bases</h2>
<p>7.4 正交基</p>
<p>An orthonormal basis is a basis of a vector space in which all
vectors are both orthogonal to each other and have unit length. Such
bases are the most convenient possible coordinate systems: computations
involving inner products, projections, and norms become exceptionally
simple.
正交基是向量空间中的一种基，其中所有向量彼此正交且具有单位长度。这样的基是最方便的坐标系：涉及内积、投影和范数的计算变得异常简单。</p>
<h3 id="definition-3">Definition</h3>
<p>定义</p>
<p>A set of vectors <span class="math inline">\(\{\mathbf{u}_1,
\mathbf{u}_2, \dots, \mathbf{u}_n\}\)</span> in an inner product space
<span class="math inline">\(V\)</span> is called an orthonormal basis if
内积空间 <span class="math inline">\(V\)</span> 中的一组向量 <span
class="math inline">\(\{\mathbf{u}_1, \mathbf{u}_2, \dots,
\mathbf{u}_n\}\)</span> 称为正交基，若</p>
<ol type="1">
<li><span class="math inline">\(\langle \mathbf{u}_i, \mathbf{u}_j
\rangle = 0\)</span> whenever <span class="math inline">\(i \neq
j\)</span> (orthogonality), <span class="math inline">\(\langle
\mathbf{u}_i, \mathbf{u}_j \rangle = 0\)</span> 每当 <span
class="math inline">\(i \neq j\)</span> （正交性）</li>
<li><span class="math inline">\(\|\mathbf{u}_i\| = 1\)</span> for all
<span class="math inline">\(i\)</span> (normalization), 对所有 <span
class="math inline">\(i\)</span> 进行 <span
class="math inline">\(\|\mathbf{u}_i\| = 1\)</span> （规范化），</li>
<li>The set spans <span class="math inline">\(V\)</span>. 该集合跨越
<span class="math inline">\(V\)</span> 。</li>
</ol>
<h3 id="examples-6">Examples</h3>
<p>示例</p>
<p>Example 7.4.1. In <span class="math inline">\(\mathbb{R}^2\)</span>,
the standard basis 例 7.4.1. 在 <span
class="math inline">\(\mathbb{R}^2\)</span> 中，标准基础</p>
<p><span class="math display">\[
\mathbf{e}_1 = (1,0), \quad \mathbf{e}_2 = (0,1)
\]</span></p>
<p>is orthonormal under the dot product. 在点积下是正交的。</p>
<p>Example 7.4.2. In <span class="math inline">\(\mathbb{R}^3\)</span>,
the standard basis 例 7.4.2. 在 <span
class="math inline">\(\mathbb{R}^3\)</span> 中，标准基础</p>
<p><span class="math display">\[
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3
= (0,0,1)
\]</span></p>
<p>is orthonormal. 是正交的。</p>
<p>Example 7.4.3. Fourier basis on functions: 例 7.4.3.
函数的傅里叶基：</p>
<p><span class="math display">\[
\{1, \cos x, \sin x, \cos 2x, \sin 2x, \dots\}
\]</span></p>
<p>is an orthogonal set in the space of square-integrable functions on
<span class="math inline">\([-\pi,\pi]\)</span> with inner product 是
<span class="math inline">\([-\pi,\pi]\)</span>
上平方可积函数空间中的正交集，具有内积</p>
<p><span class="math display">\[
\langle f,g \rangle = \int_{-\pi}^{\pi} f(x) g(x)\, dx.
\]</span></p>
<p>After normalization, it becomes an orthonormal basis.
经过归一化之后，它就变成了正交基。</p>
<h3 id="properties">Properties</h3>
<p>特性</p>
<ol type="1">
<li><p>Coordinate simplicity: If <span
class="math inline">\(\{\mathbf{u}_1,\dots,\mathbf{u}_n\}\)</span> is an
orthonormal basis of <span class="math inline">\(V\)</span>, then any
vector <span class="math inline">\(\mathbf{v}\in V\)</span> has
coordinates 坐标简单性：如果 <span
class="math inline">\(\{\mathbf{u}_1,\dots,\mathbf{u}_n\}\)</span> 是
<span class="math inline">\(V\)</span> 的正交基，则任何向量 <span
class="math inline">\(\mathbf{v}\in V\)</span> 都有坐标</p>
<p><span class="math display">\[
[\mathbf{v}] = \begin{bmatrix} \langle \mathbf{v}, \mathbf{u}_1 \rangle
\\ \vdots \\ \langle \mathbf{v}, \mathbf{u}_n \rangle \end{bmatrix}.
\]</span></p>
<p>That is, coordinates are just inner products.
也就是说，坐标只是内积。</p></li>
<li><p>Parseval’s identity: For any <span
class="math inline">\(\mathbf{v} \in V\)</span>, 帕塞瓦尔的身份：
对于任意的 <span class="math inline">\(\mathbf{v} \in V\)</span> ，</p>
<p><span class="math display">\[
\|\mathbf{v}\|^2 = \sum_{i=1}^n |\langle \mathbf{v}, \mathbf{u}_i
\rangle|^2.
\]</span></p></li>
<li><p>Projections: The orthogonal projection onto the span of <span
class="math inline">\(\\{\mathbf{u}_1,\dots,\mathbf{u}_k\\}\)</span> is
预测： 𝑢 跨度上的正交投影 1 , … , 𝑢 𝑘 u 1 ​ ，…，你 k ​ 是</p>
<p><span class="math display">\[
\text{proj}(\mathbf{v}) = \sum_{i=1}^k \langle \mathbf{v}, \mathbf{u}_i
\rangle \mathbf{u}_i.
\]</span></p></li>
</ol>
<h3 id="constructing-orthonormal-bases">Constructing Orthonormal
Bases</h3>
<p>构造正交基</p>
<ul>
<li>Start with any linearly independent set, then apply the Gram–Schmidt
process to obtain an orthonormal set spanning the same subspace.
从任意线性无关集开始，然后应用 Gram-Schmidt
过程来获取跨越相同子空间的正交集。</li>
<li>In practice, orthonormal bases are often chosen for numerical
stability and simplicity of computation.
在实践中，通常选择正交基来实现数值稳定性和计算简单性。</li>
</ul>
<h3 id="geometric-interpretation-15">Geometric Interpretation</h3>
<p>几何解释</p>
<p>An orthonormal basis is like a perfectly aligned and equally scaled
coordinate system. Distances and angles are computed directly using
coordinates without correction factors. They are the ideal rulers of
linear algebra.
正交基就像一个完美对齐且等比例缩放的坐标系。距离和角度直接使用坐标计算，无需校正因子。它们是线性代数的理想标尺。</p>
<h3 id="why-this-matters-26">Why this matters</h3>
<p>为什么这很重要</p>
<p>Orthonormal bases simplify every aspect of linear algebra: solving
systems, computing projections, expanding functions, diagonalizing
symmetric matrices, and working with Fourier series. In data science,
principal component analysis produces orthonormal directions capturing
maximum variance.
正交基简化了线性代数的各个方面：求解系统、计算投影、展开函数、对角化对称矩阵以及处理傅里叶级数。在数据科学中，主成分分析可以生成正交方向，从而捕捉最大方差。</p>
<h3 id="exercises-7.4">Exercises 7.4</h3>
<p>练习 7.4</p>
<ol type="1">
<li>Verify that <span class="math inline">\((1/\\sqrt{2})(1,1)\)</span>
and <span class="math inline">\((1/\\sqrt{2})(1,-1)\)</span> form an
orthonormal basis of <span class="math inline">\(\mathbb{R}^2\)</span>.
验证 <span class="math inline">\((1/\\sqrt{2})(1,1)\)</span> 和 <span
class="math inline">\((1/\\sqrt{2})(1,-1)\)</span> 是否构成 <span
class="math inline">\(\mathbb{R}^2\)</span> 的正交基。</li>
<li>Express <span class="math inline">\((3,4)\)</span> in terms of the
orthonormal basis <span class="math inline">\(\{(1/\\sqrt{2})(1,1),
(1/\\sqrt{2})(1,-1)\}\)</span>. 用正交基 <span
class="math inline">\(\{(1/\\sqrt{2})(1,1),
(1/\\sqrt{2})(1,-1)\}\)</span> 表示 <span
class="math inline">\((3,4)\)</span> 。</li>
<li>Prove Parseval’s identity for <span
class="math inline">\(\\mathbb{R}^n\)</span> with the dot product.
使用点积证明 <span class="math inline">\(\\mathbb{R}^n\)</span>
的帕塞瓦尔恒等式。</li>
<li>Find an orthonormal basis for the plane <span
class="math inline">\(x+y+z=0\)</span> in <span
class="math inline">\(\\mathbb{R}^3\)</span>. 在 <span
class="math inline">\(\\mathbb{R}^3\)</span> 中找出平面 <span
class="math inline">\(x+y+z=0\)</span> 的正交基。</li>
<li>Explain why orthonormal bases are numerically more stable than
arbitrary bases in computations.
解释为什么正交基在计算中比任意基在数值上更稳定。</li>
</ol>
<h1 id="chapter-8.-eigenvalues-and-eigenvectors">Chapter 8. Eigenvalues
and eigenvectors</h1>
<p>第 8 章 特征值和特征向量</p>
<h2 id="definitions-and-intuition">8.1 Definitions and Intuition</h2>
<p>8.1 定义和直觉</p>
<p>The concepts of eigenvalues and eigenvectors reveal the most
fundamental behavior of linear transformations. They identify the
special directions in which a transformation acts by simple stretching
or compressing, without rotation or distortion.
特征值和特征向量的概念揭示了线性变换最基本的行为。它们通过简单的拉伸或压缩（不进行旋转或变形）来识别变换所作用的特定方向。</p>
<h3 id="definition-4">Definition</h3>
<p>定义</p>
<p>Let <span class="math inline">\(T: V \to V\)</span> be a linear
transformation on a vector space <span class="math inline">\(V\)</span>.
A nonzero vector <span class="math inline">\(\mathbf{v} \in V\)</span>
is called an eigenvector of <span class="math inline">\(T\)</span> if 令
<span class="math inline">\(T: V \to V\)</span> 为向量空间 <span
class="math inline">\(V\)</span> 上的线性变换。非零向量 <span
class="math inline">\(\mathbf{v} \in V\)</span> 称为 <span
class="math inline">\(T\)</span> 的特征向量，若</p>
<p><span class="math display">\[
T(\mathbf{v}) = \lambda \mathbf{v}
\]</span></p>
<p>for some scalar <span class="math inline">\(\lambda \in
\mathbb{R}\)</span> (or <span
class="math inline">\(\mathbb{C}\)</span>). The scalar <span
class="math inline">\(\lambda\)</span> is the eigenvalue corresponding
to <span class="math inline">\(\mathbf{v}\)</span>. 某个标量 <span
class="math inline">\(\lambda \in \mathbb{R}\)</span> （或 <span
class="math inline">\(\mathbb{C}\)</span> ）。标量 <span
class="math inline">\(\lambda\)</span> 是对应于 <span
class="math inline">\(\mathbf{v}\)</span> 的特征值。</p>
<p>Equivalently, if <span class="math inline">\(A\)</span> is the matrix
of <span class="math inline">\(T\)</span>, then eigenvalues and
eigenvectors satisfy 等价地，如果 <span class="math inline">\(A\)</span>
是 <span class="math inline">\(T\)</span>
的矩阵，则特征值和特征向量满足</p>
<p><span class="math display">\[
A\mathbf{v} = \lambda \mathbf{v}.
\]</span></p>
<h3 id="basic-examples">Basic Examples</h3>
<p>基本示例</p>
<p>Example 8.1.1. Let 例 8.1.1. 设</p>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 0 \\ 0 &amp; 3 \end{bmatrix}.
\]</span></p>
<p>Then 然后</p>
<p><span class="math display">\[
A(1,0)^T = 2(1,0)^T, \quad A(0,1)^T = 3(0,1)^T.
\]</span></p>
<p>So <span class="math inline">\((1,0)\)</span> is an eigenvector with
eigenvalue <span class="math inline">\(2\)</span>, and <span
class="math inline">\((0,1) is an eigenvector with eigenvalue
\\3\)</span>. 因此 <span class="math inline">\((1,0)\)</span> 是特征值为
$2 的特征向量， $, and $ (0,1) 是特征值为 \ 3$ 的特征向量 。</p>
<p>Example 8.1.2. Rotation matrix in <span
class="math inline">\(\mathbb{R}^2\)</span>: 例 8.1.2。 <span
class="math inline">\(\mathbb{R}^2\)</span> 中的旋转矩阵：</p>
<p><span class="math display">\[
R_\theta = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta
&amp; \cos\theta \end{bmatrix}.
\]</span></p>
<p>If <span class="math inline">\(\theta \neq 0, \pi\)</span>, <span
class="math inline">\(R_\theta\)</span> has no real eigenvalues: every
vector is rotated, not scaled. Over <span
class="math inline">\(\mathbb{C}\)</span>, however, it has eigenvalues
<span class="math inline">\(e^{i\theta}, e^{-i\theta}\)</span>. 如果
<span class="math inline">\(\theta \neq 0, \pi\)</span> ，𝑅 𝜃 R θ ​
没有实数特征值：每个向量都经过旋转，而不是缩放。然而，在 <span
class="math inline">\(\mathbb{C}\)</span> 上，它的特征值为 <span
class="math inline">\(e^{i\theta}, e^{-i\theta}\)</span> 。</p>
<h3 id="algebraic-formulation">Algebraic Formulation</h3>
<p>代数公式</p>
<p>Eigenvalues arise from solving the characteristic equation:
特征值由求解特征方程得出：</p>
<p><span class="math display">\[
\det(A - \lambda I) = 0.
\]</span></p>
<p>This polynomial in <span class="math inline">\(\lambda\)</span> is
the characteristic polynomial. Its roots are the eigenvalues. <span
class="math inline">\(\lambda\)</span>
中的这个多项式是特征多项式。它的根就是特征值。</p>
<h3 id="geometric-intuition">Geometric Intuition</h3>
<p>几何直觉</p>
<ul>
<li>Eigenvectors are directions that remain unchanged in orientation
under a transformation; only their length is scaled.
特征向量是在变换下方向保持不变的方向；只有它们的长度被缩放。</li>
<li>Eigenvalues tell us the scaling factor along those directions.
特征值告诉我们沿这些方向的缩放因子。</li>
<li>If a matrix has many independent eigenvectors, it can often be
simplified (diagonalized) by changing basis.
如果矩阵具有许多独立的特征向量，则通常可以通过改变基来简化（对角化）。</li>
</ul>
<h3 id="applications-in-geometry-and-science">Applications in Geometry
and Science</h3>
<p>几何和科学中的应用</p>
<ul>
<li>Stretching along principal axes of an ellipse (quadratic forms).
沿椭圆的主轴拉伸（二次型）。</li>
<li>Stable directions of dynamical systems. 动力系统的稳定方向。</li>
<li>Principal components in statistics and machine learning.
统计学和机器学习中的主要成分。</li>
<li>Quantum mechanics, where observables correspond to operators with
eigenvalues. 量子力学，其中可观测量对应于具有特征值的算子。</li>
</ul>
<h3 id="why-this-matters-27">Why this matters</h3>
<p>为什么这很重要</p>
<p>Eigenvalues and eigenvectors are a bridge between algebra and
geometry. They provide a lens for understanding linear transformations
in their simplest form. Nearly every application of linear
algebra-differential equations, statistics, physics, computer
science-relies on eigen-analysis.
特征值和特征向量是代数和几何之间的桥梁。它们为理解最简形式的线性变换提供了一个视角。几乎所有线性代数的应用——微分方程、统计学、物理学、计算机科学——都依赖于特征分析。</p>
<h3 id="exercises-8.1">Exercises 8.1</h3>
<p>练习 8.1</p>
<ol type="1">
<li>Find the eigenvalues and eigenvectors of <span
class="math inline">\(\begin{bmatrix} 4 &amp; 0 \\ 0 &amp; -1
\end{bmatrix}\)</span>. 找到特征值和特征向量 <span
class="math inline">\(\begin{bmatrix} 4 &amp; 0 \\ 0 &amp; -1
\end{bmatrix}\)</span> .</li>
<li>Show that every scalar multiple of an eigenvector is again an
eigenvector for the same eigenvalue.
证明特征向量的每个标量倍数又是同一特征值的特征向量。</li>
<li>Verify that the rotation matrix <span
class="math inline">\(R_\theta\)</span> has no real eigenvalues unless
<span class="math inline">\(\theta = 0\)</span> or <span
class="math inline">\(\pi\)</span>. 验证旋转矩阵𝑅 𝜃 R θ ​ 除非 <span
class="math inline">\(\theta = 0\)</span> 或 <span
class="math inline">\(\pi\)</span> ，否则没有实数特征值。</li>
<li>Compute the characteristic polynomial of <span
class="math inline">\(\begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 1
\end{bmatrix}\)</span>. 计算特征多项式 <span
class="math inline">\(\begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 1
\end{bmatrix}\)</span> .</li>
<li>Explain geometrically what eigenvectors and eigenvalues represent
for the shear matrix <span class="math inline">\(\begin{bmatrix} 1 &amp;
1 \\ 0 &amp; 1 \end{bmatrix}\)</span>.
从几何角度解释特征向量和特征值对于剪切矩阵的意义 <span
class="math inline">\(\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1
\end{bmatrix}\)</span> .</li>
</ol>
<h2 id="diagonalization">8.2 Diagonalization</h2>
<p>8.2 对角化</p>
<p>A central goal in linear algebra is to simplify the action of a
matrix by choosing a good basis. Diagonalization is the process of
rewriting a matrix so that it acts by simple scaling along independent
directions. This makes computations such as powers, exponentials, and
solving differential equations far easier.
线性代数的核心目标是通过选择合适的基来简化矩阵的运算。对角化是将矩阵重写，使其能够沿独立方向进行简单的缩放。这使得幂、指数和微分方程等计算变得更加容易。</p>
<h3 id="definition-5">Definition</h3>
<p>定义</p>
<p>A square matrix <span class="math inline">\(A \in \mathbb{R}^{n
\times n}\)</span> is diagonalizable if there exists an invertible
matrix <span class="math inline">\(P\)</span> such that 如果存在可逆矩阵
<span class="math inline">\(P\)</span> 并且满足以下条件，则方阵 <span
class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>
可对角化</p>
<p><span class="math display">\[
P^{-1} A P = D,
\]</span></p>
<p>where <span class="math inline">\(D\)</span> is a diagonal matrix.
其中 <span class="math inline">\(D\)</span> 是一个对角矩阵。</p>
<p>The diagonal entries of <span class="math inline">\(D\)</span> are
eigenvalues of <span class="math inline">\(A\)</span>, and the columns
of <span class="math inline">\(P\)</span> are the corresponding
eigenvectors. <span class="math inline">\(D\)</span> 的对角线项是 <span
class="math inline">\(A\)</span> 的特征值， <span
class="math inline">\(P\)</span> 的列是相应的特征向量。</p>
<h3 id="when-is-a-matrix-diagonalizable">When is a Matrix
Diagonalizable?</h3>
<p>矩阵何时可对角化？</p>
<ul>
<li>A matrix is diagonalizable if it has <span
class="math inline">\(n\)</span> linearly independent eigenvectors.
如果矩阵具有 <span class="math inline">\(n\)</span>
个线性无关的特征向量，则该矩阵可对角化。</li>
<li>Equivalently, the sum of the dimensions of its eigenspaces equals
<span class="math inline">\(n\)</span>. 等效地，其特征空间的维数之和等于
<span class="math inline">\(n\)</span> 。</li>
<li>Symmetric matrices (over <span
class="math inline">\(\mathbb{R}\)</span>) are always diagonalizable,
with an orthonormal basis of eigenvectors. 对称矩阵（在 <span
class="math inline">\(\mathbb{R}\)</span>
上）始终可对角化，且具有特征向量的正交基。</li>
</ul>
<h3 id="example-8.2.1">Example 8.2.1</h3>
<p>例 8.2.1</p>
<p>Let 让</p>
<p><span class="math display">\[
A = \begin{bmatrix} 4 &amp; 1 \\ 0 &amp; 2 \end{bmatrix}.
\]</span></p>
<ol type="1">
<li>Characteristic polynomial: 特征多项式：</li>
</ol>
<p><span class="math display">\[
\det(A - \lambda I) = (4-\lambda)(2-\lambda).
\]</span></p>
<p>So eigenvalues are <span class="math inline">\(\lambda_1 =
4\)</span>, <span class="math inline">\(\lambda_2 = 2\)</span>.
所以特征值是 <span class="math inline">\(\lambda_1 = 4\)</span> ， <span
class="math inline">\(\lambda_2 = 2\)</span> 。</p>
<ol start="2" type="1">
<li>Eigenvectors: 特征向量：</li>
</ol>
<ul>
<li>For <span class="math inline">\(\lambda = 4\)</span>, solve <span
class="math inline">\((A-4I)\mathbf{v}=0\)</span>: <span
class="math inline">\(\begin{bmatrix} 0 &amp; 1 \\ 0 &amp; -2
\end{bmatrix}\mathbf{v} = 0\)</span>, giving <span
class="math inline">\(\mathbf{v}_1 = (1,0)\)</span>. 对于 <span
class="math inline">\(\lambda = 4\)</span> ，求解 <span
class="math inline">\((A-4I)\mathbf{v}=0\)</span> ： <span
class="math inline">\(\begin{bmatrix} 0 &amp; 1 \\ 0 &amp; -2
\end{bmatrix}\mathbf{v} = 0\)</span> ，得到 <span
class="math inline">\(\mathbf{v}_1 = (1,0)\)</span> 。</li>
<li>For <span class="math inline">\(\lambda = 2\)</span>: <span
class="math inline">\((A-2I)\mathbf{v}=0\)</span>, giving <span
class="math inline">\(\mathbf{v}_2 = (1,-2)\)</span>. 对于 <span
class="math inline">\(\lambda = 2\)</span> ： <span
class="math inline">\((A-2I)\mathbf{v}=0\)</span> ，给出 <span
class="math inline">\(\mathbf{v}_2 = (1,-2)\)</span> 。</li>
</ul>
<ol start="3" type="1">
<li>Construct <span class="math inline">\(P = \begin{bmatrix} 1 &amp; 1
\\ 0 &amp; -2 \end{bmatrix}\)</span>. Then 构造 <span
class="math inline">\(P = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; -2
\end{bmatrix}\)</span> 。然后</li>
</ol>
<p><span class="math display">\[
P^{-1} A P = \begin{bmatrix} 4 &amp; 0 \\ 0 &amp; 2 \end{bmatrix}.
\]</span></p>
<p>Thus, <span class="math inline">\(A\)</span> is diagonalizable.
因此， <span class="math inline">\(A\)</span> 是可对角化的。</p>
<h3 id="why-diagonalize">Why Diagonalize?</h3>
<p>为什么要对角化？</p>
<ul>
<li><p>Computing powers: If <span class="math inline">\(A = P D
P^{-1}\)</span>, then 计算能力： 如果 <span class="math inline">\(A = P
D P^{-1}\)</span> ，则</p>
<p><span class="math display">\[
A^k = P D^k P^{-1}.
\]</span></p>
<p>Since <span class="math inline">\(D\)</span> is diagonal, <span
class="math inline">\(D^k\)</span> is easy to compute. 由于 <span
class="math inline">\(D\)</span> 是对角线，因此 <span
class="math inline">\(D^k\)</span> 很容易计算。</p></li>
<li><p>Matrix exponentials: <span class="math inline">\(e^A = P e^D
P^{-1}\)</span>, useful in solving differential equations. 矩阵指数：
<span class="math inline">\(e^A = P e^D P^{-1}\)</span>
，有助于解决微分方程。</p></li>
<li><p>Understanding geometry: Diagonalization reveals the directions
along which a transformation stretches or compresses space
independently.
理解几何：对角化揭示了变换独立拉伸或压缩空间的方向。</p></li>
</ul>
<h3 id="non-diagonalizable-example">Non-Diagonalizable Example</h3>
<p>不可对角化的例子</p>
<p>Not all matrices can be diagonalized. 并非所有矩阵都可以对角化。</p>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>has only one eigenvalue <span class="math inline">\(\lambda =
1\)</span>, with eigenspace dimension 1. Since <span
class="math inline">\(n=2\)</span> but we only have 1 independent
eigenvector, <span class="math inline">\(A\)</span> is not
diagonalizable. 只有一个特征值 <span class="math inline">\(\lambda =
1\)</span> ，特征空间维数为 1。由于 <span
class="math inline">\(n=2\)</span> 但我们只有 1 个独立特征向量，因此
<span class="math inline">\(A\)</span> 不可对角化。</p>
<h3 id="geometric-interpretation-16">Geometric Interpretation</h3>
<p>几何解释</p>
<p>Diagonalization means we have found a basis of eigenvectors. In this
basis, the matrix acts by simple scaling along each coordinate axis. It
transforms complicated motion into independent 1D motions.
对角化意味着我们找到了特征向量的基。在此基上，矩阵通过沿每个坐标轴进行简单的缩放来发挥作用。它将复杂的运动转化为独立的一维运动。</p>
<h3 id="why-this-matters-28">Why this matters</h3>
<p>为什么这很重要</p>
<p>Diagonalization is a cornerstone of linear algebra. It simplifies
computation, reveals structure, and is the starting point for the
spectral theorem, Jordan form, and many applications in physics,
engineering, and data science.
对角化是线性代数的基石。它简化了计算，揭示了结构，并且是谱定理、若尔当形式以及物理、工程和数据科学中许多应用的起点。</p>
<h3 id="exercises-8.2">Exercises 8.2</h3>
<p>练习 8.2</p>
<ol type="1">
<li><p>Diagonalize 对角化</p>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 0 \\ 0 &amp; 3 \end{bmatrix}.
\]</span></p></li>
<li><p>Determine whether 确定是否</p>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>is diagonalizable. Why or why not?
是可对角化的。为什么或为什么不？</p></li>
<li><p>Find <span class="math inline">\(A^5\)</span> for 查找 <span
class="math inline">\(A^5\)</span></p>
<p><span class="math display">\[
A = \begin{bmatrix} 4 &amp; 1 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<p>using diagonalization. 使用对角化。</p></li>
<li><p>Show that any <span class="math inline">\(n \times n\)</span>
matrix with <span class="math inline">\(n\)</span> distinct eigenvalues
is diagonalizable. 证明任何具有 <span class="math inline">\(n\)</span>
个不同特征值的 <span class="math inline">\(n \times n\)</span>
矩阵都是可对角化的。</p></li>
<li><p>Explain why real symmetric matrices are always diagonalizable.
解释为什么实对称矩阵总是可对角化的。</p></li>
</ol>
<h2 id="characteristic-polynomials">8.3 Characteristic Polynomials</h2>
<p>8.3 特征多项式</p>
<p>The key to finding eigenvalues is the characteristic polynomial of a
matrix. This polynomial encodes the values of <span
class="math inline">\(\lambda\)</span> for which the matrix <span
class="math inline">\(A - \lambda I\)</span> fails to be invertible.
寻找特征值的关键是矩阵的特征多项式。该多项式对值进行编码 矩阵 <span
class="math inline">\(A - \lambda I\)</span> 不可逆，其中 <span
class="math inline">\(\lambda\)</span> 。</p>
<h3 id="definition-6">Definition</h3>
<p>定义</p>
<p>For an <span class="math inline">\(n \times n\)</span> matrix <span
class="math inline">\(A\)</span>, the characteristic polynomial is 对于
<span class="math inline">\(n \times n\)</span> 矩阵 <span
class="math inline">\(A\)</span> ，特征多项式为</p>
<p><span class="math display">\[
p_A(\lambda) = \det(A - \lambda I).
\]</span></p>
<p>The roots of <span class="math inline">\(p_A(\lambda)\)</span> are
the eigenvalues of <span class="math inline">\(A\)</span>. <span
class="math inline">\(p_A(\lambda)\)</span> 的根是 <span
class="math inline">\(A\)</span> 的特征值。</p>
<h3 id="examples-7">Examples</h3>
<p>示例</p>
<p>Example 8.3.1. Let 例 8.3.1. 设</p>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}.
\]</span></p>
<p>Then 然后</p>
<p><span class="math display">\[
p_A(\lambda) = \det\!\begin{bmatrix} 2-\lambda &amp; 1 \\ 1 &amp;
2-\lambda \end{bmatrix}= (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
\]</span></p>
<p>Thus eigenvalues are <span class="math inline">\(\lambda = 1,
3\)</span>. 因此特征值为 <span class="math inline">\(\lambda = 1,
3\)</span> 。</p>
<p>Example 8.3.2. For 例 8.3.2. 对于</p>
<p><span class="math display">\[
A = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}
\]</span></p>
<p>(rotation by 90°), （旋转 90°），</p>
<p><span class="math display">\[
p_A(\lambda) = \det\!\begin{bmatrix} -\lambda &amp; -1 \\ 1 &amp;
-\lambda \end{bmatrix}= \lambda^2 + 1.
\]</span></p>
<p>Eigenvalues are <span class="math inline">\(\lambda = \pm i\)</span>.
No real eigenvalues exist, consistent with pure rotation. 特征值为 <span
class="math inline">\(\lambda = \pm i\)</span>
。不存在实数特征值，与纯旋转一致。</p>
<p>Example 8.3.3. For a triangular matrix 例 8.3.3. 对于三角矩阵</p>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 0 &amp; 3 &amp; 5 \\ 0 &amp; 0
&amp; 4 \end{bmatrix},
\]</span></p>
<p>the determinant is simply the product of diagonal entries minus <span
class="math inline">\(\lambda\)</span>: 行列式仅仅是对角线项的乘积减去
<span class="math inline">\(\lambda\)</span> ：</p>
<p><span class="math display">\[
p_A(\lambda) = (2-\lambda)(3-\lambda)(4-\lambda).
\]</span></p>
<p>So eigenvalues are 2,3,4. 所以特征值为 2,3,4 。</p>
<h3 id="properties-1">Properties</h3>
<p>特性</p>
<ol type="1">
<li><p>The characteristic polynomial of an <span class="math inline">\(n
\times n\)</span> matrix has degree <span
class="math inline">\(n\)</span>. <span class="math inline">\(n \times
n\)</span> 矩阵的特征多项式的度为 <span class="math inline">\(n\)</span>
。</p></li>
<li><p>The sum of the eigenvalues (counted with multiplicity) equals the
trace of <span class="math inline">\(A\)</span>:
特征值（按重数计算）的和等于 <span class="math inline">\(A\)</span>
的迹：</p>
<p><span class="math display">\[
\text{tr}(A) = \lambda_1 + \cdots + \lambda_n.
\]</span></p></li>
<li><p>The product of the eigenvalues equals the determinant of <span
class="math inline">\(A\)</span>: 特征值的乘积等于 <span
class="math inline">\(A\)</span> 的行列式：</p>
<p><span class="math display">\[
\det(A) = \lambda_1 \cdots \lambda_n.
\]</span></p></li>
<li><p>Similar matrices have the same characteristic polynomial, hence
the same eigenvalues.
相似的矩阵具有相同的特征多项式，因此具有相同的特征值。</p></li>
</ol>
<h3 id="geometric-interpretation-17">Geometric Interpretation</h3>
<p>几何解释</p>
<p>The characteristic polynomial captures when <span
class="math inline">\(A - \lambda I\)</span> collapses space: its
determinant is zero precisely when the transformation <span
class="math inline">\(A - \lambda I\)</span> is singular. Thus,
eigenvalues mark the critical scalings where the matrix loses
invertibility. 特征多项式捕捉了 <span class="math inline">\(A - \lambda
I\)</span> 何时使空间坍缩：当变换 <span class="math inline">\(A -
\lambda I\)</span>
为奇异时，其行列式恰好为零。因此，特征值标记了矩阵失去可逆性的临界尺度。</p>
<h3 id="why-this-matters-29">Why this matters</h3>
<p>为什么这很重要</p>
<p>Characteristic polynomials provide the computational tool to extract
eigenvalues. They connect matrix invariants (trace and determinant) with
geometry, and form the foundation for diagonalization, spectral
theorems, and stability analysis in dynamical systems.
特征多项式提供了提取特征值的计算工具。它们将矩阵不变量（迹和行列式）与几何联系起来，并构成了动力系统中对角化、谱定理和稳定性分析的基础。</p>
<h3 id="exercises-8.3">Exercises 8.3</h3>
<p>练习 8.3</p>
<ol type="1">
<li><p>Compute the characteristic polynomial of 计算特征多项式</p>
<p><span class="math display">\[
A = \begin{bmatrix} 4 &amp; 2 \\ 1 &amp; 3 \end{bmatrix}.
\]</span></p></li>
<li><p>Verify that the sum of the eigenvalues of <span
class="math inline">\(\begin{bmatrix} 5 &amp; 0 \\ 0 &amp; -2
\end{bmatrix}\)</span> equals its trace, and their product equals its
determinant. 验证特征值之和 <span class="math inline">\(\begin{bmatrix}
5 &amp; 0 \\ 0 &amp; -2 \end{bmatrix}\)</span>
等于它的迹，它们的乘积等于它的行列式。</p></li>
<li><p>Show that for any triangular matrix, the eigenvalues are just the
diagonal entries. 证明对于任何三角矩阵，特征值只是对角线项。</p></li>
<li><p>Prove that if <span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> are similar matrices, then <span
class="math inline">\(p_A(\lambda) = p_B(\lambda)\)</span>. 证明如果
<span class="math inline">\(A\)</span> 和 <span
class="math inline">\(B\)</span> 是相似矩阵，则 <span
class="math inline">\(p_A(\lambda) = p_B(\lambda)\)</span> 。</p></li>
<li><p>Compute the characteristic polynomial of <span
class="math inline">\(\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 0 &amp; 1
&amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)</span>. 计算特征多项式 [ 1
1 0 0 1 1 0 0 1 ] ​ 1 0 0 ​ 1 1 0 ​ 0 1 1 ​ ​ .</p></li>
</ol>
<h2 id="applications-differential-equations-markov-chains">8.4
Applications (Differential Equations, Markov Chains)</h2>
<p>8.4 应用（微分方程、马尔可夫链）</p>
<p>Eigenvalues and eigenvectors are not only central to the theory of
linear algebra-they are indispensable tools across mathematics and
applied science. Two classic applications are solving systems of
differential equations and analyzing Markov chains.
特征值和特征向量不仅是线性代数理论的核心，也是数学和应用科学领域中不可或缺的工具。两个经典的应用是求解微分方程组和分析马尔可夫链。</p>
<h3 id="linear-differential-equations">Linear Differential
Equations</h3>
<p>线性微分方程</p>
<p>Consider the system 考虑系统</p>
<p><span class="math display">\[
\frac{d\mathbf{x}}{dt} = A \mathbf{x},
\]</span></p>
<p>where <span class="math inline">\(A\)</span> is an <span
class="math inline">\(n \times n\)</span> matrix and <span
class="math inline">\(\mathbf{x}(t)\)</span> is a vector-valued
function. 其中 <span class="math inline">\(A\)</span> 是 <span
class="math inline">\(n \times n\)</span> 矩阵， <span
class="math inline">\(\mathbf{x}(t)\)</span> 是矢量值函数。</p>
<p>If <span class="math inline">\(\mathbf{v}\)</span> is an eigenvector
of <span class="math inline">\(A\)</span> with eigenvalue <span
class="math inline">\(\lambda\)</span>, then the function 如果 <span
class="math inline">\(\mathbf{v}\)</span> 是 <span
class="math inline">\(A\)</span> 的特征向量，其特征值为 <span
class="math inline">\(\lambda\)</span> ，则函数</p>
<p><span class="math display">\[
\mathbf{x}(t) = e^{\lambda t}\mathbf{v}
\]</span></p>
<p>is a solution. 是一个解决方案。</p>
<ul>
<li><p>Eigenvalues determine the growth or decay rate:
特征值决定增长率或衰减率：</p>
<ul>
<li>If <span class="math inline">\(\lambda &lt; 0\)</span>, solutions
decay (stable). 如果 <span class="math inline">\(\lambda &lt; 0\)</span>
，则解决方案衰减（稳定）。</li>
<li>If <span class="math inline">\(\lambda &gt; 0\)</span>, solutions
grow (unstable). 如果 <span class="math inline">\(\lambda &gt;
0\)</span> ，则解决方案会增长（不稳定）。</li>
<li>If <span class="math inline">\(\lambda\)</span> is complex,
oscillations occur. 如果 <span class="math inline">\(\lambda\)</span>
是复数，则会发生振荡。</li>
</ul></li>
</ul>
<p>By combining eigenvector solutions, we can solve general initial
conditions. 通过结合特征向量解，我们可以解决一般的初始条件。</p>
<p>Example 8.4.1. Let 例 8.4.1. 设</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 0 \\0 &amp; -1 \end{bmatrix}.
\]</span></p>
<p>Then eigenvalues are <span class="math inline">\(2, -1\)</span>with
eigenvectors<span class="math inline">\((1,0)\)</span>, <span
class="math inline">\((0,1)\)</span>. Solutions are 则特征值为 $2, -1
<span class="math inline">\(with eigenvectors\)</span> (1,0) $, $
(0,1)$。解为</p>
<p><span class="math display">\[
\mathbf{x}(t) = c_1 e^{2t}(1,0) + c_2 e^{-t}(0,1).
\]</span></p>
<p>Thus one component grows exponentially, the other decays.
因此，一个部分呈指数增长，另一个部分则衰减。</p>
<h3 id="markov-chains">Markov Chains</h3>
<p>马尔可夫链</p>
<p>A Markov chain is described by a stochastic matrix <span
class="math inline">\(P\)</span>, where each column sums to 1 and
entries are nonnegative. If <span
class="math inline">\(\mathbf{x}_k\)</span> represents the probability
distribution after <span class="math inline">\(k\)</span> steps, then
马尔可夫链可以用随机矩阵 <span class="math inline">\(P\)</span>
来描述，其中每列和为 1，且元素为非负值。如果 𝑥 𝑘 x k ​ 表示 <span
class="math inline">\(k\)</span> 步后的概率分布，则</p>
<p><span class="math display">\[
\mathbf{x}_{k+1} = P \mathbf{x}_k.
\]</span></p>
<p>Iterating gives 迭代得到</p>
<p><span class="math display">\[
\mathbf{x}_k = P^k \mathbf{x}_0.
\]</span></p>
<p>Understanding long-term behavior reduces to analyzing powers of <span
class="math inline">\(P\)</span>. 理解长期行为可以归结为分析 <span
class="math inline">\(P\)</span> 的力量。</p>
<ul>
<li>The eigenvalue <span class="math inline">\(\lambda = 1\)</span>
always exists. Its eigenvector gives the steady-state distribution.
特征值 <span class="math inline">\(\lambda = 1\)</span>
始终存在。其特征向量给出了稳态分布。</li>
<li>All other eigenvalues satisfy <span class="math inline">\(|\lambda|
\leq 1\)</span>. Their influence decays as <span class="math inline">\(k
\to \infty\)</span>. 所有其他特征值都满足 <span
class="math inline">\(|\lambda| \leq 1\)</span> 。它们的影响衰减为 <span
class="math inline">\(k \to \infty\)</span> 。</li>
</ul>
<p>Example 8.4.2. Consider 例 8.4.2. 考虑</p>
<p><span class="math display">\[
P = \begin{bmatrix}0.9 &amp; 0.5 \\0.1 &amp; 0.5 \end{bmatrix}.
\]</span></p>
<p>Eigenvalues are <span class="math inline">\(\lambda_1 = 1\)</span>,
<span class="math inline">\(\lambda_2 = 0.4\)</span>. The eigenvector
for <span class="math inline">\(\lambda = 1\)</span> is proportional to
<span class="math inline">\((5,1)\)</span>. Normalizing gives the steady
state 特征值为 <span class="math inline">\(\lambda_1 = 1\)</span> ,
<span class="math inline">\(\lambda_2 = 0.4\)</span> 。 <span
class="math inline">\(\lambda = 1\)</span> 的特征向量与 <span
class="math inline">\((5,1)\)</span> 成正比。归一化后可得到稳态</p>
<p><span class="math display">\[
\pi = \left(\tfrac{5}{6}, \tfrac{1}{6}\right).
\]</span></p>
<p>Thus, regardless of the starting distribution, the chain converges to
<span class="math inline">\(\pi\)</span>.
因此，无论起始分布如何，链都会收敛到 <span
class="math inline">\(\pi\)</span> 。</p>
<h3 id="geometric-interpretation-18">Geometric Interpretation</h3>
<p>几何解释</p>
<ul>
<li>In differential equations, eigenvalues determine the time evolution:
exponential growth, decay, or oscillation.
在微分方程中，特征值决定时间的演变：指数增长、衰减或振荡。</li>
<li>In Markov chains, eigenvalues determine the long-term equilibrium of
stochastic processes.
在马尔可夫链中，特征值决定了随机过程的长期均衡。</li>
</ul>
<h3 id="why-this-matters-30">Why this matters</h3>
<p>为什么这很重要</p>
<p>Eigenvalue methods turn complex iterative or dynamical systems into
tractable problems. In physics, engineering, and finance, they describe
stability and resonance. In computer science and statistics, they power
algorithms from Google’s PageRank to modern machine learning.
特征值方法将复杂的迭代或动态系统转化为易于处理的问题。在物理学、工程学和金融学领域，它们描述稳定性和共振。在计算机科学和统计学领域，它们为从谷歌的
PageRank 到现代机器学习等各种算法提供支持。</p>
<h3 id="exercises-8.4">Exercises 8.4</h3>
<p>练习 8.4</p>
<ol type="1">
<li><p>Solve <span class="math inline">\(\tfrac{d}{dt}\mathbf{x} =
\begin{bmatrix} 3 &amp; 0 \\ 0 &amp; -2
\end{bmatrix}\mathbf{x}\)</span>. 解出 <span
class="math inline">\(\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 &amp;
0 \\ 0 &amp; -2 \end{bmatrix}\mathbf{x}\)</span> 。</p></li>
<li><p>Show that if <span class="math inline">\(A\)</span> has a complex
eigenvalue <span class="math inline">\(\alpha \pm i\beta\)</span>, then
solutions of <span class="math inline">\(\tfrac{d}{dt}\mathbf{x} =
A\mathbf{x}\)</span> involve oscillations of frequency <span
class="math inline">\(\beta\)</span>. 证明如果 <span
class="math inline">\(A\)</span> 具有复特征值 <span
class="math inline">\(\alpha \pm i\beta\)</span> ，则 <span
class="math inline">\(\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}\)</span>
的解涉及频率 <span class="math inline">\(\beta\)</span>
的振荡。</p></li>
<li><p>Find the steady-state distribution of 找到稳态分布</p>
<p><span class="math display">\[
P = \begin{bmatrix} 0.7 &amp; 0.2 \\ 0.3 &amp; 0.8 \end{bmatrix}.
\]</span></p></li>
<li><p>Prove that for any stochastic matrix <span
class="math inline">\(P\)</span>, 1 is always an eigenvalue.
证明对于任何随机矩阵 <span class="math inline">\(P\)</span> ， 1
始终是特征值。</p></li>
<li><p>Explain why all eigenvalues of a stochastic matrix satisfy <span
class="math inline">\(|\lambda| \leq 1\)</span>.
解释为什么随机矩阵的所有特征值都满足 <span
class="math inline">\(|\lambda| \leq 1\)</span> 。</p></li>
</ol>
<h1 id="chapter-9.-quadratic-forms-and-spectral-theorems">Chapter 9.
Quadratic Forms and Spectral Theorems</h1>
<p>第九章二次型和谱定理</p>
<h2 id="quadratic-forms">9.1 Quadratic Forms</h2>
<p>9.1 二次型</p>
<p>A quadratic form is a polynomial of degree two in several variables,
expressed neatly using matrices. Quadratic forms appear throughout
mathematics: in optimization, geometry of conic sections, statistics
(variance), and physics (energy functions).
二次型是多元二次多项式，可以用矩阵简洁地表示。二次型在数学中随处可见：优化、圆锥曲线几何、统计学（方差）和物理学（能量函数）。</p>
<h3 id="definition-7">Definition</h3>
<p>定义</p>
<p>Let <span class="math inline">\(A\)</span> be an <span
class="math inline">\(n \times n\)</span> symmetric matrix and <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>. The
quadratic form associated with <span class="math inline">\(A\)</span> is
令 <span class="math inline">\(A\)</span> 为 <span
class="math inline">\(n \times n\)</span> 对称矩阵， <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> 。与 <span
class="math inline">\(A\)</span> 相关的二次式为</p>
<p><span class="math display">\[
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}.
\]</span></p>
<p>Expanded, 扩展，</p>
<p><span class="math display">\[
Q(\mathbf{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j.
\]</span></p>
<p>Because <span class="math inline">\(A\)</span> is symmetric (<span
class="math inline">\(a_{ij} = a_{ji}\)</span>), the cross-terms can be
grouped naturally. 因为 <span class="math inline">\(A\)</span> 是对称的
(𝑎 𝑖 𝑗 = 𝑎 𝑗 𝑖 a 伊奇 ​ =a 姬 ​ )，交叉项可以自然分组。</p>
<h3 id="examples-8">Examples</h3>
<p>示例</p>
<p>Example 9.1.1. For 例 9.1.1. 对于</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 1 \\1 &amp; 3 \end{bmatrix}, \quad \mathbf{x}
= \begin{bmatrix}x \\y \end{bmatrix},
\]</span></p>
<p><span class="math display">\[
Q(x,y) = \begin{bmatrix} x &amp; y \end{bmatrix}\begin{bmatrix}2 &amp; 1
\\1 &amp; 3 \end{bmatrix}\begin{bmatrix}x \\y \end{bmatrix}= 2x^2 + 2xy
+ 3y^2.
\]</span></p>
<p>Example 9.1.2. The quadratic form 例 9.1.2. 二次型</p>
<p><span class="math display">\[
Q(x,y) = x^2 + y^2
\]</span></p>
<p>corresponds to the matrix <span class="math inline">\(A =
I_2\)</span>. It measures squared Euclidean distance from the origin.
对应于矩阵𝐴 = 𝐼 2 A=I 2 ​ . 它测量距离原点的平方欧几里得距离。</p>
<p>Example 9.1.3. The conic section equation 例 9.1.3 圆锥曲线方程</p>
<p><span class="math display">\[
4x^2 + 2xy + 5y^2 = 1
\]</span></p>
<p>is described by the quadratic form <span
class="math inline">\(\mathbf{x}^T A \mathbf{x} = 1\)</span> with
由二次型 <span class="math inline">\(\mathbf{x}^T A \mathbf{x} =
1\)</span> 描述</p>
<p><span class="math display">\[
A = \begin{bmatrix}4 &amp; 1 \\1 &amp; 5\end{bmatrix}.
\]</span></p>
<h3 id="diagonalization-of-quadratic-forms">Diagonalization of Quadratic
Forms</h3>
<p>二次型的对角化</p>
<p>By choosing a new basis consisting of eigenvectors of <span
class="math inline">\(A\)</span>, we can rewrite the quadratic form
without cross terms. If <span class="math inline">\(A =
PDP^{-1}\)</span> with <span class="math inline">\(D\)</span> diagonal,
then 通过选择由 <span class="math inline">\(A\)</span>
的特征向量组成的新基，我们可以重写没有交叉项的二次型。如果 <span
class="math inline">\(A = PDP^{-1}\)</span> 以 <span
class="math inline">\(D\)</span> 为对角线，则</p>
<p><span class="math display">\[
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} = (P^{-1}\mathbf{x})^T D
(P^{-1}\mathbf{x}).
\]</span></p>
<p>Thus quadratic forms can always be expressed as a sum of weighted
squares: 因此二次型总是可以表示为加权平方和：</p>
<p><span class="math display">\[
Q(\mathbf{y}) = \lambda_1 y_1^2 + \cdots + \lambda_n y_n^2,
\]</span></p>
<p>where <span class="math inline">\(\lambda_i\)</span> are the
eigenvalues of <span class="math inline">\(A\)</span>. 其中𝜆 𝑖 λ i ​ 是
<span class="math inline">\(A\)</span> 的特征值。</p>
<h3 id="geometric-interpretation-19">Geometric Interpretation</h3>
<p>几何解释</p>
<p>Quadratic forms describe geometric shapes: 二次型描述几何形状：</p>
<ul>
<li>In 2D: ellipses, parabolas, hyperbolas.
二维：椭圆、抛物线、双曲线。</li>
<li>In 3D: ellipsoids, paraboloids, hyperboloids. 在 3D
中：椭圆体、抛物面、双曲面。</li>
<li>In higher dimensions: generalizations of ellipsoids.
在更高维度中：椭圆体的概括。</li>
</ul>
<p>Diagonalization aligns the coordinate axes with the principal axes of
the shape. 对角化将坐标轴与形状的主轴对齐。</p>
<h3 id="why-this-matters-31">Why this matters</h3>
<p>为什么这很重要</p>
<p>Quadratic forms unify geometry and algebra. They are central in
optimization (minimizing energy functions), statistics ( covariance
matrices and variance), mechanics (kinetic energy), and numerical
analysis. Understanding quadratic forms leads directly to the spectral
theorem.
二次型统一了几何和代数。它们在优化（最小化能量函数）、统计学（协方差矩阵和方差）、力学（动能）和数值分析中都至关重要。理解二次型可以直接引出谱定理。</p>
<h3 id="exercises-9.1">Exercises 9.1</h3>
<p>练习 9.1</p>
<ol type="1">
<li>Write the quadratic form <span class="math inline">\(Q(x,y) = 3x^2 +
4xy + y^2\)</span> as <span class="math inline">\(\mathbf{x}^T A
\mathbf{x}\)</span> for some symmetric matrix <span
class="math inline">\(A\)</span>. 对于某些对称矩阵 <span
class="math inline">\(A\)</span> ，将二次型 <span
class="math inline">\(Q(x,y) = 3x^2 + 4xy + y^2\)</span> 写为 <span
class="math inline">\(\mathbf{x}^T A \mathbf{x}\)</span> 。</li>
<li>For <span class="math inline">\(A = \begin{bmatrix} 1 &amp; 2 \\ 2
&amp; 1 \end{bmatrix}\)</span>, compute <span
class="math inline">\(Q(x,y)\)</span> explicitly. 对于 <span
class="math inline">\(A = \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 1
\end{bmatrix}\)</span> ，明确计算 <span
class="math inline">\(Q(x,y)\)</span> 。</li>
<li>Diagonalize the quadratic form <span class="math inline">\(Q(x,y) =
2x^2 + 2xy + 3y^2\)</span>. 将二次型 <span class="math inline">\(Q(x,y)
= 2x^2 + 2xy + 3y^2\)</span> 对角化。</li>
<li>Identify the conic section given by <span
class="math inline">\(Q(x,y) = x^2 - y^2\)</span>. 确定由 <span
class="math inline">\(Q(x,y) = x^2 - y^2\)</span> 给出的圆锥截面。</li>
<li>Show that if <span class="math inline">\(A\)</span> is symmetric,
quadratic forms defined by <span class="math inline">\(A\)</span> and
<span class="math inline">\(A^T\)</span> are identical. 证明如果 <span
class="math inline">\(A\)</span> 是对称的，则由 <span
class="math inline">\(A\)</span> 和 <span
class="math inline">\(A^T\)</span> 定义的二次型是相同的。</li>
</ol>
<h2 id="positive-definite-matrices">9.2 Positive Definite Matrices</h2>
<p>9.2 正定矩阵</p>
<p>Quadratic forms are especially important when their associated
matrices are positive definite, since these guarantee positivity of
energy, distance, or variance. Positive definiteness is a cornerstone in
optimization, numerical analysis, and statistics.
当二次型的相关矩阵为正定矩阵时，它们尤为重要，因为它们可以保证能量、距离或方差的正性。正定性是优化、数值分析和统计学的基石。</p>
<h3 id="definition-8">Definition</h3>
<p>定义</p>
<p>A symmetric matrix <span class="math inline">\(A \in \mathbb{R}^{n
\times n}\)</span> is called: 对称矩阵 <span class="math inline">\(A \in
\mathbb{R}^{n \times n}\)</span> 称为：</p>
<ul>
<li><p>Positive definite if 正定如果</p>
<p><span class="math display">\[
\mathbf{x}^T A \mathbf{x} &gt; 0 \quad \text{for all nonzero }
\mathbf{x} \in \mathbb{R}^n.
\]</span></p></li>
<li><p>Positive semidefinite if 正半定的，如果</p>
<p><span class="math display">\[
\mathbf{x}^T A \mathbf{x} \geq 0 \quad \text{for all } \mathbf{x}.
\]</span></p></li>
</ul>
<p>Similarly, negative definite (always &lt; 0) and indefinite (can be
both &lt; 0 and &gt; 0) matrices are defined. 类似地，定义了负定（始终
&lt; 0）和不定（可以同时 &lt; 0 和 &gt; 0）矩阵。</p>
<h3 id="examples-9">Examples</h3>
<p>示例</p>
<p>Example 9.2.1. 例 9.2.1。</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 0 \\0 &amp; 3 \end{bmatrix}
\]</span></p>
<p>is positive definite, since 是正定的，因为</p>
<p><span class="math display">\[
Q(x,y) = 2x^2 + 3y^2 &gt; 0
\]</span></p>
<p>for all <span class="math inline">\((x,y) \neq (0,0)\)</span>.
对于所有 <span class="math inline">\((x,y) \neq (0,0)\)</span> 。</p>
<p>Example 9.2.2. 例 9.2.2。</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\2 &amp; 1 \end{bmatrix}
\]</span></p>
<p>has quadratic form 具有二次形式</p>
<p><span class="math display">\[
Q(x,y) = x^2 + 4xy + y^2.
\]</span></p>
<p>This matrix is not positive definite, since <span
class="math inline">\(Q(1,-1) = -2 &lt; 0\)</span>.
该矩阵不是正定的，因为 <span class="math inline">\(Q(1,-1) = -2 &lt;
0\)</span> 。</p>
<h3 id="characterizations">Characterizations</h3>
<p>特征</p>
<p>For a symmetric matrix <span class="math inline">\(A\)</span>:
对于对称矩阵 <span class="math inline">\(A\)</span> ：</p>
<ol type="1">
<li><p>Eigenvalue test: <span class="math inline">\(A\)</span> is
positive definite if and only if all eigenvalues of <span
class="math inline">\(A\)</span> are positive. 特征值检验：当且仅当
<span class="math inline">\(A\)</span> 的所有特征值都为正时， <span
class="math inline">\(A\)</span> 才是正定的。</p></li>
<li><p>Principal minors test (Sylvester’s criterion): <span
class="math inline">\(A\)</span> is positive definite if and only if all
leading principal minors ( determinants of top-left <span
class="math inline">\(k \times k\)</span> submatrices) are positive.
主子式检验（西尔维斯特标准）：当且仅当所有首项主子式（左上角 <span
class="math inline">\(k \times k\)</span> 子矩阵的行列式）均为正时，
<span class="math inline">\(A\)</span> 才是正定的。</p></li>
<li><p>Cholesky factorization: <span class="math inline">\(A\)</span> is
positive definite if and only if it can be written as Cholesky 分解：
<span class="math inline">\(A\)</span> 为正定当且仅当它可以写成</p>
<p><span class="math display">\[
A = R^T R,
\]</span></p>
<p>where <span class="math inline">\(R\)</span> is an upper triangular
matrix with positive diagonal entries. 其中 <span
class="math inline">\(R\)</span> 是具有正对角线项的上三角矩阵。</p></li>
</ol>
<h3 id="geometric-interpretation-20">Geometric Interpretation</h3>
<p>几何解释</p>
<ul>
<li>Positive definite matrices correspond to quadratic forms that define
ellipsoids centered at the origin.
正定矩阵对应于定义以原点为中心的椭圆体的二次型。</li>
<li>Positive semidefinite matrices define flattened ellipsoids (possibly
degenerate). 正半定矩阵定义扁平的椭球体（可能是退化的）。</li>
<li>Indefinite matrices define hyperbolas or saddle-shaped surfaces.
不定矩阵定义双曲线或马鞍形曲面。</li>
</ul>
<h3 id="applications">Applications</h3>
<p>应用</p>
<ul>
<li>Optimization: Hessians of convex functions are positive
semidefinite; strict convexity corresponds to positive definite
Hessians. 优化：凸函数的 Hessian 矩阵是正半定的；严格凸性对应于正定的
Hessian 矩阵。</li>
<li>Statistics: Covariance matrices are positive semidefinite.
统计：协方差矩阵是正半定的。</li>
<li>Numerical methods: Cholesky decomposition is widely used to solve
systems with positive definite matrices efficiently. 数值方法：Cholesky
分解被广泛用于有效地解决具有正定矩阵的系统。</li>
</ul>
<h3 id="why-this-matters-32">Why this matters</h3>
<p>为什么这很重要</p>
<p>Positive definiteness provides stability and guarantees in
mathematics and computation. It ensures energy functions are bounded
below, optimization problems have unique solutions, and statistical
models are meaningful.
正定性在数学和计算中提供了稳定性和保证。它确保能量函数有界，优化问题有唯一解，统计模型有意义。</p>
<h3 id="exercises-9.2">Exercises 9.2</h3>
<p>练习 9.2</p>
<ol type="1">
<li><p>Use Sylvester’s criterion to check whether 使用 Sylvester
标准检查</p>
<p><span class="math display">\[
A = \begin{bmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{bmatrix}
\]</span></p>
<p>is positive definite. 是正定的。</p></li>
<li><p>Determine whether 确定是否</p>
<p><span class="math display">\[
A = \begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}
\]</span></p>
<p>is positive definite, semidefinite, or indefinite.
是正定的、半定的或不定的。</p></li>
<li><p>Find the eigenvalues of 找到特征值</p>
<p><span class="math display">\[
A = \begin{bmatrix} 4 &amp; 2 \\ 2 &amp; 3 \end{bmatrix},
\]</span></p>
<p>and use them to classify definiteness.
并用它们来对确定性进行分类。</p></li>
<li><p>Prove that all diagonal matrices with positive entries are
positive definite. 证明所有具有正项的对角矩阵都是正定的。</p></li>
<li><p>Show that if <span class="math inline">\(A\)</span> is positive
definite, then so is <span class="math inline">\(P^T A P\)</span> for
any invertible matrix <span class="math inline">\(P\)</span>. 证明如果
<span class="math inline">\(A\)</span> 为正定矩阵，则对于任何可逆矩阵
<span class="math inline">\(P\)</span> ， <span
class="math inline">\(P^T A P\)</span> 也为正定矩阵。</p></li>
</ol>
<h2 id="spectral-theorem">9.3 Spectral Theorem</h2>
<p>9.3 谱定理</p>
<p>The spectral theorem is one of the most powerful results in linear
algebra. It states that symmetric matrices can always be diagonalized by
an orthogonal basis of eigenvectors. This links algebra (eigenvalues),
geometry (orthogonal directions), and applications (stability,
optimization, statistics).
谱定理是线性代数中最有力的结论之一。它指出对称矩阵总是可以通过特征向量的正交基对角化。这连接了代数（特征值）、几何（正交方向）和应用（稳定性、优化、统计）。</p>
<h3 id="statement-of-the-spectral-theorem">Statement of the Spectral
Theorem</h3>
<p>谱定理表述</p>
<p>If <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>
is symmetric (<span class="math inline">\(A^T = A\)</span>), then: 如果
<span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>
是对称的（ <span class="math inline">\(A^T = A\)</span> ），则：</p>
<ol type="1">
<li><p>All eigenvalues of <span class="math inline">\(A\)</span> are
real. <span class="math inline">\(A\)</span>
的所有特征值都是实数。</p></li>
<li><p>There exists an orthonormal basis of <span
class="math inline">\(\mathbb{R}^n\)</span> consisting of eigenvectors
of <span class="math inline">\(A\)</span>. 存在由 <span
class="math inline">\(A\)</span> 的特征向量组成的 <span
class="math inline">\(\mathbb{R}^n\)</span> 正交基。</p></li>
<li><p>Thus, <span class="math inline">\(A\)</span> can be written as
因此， <span class="math inline">\(A\)</span> 可以写成</p>
<p><span class="math display">\[
A = Q \Lambda Q^T,
\]</span></p>
<p>where <span class="math inline">\(Q\)</span> is an orthogonal matrix
(<span class="math inline">\(Q^T Q = I\)</span>) and <span
class="math inline">\(\Lambda\)</span> is diagonal with eigenvalues of
<span class="math inline">\(A\)</span> on the diagonal. 其中 <span
class="math inline">\(Q\)</span> 是正交矩阵 ( <span
class="math inline">\(Q^T Q = I\)</span> )， <span
class="math inline">\(\Lambda\)</span> 是对角矩阵，其特征值 <span
class="math inline">\(A\)</span> 位于对角线上。</p></li>
</ol>
<h3 id="consequences">Consequences</h3>
<p>结果</p>
<ul>
<li>Symmetric matrices are always diagonalizable, and the
diagonalization is numerically stable.
对称矩阵总是可对角化的，并且对角化在数值上是稳定的。</li>
<li>Quadratic forms <span class="math inline">\(\mathbf{x}^T A
\mathbf{x}\)</span> can be expressed in terms of eigenvalues and
eigenvectors, showing ellipsoids aligned with eigen-directions. 二次型
<span class="math inline">\(\mathbf{x}^T A \mathbf{x}\)</span>
可以用特征值和特征向量来表示，显示与特征方向对齐的椭圆体。</li>
<li>Positive definiteness can be checked by confirming that all
eigenvalues are positive.
可以通过确认所有特征值都为正来检查正定性。</li>
</ul>
<h3 id="example-9.3.1">Example 9.3.1</h3>
<p>例 9.3.1</p>
<p>Let 让</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 1 \\1 &amp; 2 \end{bmatrix}.
\]</span></p>
<ol type="1">
<li>Characteristic polynomial: 特征多项式：</li>
</ol>
<p><span class="math display">\[
p(\lambda) = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
\]</span></p>
<p>Eigenvalues: <span class="math inline">\(\lambda_1 = 1, \ \lambda_2 =
3\)</span>. 特征值： <span class="math inline">\(\lambda_1 = 1, \
\lambda_2 = 3\)</span> 。</p>
<ol start="2" type="1">
<li>Eigenvectors: 特征向量：</li>
</ol>
<ul>
<li>For <span class="math inline">\(\lambda=1\)</span>: solve <span
class="math inline">\((A-I)\mathbf{v} = 0\)</span>, giving <span
class="math inline">\((1,-1)\)</span>. 对于 <span
class="math inline">\(\lambda=1\)</span> ：求解 <span
class="math inline">\((A-I)\mathbf{v} = 0\)</span> ，得到 <span
class="math inline">\((1,-1)\)</span> 。</li>
<li>For <span class="math inline">\(\lambda=3\)</span>: solve <span
class="math inline">\((A-3I)\mathbf{v} = 0\)</span>, giving <span
class="math inline">\((1,1)\)</span>. 对于 <span
class="math inline">\(\lambda=3\)</span> ：求解 <span
class="math inline">\((A-3I)\mathbf{v} = 0\)</span> ，得到 <span
class="math inline">\((1,1)\)</span> 。</li>
</ul>
<ol start="3" type="1">
<li>Normalize eigenvectors: 归一化特征向量：</li>
</ol>
<p><span class="math display">\[
\mathbf{u}_1 = \tfrac{1}{\sqrt{2}}(1,-1), \quad \mathbf{u}_2 =
\tfrac{1}{\sqrt{2}}(1,1).
\]</span></p>
<ol start="4" type="1">
<li>Then 然后</li>
</ol>
<p><span class="math display">\[
Q =\begin{bmatrix}\tfrac{1}{\sqrt{2}} &amp; \tfrac{1}{\sqrt{2}} \\[6pt]
-\tfrac{1}{\sqrt{2}} &amp; \tfrac{1}{\sqrt{2}}\end{bmatrix},
\quad\Lambda =\begin{bmatrix}1 &amp; 0 \\0 &amp; 3\end{bmatrix}.
\]</span></p>
<p>So 所以</p>
<p><span class="math display">\[
A = Q \Lambda Q^T.
\]</span></p>
<h3 id="geometric-interpretation-21">Geometric Interpretation</h3>
<p>几何解释</p>
<p>The spectral theorem says every symmetric matrix acts like
independent scaling along orthogonal directions. In geometry, this
corresponds to stretching space along perpendicular axes.
谱定理指出，每个对称矩阵都像沿正交方向的独立缩放一样。在几何学中，这相当于沿垂直轴拉伸空间。</p>
<ul>
<li>Ellipses, ellipsoids, and quadratic surfaces can be fully understood
via eigenvalues and eigenvectors.
通过特征值和特征向量可以充分理解椭圆、椭圆体和二次曲面。</li>
<li>Orthogonality ensures directions remain perpendicular after
transformation. 正交性确保方向在变换后保持垂直。</li>
</ul>
<h3 id="applications-1">Applications</h3>
<p>应用</p>
<ul>
<li>Optimization: The spectral theorem underlies classification of
critical points via eigenvalues of the Hessian. 优化：谱定理是通过
Hessian 的特征值对临界点进行分类的基础。</li>
<li>PCA (Principal Component Analysis): Data covariance matrices are
symmetric, and PCA finds orthogonal directions of maximum variance.
PCA（主成分分析）：数据协方差矩阵是对称的，PCA
找到最大方差的正交方向。</li>
<li>Differential equations &amp; physics: Symmetric operators correspond
to measurable quantities with real eigenvalues ( stability, energy).
微分方程和物理学：对称算子对应于具有实特征值（稳定性、能量）的可测量量。</li>
</ul>
<h3 id="why-this-matters-33">Why this matters</h3>
<p>为什么这很重要</p>
<p>The spectral theorem guarantees that symmetric matrices are as simple
as possible: they can always be analyzed in terms of real, orthogonal
eigenvectors. This provides both deep theoretical insight and powerful
computational tools.
谱定理保证对称矩阵尽可能简单：它们总是可以用实数正交特征向量来分析。这既提供了深刻的理论见解，也提供了强大的计算工具。</p>
<h3 id="exercises-9.3">Exercises 9.3</h3>
<p>练习 9.3</p>
<ol type="1">
<li><p>Diagonalize 对角化</p>
<p><span class="math display">\[
A = \begin{bmatrix} 4 &amp; 2 \\ 2 &amp; 3 \end{bmatrix}
\]</span></p>
<p>using the spectral theorem. 使用谱定理。</p></li>
<li><p>Prove that all eigenvalues of a real symmetric matrix are real.
证明实对称矩阵的所有特征值都是实数。</p></li>
<li><p>Show that eigenvectors corresponding to distinct eigenvalues of a
symmetric matrix are orthogonal.
证明对称矩阵的不同特征值对应的特征向量是正交的。</p></li>
<li><p>Explain geometrically how the spectral theorem describes
ellipsoids defined by quadratic forms.
从几何角度解释谱定理如何描述由二次型定义的椭球体。</p></li>
<li><p>Apply the spectral theorem to the covariance matrix
将谱定理应用于协方差矩阵</p>
<p><span class="math display">\[
\Sigma = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix},
\]</span></p>
<p>and interpret the eigenvectors as principal directions of variance.
并将特征向量解释为方差的主方向。</p></li>
</ol>
<h2 id="principal-component-analysis-pca">9.4 Principal Component
Analysis (PCA)</h2>
<p>9.4 主成分分析（PCA）</p>
<p>Principal Component Analysis (PCA) is a widely used technique in data
science, machine learning, and statistics. At its core, PCA is an
application of the spectral theorem to covariance matrices: it finds
orthogonal directions (principal components) that capture the maximum
variance in data. 主成分分析 (PCA)
是数据科学、机器学习和统计学中广泛使用的技术。PCA
的核心是谱定理在协方差矩阵中的应用：它找到能够捕捉数据中最大方差的正交方向（主成分）。</p>
<h3 id="the-idea-1">The Idea</h3>
<p>理念</p>
<p>Given a dataset of vectors <span class="math inline">\(\mathbf{x}_1,
\mathbf{x}_2, \dots, \mathbf{x}_m \in \mathbb{R}^n\)</span>:
给定向量数据集 <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2,
\dots, \mathbf{x}_m \in \mathbb{R}^n\)</span> ：</p>
<ol type="1">
<li><p>Center the data by subtracting the mean vector <span
class="math inline">\(\bar{\mathbf{x}}\)</span>. 通过减去平均向量 <span
class="math inline">\(\bar{\mathbf{x}}\)</span> 使数据居中。</p></li>
<li><p>Form the covariance matrix 形成协方差矩阵</p>
<p><span class="math display">\[
\Sigma = \frac{1}{m} \sum_{i=1}^m (\mathbf{x}_i -
\bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T.
\]</span></p></li>
<li><p>Apply the spectral theorem: <span class="math inline">\(\Sigma =
Q \Lambda Q^T\)</span>. 应用谱定理： <span class="math inline">\(\Sigma
= Q \Lambda Q^T\)</span> 。</p>
<ul>
<li>Columns of <span class="math inline">\(Q\)</span> are orthonormal
eigenvectors (principal directions). <span
class="math inline">\(Q\)</span> 的列是正交特征向量（主方向）。</li>
<li>Eigenvalues in <span class="math inline">\(\Lambda\)</span> measure
variance explained by each direction. <span
class="math inline">\(\Lambda\)</span>
中的特征值测量每个方向解释的方差。</li>
</ul></li>
</ol>
<p>The first principal component is the eigenvector corresponding to the
largest eigenvalue; it is the direction of maximum variance.
第一个主成分是最大特征值对应的特征向量，是方差最大的方向。</p>
<h3 id="example-9.4.1">Example 9.4.1</h3>
<p>例 9.4.1</p>
<p>Suppose we have two-dimensional data points roughly aligned along the
line <span class="math inline">\(y = x\)</span>. The covariance matrix
is approximately 假设我们有二维数据点大致沿着直线 <span
class="math inline">\(y = x\)</span> 排列。协方差矩阵大约为</p>
<p><span class="math display">\[
\Sigma =\begin{bmatrix}2 &amp; 1.9 \\1.9 &amp; 2\end{bmatrix}.
\]</span></p>
<p>Eigenvalues are about <span class="math inline">\(3.9 and
\\0.1\)</span>. The eigenvector for <span class="math inline">\(\\lambda
= 3.9\)</span>is approximately<span
class="math inline">\((1,1)/\\sqrt{2}\)</span>. 特征值约为 $3.9 和 \ 0.1
$. The eigenvector for $ \lambda = 3.9 <span class="math inline">\(is
approximately\)</span> (1,1)/\sqrt{2}$。</p>
<ul>
<li>First principal component: the line <span class="math inline">\(y =
x\)</span>. 第一个主成分：线 <span class="math inline">\(y = x\)</span>
。</li>
<li>Most variance lies along this direction.
大部分差异都发生在这个方向。</li>
<li>Second component is nearly orthogonal (<span class="math inline">\(y
= -x\)</span>), but variance there is tiny. 第二个成分几乎正交（ <span
class="math inline">\(y = -x\)</span> ），但那里的方差很小。</li>
</ul>
<p>Thus PCA reduces the data to essentially one dimension. 因此，PCA
将数据简化为一个维度。</p>
<h3 id="applications-of-pca">Applications of PCA</h3>
<p>PCA 的应用</p>
<ol type="1">
<li>Dimensionality reduction: Represent data with fewer features while
retaining most variance.
降维：用较少的特征表示数据，同时保留大部分的方差。</li>
<li>Noise reduction: Small eigenvalues correspond to noise; discarding
them filters data.
降噪：较小的特征值对应噪声；丢弃它们可以过滤数据。</li>
<li>Visualization: Projecting high-dimensional data onto top 2 or 3
principal components reveals structure. 可视化：将高维数据投影到前 2
个或 3 个主成分上可以揭示结构。</li>
<li>Compression: PCA is used in image and signal compression. 压缩：PCA
用于图像和信号压缩。</li>
</ol>
<h3 id="connection-to-the-spectral-theorem">Connection to the Spectral
Theorem</h3>
<p>与谱定理的联系</p>
<p>The covariance matrix <span class="math inline">\(\Sigma\)</span> is
always symmetric and positive semidefinite. Hence by the spectral
theorem, it has an orthonormal basis of eigenvectors and nonnegative
real eigenvalues. PCA is nothing more than re-expressing data in this
eigenbasis. 协方差矩阵 <span class="math inline">\(\Sigma\)</span>
始终是对称的，且为半正定矩阵。因此，根据谱定理，它有一个由特征向量和非负实特征值组成的正交基。PCA
只不过是在这个特征基上重新表达数据。</p>
<h3 id="why-this-matters-34">Why this matters</h3>
<p>为什么这很重要</p>
<p>PCA demonstrates how abstract linear algebra directly powers modern
applications. Eigenvalues and eigenvectors give a practical method for
simplifying data, revealing patterns, and reducing complexity. It is one
of the most important algorithms derived from the spectral theorem. PCA
展示了抽象线性代数如何直接驱动现代应用。特征值和特征向量提供了一种简化数据、揭示模式和降低复杂性的实用方法。它是从谱定理中推导出的最重要的算法之一。</p>
<h3 id="exercises-9.4">Exercises 9.4</h3>
<p>练习 9.4</p>
<ol type="1">
<li>Show that the covariance matrix is symmetric and positive
semidefinite. 证明协方差矩阵是对称的和半正定的。</li>
<li>Compute the covariance matrix of the dataset <span
class="math inline">\((1,2), (2,3), (3,4)\)</span>, and find its
eigenvalues and eigenvectors. 计算数据集 <span
class="math inline">\((1,2), (2,3), (3,4)\)</span>
的协方差矩阵，并找到其特征值和特征向量。</li>
<li>Explain why the first principal component captures the maximum
variance. 解释为什么第一个主成分捕获最大方差。</li>
<li>In image compression, explain how PCA can reduce storage by keeping
only the top <span class="math inline">\(k\)</span> principal
components. 在图像压缩中，解释 PCA 如何通过仅保留前 <span
class="math inline">\(k\)</span> 个主成分来减少存储。</li>
<li>Prove that the sum of the eigenvalues of the covariance matrix
equals the total variance of the dataset.
证明协方差矩阵的特征值之和等于数据集的总方差。</li>
</ol>
<h1 id="chapter-10.-linear-algebra-in-practice">Chapter 10. Linear
Algebra in Practice</h1>
<p>第 10 章 线性代数实践</p>
<h2 id="computer-graphics-rotations-projections">10.1 Computer Graphics
(Rotations, Projections)</h2>
<p>10.1 计算机图形学（旋转、投影）</p>
<p>Linear algebra is the language of modern computer graphics. Every
image rendered on a screen, every 3D model rotated or projected, is
ultimately the result of applying matrices to vectors. Rotations,
reflections, scalings, and projections are all linear transformations,
making matrices the natural tool for manipulating geometry.
线性代数是现代计算机图形学的语言。屏幕上渲染的每一幅图像，以及旋转或投影的每一个
3D
模型，最终都是将矩阵应用于向量的结果。旋转、反射、缩放和投影都是线性变换，这使得矩阵成为处理几何图形的天然工具。</p>
<h3 id="rotations-in-2d">Rotations in 2D</h3>
<p>二维旋转</p>
<p>A counterclockwise rotation by an angle <span
class="math inline">\(\theta\)</span> in the plane is represented by
在平面上逆时针旋转角度 <span class="math inline">\(\theta\)</span>
表示为</p>
<p><span class="math display">\[
R_\theta =\begin{bmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta &amp;
\cos\theta\end{bmatrix}.
\]</span></p>
<p>For any vector <span class="math inline">\(\mathbf{v} \in
\mathbb{R}^2\)</span>, the rotated vector is 对于任意向量 <span
class="math inline">\(\mathbf{v} \in \mathbb{R}^2\)</span>
，旋转后的向量为</p>
<p><span class="math display">\[
\mathbf{v}&#39; = R_\theta \mathbf{v}.
\]</span></p>
<p>This preserves lengths and angles, since <span
class="math inline">\(R_\theta\)</span> is orthogonal with determinant
1. 这保留了长度和角度，因为𝑅 𝜃 R θ ​ 与行列式 1 正交。</p>
<h3 id="rotations-in-3d">Rotations in 3D</h3>
<p>3D 旋转</p>
<p>In three dimensions, rotations are represented by <span
class="math inline">\(3 \\times 3 orthogonal matrices with determinant
\\1\)</span>. For example, arotation about the <span
class="math inline">\(z\)</span>-axis is 在三维空间中，旋转由$3 \times 3
正交矩阵表示，其行列式为 \ 1 $. For example, arotation about the $
z$轴为</p>
<p><span class="math display">\[
R_z(\theta) =\begin{bmatrix}\cos\theta &amp; -\sin\theta &amp; 0
\\\sin\theta &amp; \cos\theta &amp; 0 \\0 &amp; 0 &amp; 1\end{bmatrix}.
\]</span></p>
<p>Similar formulas exist for rotations about the <span
class="math inline">\(x\)</span>- and <span
class="math inline">\(y\)</span>-axes. 对于绕 <span
class="math inline">\(x\)</span> 轴和 <span
class="math inline">\(y\)</span> 轴的旋转也存在类似的公式。</p>
<p>More general 3D rotations can be described by axis–angle
representation or quaternions, but the underlying idea is still linear
transformations represented by matrices. 更一般的 3D
旋转可以用轴角表示或四元数来描述，但其基本思想仍然是矩阵表示的线性变换。</p>
<h3 id="projections-1">Projections</h3>
<p>预测</p>
<p>To display 3D objects on a 2D screen, we use projections: 为了在 2D
屏幕上显示 3D 对象，我们使用投影：</p>
<ol type="1">
<li><p>Orthogonal projection: drops the <span
class="math inline">\(z\)</span>-coordinate, mapping <span
class="math inline">\((x,y,z) \mapsto (x,y)\)</span>. 正交投影：删除
<span class="math inline">\(z\)</span> 坐标，映射 <span
class="math inline">\((x,y,z) \mapsto (x,y)\)</span> 。</p>
<p><span class="math display">\[
P = \begin{bmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0\end{bmatrix}.
\]</span></p></li>
<li><p>Perspective projection: mimics the effect of a camera. A point
<span class="math inline">\((x,y,z)\)</span> projects to
透视投影：模拟相机的效果。点 <span
class="math inline">\((x,y,z)\)</span> 投影到</p>
<p><span class="math display">\[
\left(\frac{x}{z}, \frac{y}{z}\right),
\]</span></p>
<p>capturing how distant objects appear smaller.
捕捉远处物体如何显得更小。</p></li>
</ol>
<p>These operations are linear (orthogonal projection) or nearly linear
(perspective projection becomes linear in homogeneous coordinates).
这些操作是线性的（正交投影）或近似线性的（透视投影在齐次坐标中变为线性）。</p>
<h3 id="homogeneous-coordinates">Homogeneous Coordinates</h3>
<p>齐次坐标</p>
<p>To unify translations and projections with linear transformations,
computer graphics uses homogeneous coordinates. A 3D point <span
class="math inline">\((x,y,z)\)</span> is represented as a 4D vector
<span class="math inline">\((x,y,z,1)\)</span>. Transformations are then
4×4 matrices, which can represent rotations, scalings, and translations
in a single framework.
为了将平移和投影与线性变换统一起来，计算机图形学使用齐次坐标。3D 点
<span class="math inline">\((x,y,z)\)</span> 表示为四维向量 <span
class="math inline">\((x,y,z,1)\)</span> 。变换则表示为矩阵 4×4
，可以在单个框架中表示旋转、缩放和平移。</p>
<p>Example: Translation by <span class="math inline">\((a,b,c)\)</span>:
例如： <span class="math inline">\((a,b,c)\)</span> 翻译：</p>
<p><span class="math display">\[
T = \begin{bmatrix}1 &amp; 0 &amp; 0 &amp; a \\0 &amp; 1 &amp; 0 &amp; b
\\0 &amp; 0 &amp; 1 &amp; c \\0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix}.
\]</span></p>
<h3 id="geometric-interpretation-22">Geometric Interpretation</h3>
<p>几何解释</p>
<ul>
<li>Rotations preserve shape and size, only changing orientation.
旋转保持形状和大小，仅改变方向。</li>
<li>Projections reduce dimension: from 3D world space to 2D screen
space. 投影减少维度：从 3D 世界空间到 2D 屏幕空间。</li>
<li>Homogeneous coordinates allow us to combine multiple transformations
(rotation + translation + projection) into a single matrix
multiplication.
齐次坐标允许我们将多个变换（旋转+平移+投影）组合成单个矩阵乘法。</li>
</ul>
<h3 id="why-this-matters-35">Why this matters</h3>
<p>为什么这很重要</p>
<p>Linear algebra enables all real-time graphics: video games,
simulations, CAD software, and movie effects. By chaining simple matrix
operations, complex transformations are applied efficiently to millions
of points per second. 线性代数支持所有实时图形：视频游戏、模拟、CAD
软件和电影特效。通过链接简单的矩阵运算，复杂的变换可以高效地应用于每秒数百万个点。</p>
<h3 id="exercises-10.1">Exercises 10.1</h3>
<p>练习10.1</p>
<ol type="1">
<li>Write the rotation matrix for a 90° counterclockwise rotation in
<span class="math inline">\(\mathbb{R}^2\)</span>. Apply it to <span
class="math inline">\((1,0)\)</span>. 在 <span
class="math inline">\(\mathbb{R}^2\)</span> 中写出逆时针旋转 90°
的旋转矩阵。将其应用到 <span class="math inline">\((1,0)\)</span>
。</li>
<li>Rotate the point <span class="math inline">\((1,1,0)\)</span> about
the <span class="math inline">\(z\)</span>-axis by 180°. 将点 <span
class="math inline">\((1,1,0)\)</span> 绕 <span
class="math inline">\(z\)</span> 轴旋转 180°。</li>
<li>Show that the determinant of any 2D or 3D rotation matrix is 1.
证明任何二维或三维旋转矩阵的行列式为 1。</li>
<li>Derive the orthogonal projection matrix from <span
class="math inline">\(\mathbb{R}^3\)</span> to the <span
class="math inline">\(xy\)</span>-plane. 推导从 <span
class="math inline">\(\mathbb{R}^3\)</span> 到 <span
class="math inline">\(xy\)</span> 平面的正交投影矩阵。</li>
<li>Explain how homogeneous coordinates allow translations to be
represented as matrix multiplications.
解释齐次坐标如何允许平移表示为矩阵乘法。</li>
</ol>
<h2 id="data-science-dimensionality-reduction-least-squares">10.2 Data
Science (Dimensionality Reduction, Least Squares)</h2>
<p>10.2 数据科学（降维、最小二乘）</p>
<p>Linear algebra provides the foundation for many data science
techniques. Two of the most important are dimensionality reduction,
where high-dimensional datasets are compressed while preserving
essential information, and the least squares method, which underlies
regression and model fitting.
线性代数为许多数据科学技术奠定了基础。其中最重要的两个技术是降维（在保留基本信息的同时压缩高维数据集）和最小二乘法（回归和模型拟合的基础）。</p>
<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>
<p>降维</p>
<p>High-dimensional data often contains redundancy: many features are
correlated, meaning the data essentially lies near a lower-dimensional
subspace. Dimensionality reduction identifies these subspaces.
高维数据通常包含冗余：许多特征相互关联，这意味着数据本质上位于低维子空间附近。降维可以识别这些子空间。</p>
<ul>
<li><p>PCA (Principal Component Analysis): As introduced earlier, PCA
diagonalizes the covariance matrix of the data.
PCA（主成分分析）：如前所述，PCA 将数据的协方差矩阵对角化。</p>
<ul>
<li>Eigenvectors (principal components) define orthogonal directions of
maximum variance. 特征向量（主成分）定义最大方差的正交方向。</li>
<li>Eigenvalues measure how much variance lies along each direction.
特征值衡量每个方向上的方差。</li>
<li>Keeping only the top <span class="math inline">\(k\)</span>
components reduces data from <span
class="math inline">\(n\)</span>-dimensional space to <span
class="math inline">\(k\)</span>-dimensional space while retaining most
variability. 仅保留前 <span class="math inline">\(k\)</span>
个成分可将数据从 <span class="math inline">\(n\)</span> 维空间减少到
<span class="math inline">\(k\)</span>
维空间，同时保留大部分可变性。</li>
</ul></li>
</ul>
<p>Example 10.2.1. A dataset of 1000 images, each with 1024 pixels, may
have most variance captured by just 50 eigenvectors of the covariance
matrix. Projecting onto these components compresses the data while
preserving essential features. 例 10.2.1。一个包含 1000
幅图像的数据集，每幅图像有 1024 个像素，其大部分方差可能仅由协方差矩阵的
50 个特征向量捕获。投影到这些分量上可以压缩数据，同时保留基本特征。</p>
<h3 id="least-squares">Least Squares</h3>
<p>最小二乘法</p>
<p>Often, we have more equations than unknowns-an overdetermined system:
通常，我们的方程比未知数还多——一个超定系统：</p>
<p><span class="math display">\[
A\mathbf{x} \approx \mathbf{b}, \quad A \in \mathbb{R}^{m \times n}, \ m
&gt; n.
\]</span></p>
<p>An exact solution may not exist. Instead, we seek <span
class="math inline">\(\mathbf{x}\)</span> that minimizes the error
精确解可能不存在。因此，我们寻求最小化误差的 <span
class="math inline">\(\mathbf{x}\)</span></p>
<p><span class="math display">\[
\|A\mathbf{x} - \mathbf{b}\|^2.
\]</span></p>
<p>This leads to the normal equations: 这导致了正规方程：</p>
<p><span class="math display">\[
A^T A \mathbf{x} = A^T \mathbf{b}.
\]</span></p>
<p>The solution is the orthogonal projection of <span
class="math inline">\(\mathbf{b}\)</span> onto the column space of <span
class="math inline">\(A\)</span>. 解决方案是将 <span
class="math inline">\(\mathbf{b}\)</span> 正交投影到 <span
class="math inline">\(A\)</span> 的列空间上。</p>
<h3 id="example-10.2.2">Example 10.2.2</h3>
<p>例 10.2.2</p>
<p>Fit a line <span class="math inline">\(y = mx + c\)</span> to data
points <span class="math inline">\((x_i, y_i)\)</span>. 将线 <span
class="math inline">\(y = mx + c\)</span> 与数据点 <span
class="math inline">\((x_i, y_i)\)</span> 拟合。</p>
<p>Matrix form: 矩阵形式：</p>
<p><span class="math display">\[
A = \begin{bmatrix}x_1 &amp; 1 \\x_2 &amp; 1 \\\vdots &amp; \vdots \\x_m
&amp; 1\end{bmatrix},\quad\mathbf{b} =\begin{bmatrix}y_1 \\y_2 \\\vdots
\\y_m \end{bmatrix},\quad\mathbf{x} =\begin{bmatrix}m \\c \end{bmatrix}.
\]</span></p>
<p>Solve <span class="math inline">\(A^T A \mathbf{x} = A^T
\mathbf{b}\)</span>. This yields the best-fit line in the least squares
sense. 求解 <span class="math inline">\(A^T A \mathbf{x} = A^T
\mathbf{b}\)</span> 。这将得出最小二乘意义上的最佳拟合线。</p>
<h3 id="geometric-interpretation-23">Geometric Interpretation</h3>
<p>几何解释</p>
<ul>
<li>Dimensionality reduction: Find the best subspace capturing most
variance. 降维：找到捕获最多方差的最佳子空间。</li>
<li>Least squares: Project the target vector onto the subspace spanned
by predictors. 最小二乘：将目标向量投影到预测变量所跨越的子空间上。</li>
</ul>
<p>Both are projection problems, solved using inner products and
orthogonality. 两者都是投影问题，使用内积和正交性来解决。</p>
<h3 id="why-this-matters-36">Why this matters</h3>
<p>为什么这很重要</p>
<p>Dimensionality reduction makes large datasets tractable, filters
noise, and reveals structure. Least squares fitting powers regression,
statistics, and machine learning. Both rely directly on eigenvalues,
eigenvectors, and projections-core tools of linear algebra.
降维使大型数据集更易于处理，过滤噪声并揭示结构。最小二乘拟合为回归、统计和机器学习提供支持。两者都直接依赖于特征值、特征向量和投影——线性代数的核心工具。</p>
<h3 id="exercises-10.2">Exercises 10.2</h3>
<p>练习10.2</p>
<ol type="1">
<li>Explain why PCA reduces noise in datasets by discarding small
eigenvalue components. 解释为什么 PCA
通过丢弃较小的特征值分量来减少数据集中的噪声。</li>
<li>Compute the least squares solution to fitting a line through <span
class="math inline">\((0,0), (1,1), (2,2)\)</span>. 计算通过 <span
class="math inline">\((0,0), (1,1), (2,2)\)</span>
拟合直线的最小二乘解。</li>
<li>Show that the least squares solution is unique if and only if <span
class="math inline">\(A^T A\)</span> is invertible.
证明最小二乘解是唯一的当且仅当 <span class="math inline">\(A^T
A\)</span> 可逆。</li>
<li>Prove that the least squares solution minimizes the squared error by
projection arguments. 证明最小二乘解通过投影参数最小化平方误差。</li>
<li>Apply PCA to the data points <span class="math inline">\((1,0),
(2,1), (3,2)\)</span> and find the first principal component. 将 PCA
应用于数据点 <span class="math inline">\((1,0), (2,1), (3,2)\)</span>
并找到第一个主成分。</li>
</ol>
<h2 id="networks-and-markov-chains">10.3 Networks and Markov Chains</h2>
<p>10.3 网络和马尔可夫链</p>
<p>Graphs and networks provide a natural setting where linear algebra
comes to life. From modeling flows and connectivity to predicting
long-term behavior, matrices translate network structure into algebraic
form. Markov chains, already introduced in Section 8.4, are a central
example of networks evolving over time.
图和网络为线性代数的运用提供了自然的平台。从建模流和连接到预测长期行为，矩阵将网络结构转化为代数形式。马尔可夫链（已在
8.4 节介绍）是网络随时间演化的一个典型例子。</p>
<h3 id="adjacency-matrices">Adjacency Matrices</h3>
<p>邻接矩阵</p>
<p>A network (graph) with <span class="math inline">\(n\)</span> nodes
can be represented by an adjacency matrix <span class="math inline">\(A
\in \mathbb{R}^{n \times n}\)</span>: 具有 <span
class="math inline">\(n\)</span> 个节点的网络（图）可以用邻接矩阵 <span
class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> 表示：</p>
<p><span class="math display">\[
A_{ij} =\begin{cases}1 &amp; \text{if there is an edge from node \(i\)
to node \(j\)} \\0 &amp; \text{otherwise.}\end{cases}
\]</span></p>
<p>For weighted graphs, entries may be positive weights instead of 0/1.
对于加权图，条目可能是正权重而不是 0/1 。</p>
<ul>
<li>The number of walks of length <span class="math inline">\(k\)</span>
from node <span class="math inline">\(i\)</span> to node <span
class="math inline">\(j\)</span> is given by the entry <span
class="math inline">\((A^k)_{ij}\)</span>. 从节点 <span
class="math inline">\(i\)</span> 到节点 <span
class="math inline">\(j\)</span> 的长度为 <span
class="math inline">\(k\)</span> 的步行次数由条目 ( 𝐴 𝑘 ) 𝑖 𝑗 （一个 k )
伊奇 ​ .</li>
<li>Powers of adjacency matrices thus encode connectivity over time.
因此，邻接矩阵的幂可以对随时间变化的连通性进行编码。</li>
</ul>
<h3 id="laplacian-matrices">Laplacian Matrices</h3>
<p>拉普拉斯矩阵</p>
<p>Another important matrix is the graph Laplacian:
另一个重要的矩阵是图拉普拉斯矩阵：</p>
<p><span class="math display">\[
L = D - A,
\]</span></p>
<p>where <span class="math inline">\(D\)</span> is the diagonal degree
matrix ( <span class="math inline">\(D_{ii} = \text{degree}(i)\)</span>
). 其中 <span class="math inline">\(D\)</span> 是对角度矩阵 ( <span
class="math inline">\(D_{ii} = \text{degree}(i)\)</span> )。</p>
<ul>
<li><span class="math inline">\(L\)</span> is symmetric and positive
semidefinite. <span class="math inline">\(L\)</span>
是对称的并且是正半定的。</li>
<li>The smallest eigenvalue is always <span
class="math inline">\(0\)</span>, with eigenvector <span
class="math inline">\((1,1,\\dots,1)\)</span>. 最小特征值始终是 $0 $,
with eigenvector $ (1,1,\dots,1)$。</li>
<li>The multiplicity of eigenvalue 0 equals the number of connected
components in the graph. 特征值 0 的多重性等于图中连通分量的数量。</li>
</ul>
<p>This connection between eigenvalues and connectivity forms the basis
of spectral graph theory.
特征值和连通性之间的这种联系构成了谱图理论的基础。</p>
<h3 id="markov-chains-on-graphs">Markov Chains on Graphs</h3>
<p>图上的马尔可夫链</p>
<p>A Markov chain can be viewed as a random walk on a graph. If <span
class="math inline">\(P\)</span> is the transition matrix where <span
class="math inline">\(P_{ij}\)</span> is the probability of moving from
node <span class="math inline">\(i\)</span> to node <span
class="math inline">\(j\)</span>, then
马尔可夫链可以看作图上的随机游动。设 <span
class="math inline">\(P\)</span> 为转移矩阵，其中 𝑃 𝑖 𝑗 P 伊奇 ​ 是从节点
<span class="math inline">\(i\)</span> 移动到节点 <span
class="math inline">\(j\)</span> 的概率，那么</p>
<p><span class="math display">\[
\mathbf{x}_{k+1} = P \mathbf{x}_k
\]</span></p>
<p>describes the distribution of positions after <span
class="math inline">\(k\)</span> steps. 描述 <span
class="math inline">\(k\)</span> 步之后的位置分布。</p>
<ul>
<li>The steady-state distribution is given by the eigenvector of <span
class="math inline">\(P\)</span> with eigenvalue 1. 稳态分布由特征向量
<span class="math inline">\(P\)</span> 给出，特征值为 1 。</li>
<li>The speed of convergence depends on the gap between the largest
eigenvalue (which is always 1) and the second largest eigenvalue.
收敛速度取决于最大特征值（始终为 1 ）与第二大特征值之间的差距。</li>
</ul>
<h3 id="example-10.3.1">Example 10.3.1</h3>
<p>例 10.3.1</p>
<p>Consider a simple 3-node cycle graph: 考虑一个简单的 3
节点循环图：</p>
<p><span class="math display">\[
P = \begin{bmatrix}0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 \\1 &amp; 0
&amp; 0\end{bmatrix}.
\]</span></p>
<p>This Markov chain cycles deterministically among the nodes.
Eigenvalues are the cube roots of unity: <span class="math inline">\(1,
e^{2\\pi i/3}, e^{4\\pi i/3}. The eigenvalue \\1\)</span>corresponds to
the steady state, which is the uniformdistribution<span
class="math inline">\((1/3,1/3,1/3)\)</span>.
这个马尔可夫链在节点之间确定性地循环。特征值是 统一：$1,e^{2\pi
i/3},e^{4\pi i/3} 。特征值 \ 1 <span class="math inline">\(corresponds
to the steady state, which is the uniformdistribution\)</span>
(1/3,1/3,1/3)$。</p>
<h3 id="applications-2">Applications</h3>
<p>应用</p>
<ul>
<li>Search engines: Google’s PageRank algorithm models the web as a
Markov chain, where steady-state probabilities rank pages.
搜索引擎：Google 的 PageRank
算法将网络建模为马尔可夫链，其中稳态概率对网页进行排名。</li>
<li>Network analysis: Eigenvalues of adjacency or Laplacian matrices
reveal communities, bottlenecks, and robustness.
网络分析：邻接矩阵或拉普拉斯矩阵的特征值揭示社区、瓶颈和稳健性。</li>
<li>Epidemiology and information flow: Random walks model how diseases
or ideas spread through networks.
流行病学和信息流：随机游动模拟疾病或思想如何通过网络传播。</li>
</ul>
<h3 id="why-this-matters-37">Why this matters</h3>
<p>为什么这很重要</p>
<p>Linear algebra transforms network problems into matrix problems.
Eigenvalues and eigenvectors reveal connectivity, flow, stability, and
long-term dynamics. Networks are everywhere-social media, biology,
finance, and the internet-so these tools are indispensable.
线性代数将网络问题转化为矩阵问题。特征值和特征向量揭示了连通性、流动、稳定性和长期动态。网络无处不在——社交媒体、生物、金融和互联网——因此这些工具不可或缺。</p>
<h3 id="exercises-10.3">Exercises 10.3</h3>
<p>练习10.3</p>
<ol type="1">
<li><p>Write the adjacency matrix of a square graph with 4 nodes.
Compute <span class="math inline">\(A^2\)</span> and interpret the
entries. 写出一个有 4 个节点的正方形图的邻接矩阵。计算 <span
class="math inline">\(A^2\)</span> 并解释其中的元素。</p></li>
<li><p>Show that the Laplacian of a connected graph has exactly one zero
eigenvalue. 证明连通图的拉普拉斯算子恰好有一个零特征值。</p></li>
<li><p>Find the steady-state distribution of the Markov chain with
找到马尔可夫链的稳态分布</p>
<p><span class="math display">\[
P = \begin{bmatrix} 0.5 &amp; 0.5 \\ 0.4 &amp; 0.6 \end{bmatrix}.
\]</span></p></li>
<li><p>Explain how eigenvalues of the Laplacian can detect disconnected
components of a graph.
解释拉普拉斯算子的特征值如何检测图中不连续的组成部分。</p></li>
<li><p>Describe how PageRank modifies the transition matrix of the web
graph to ensure a unique steady-state distribution. 描述 PageRank
如何修改网络图的转换矩阵以确保唯一的稳态分布。</p></li>
</ol>
<h2 id="machine-learning-connections">10.4 Machine Learning
Connections</h2>
<p>10.4 机器学习连接</p>
<p>Modern machine learning is built on linear algebra. From the
representation of data as matrices to the optimization of large-scale
models, nearly every step relies on concepts such as vector spaces,
projections, eigenvalues, and matrix decompositions.
现代机器学习建立在线性代数的基础上。从数据矩阵表示到大规模模型的优化，几乎每一步都依赖于向量空间、投影、特征值和矩阵分解等概念。</p>
<h3 id="data-as-matrices">Data as Matrices</h3>
<p>数据作为矩阵</p>
<p>A dataset with <span class="math inline">\(m\)</span> examples and
<span class="math inline">\(n\)</span> features is represented as a
matrix <span class="math inline">\(X \in \mathbb{R}^{m \times
n}\)</span>: 具有 <span class="math inline">\(m\)</span> 个示例和 <span
class="math inline">\(n\)</span> 个特征的数据集表示为矩阵 <span
class="math inline">\(X \in \mathbb{R}^{m \times n}\)</span> ：</p>
<p><span class="math display">\[
X = \begin{bmatrix}
\text{-} &amp; \mathbf{x}_1^T &amp; \text{-} \\
\text{-} &amp; \mathbf{x}_2^T &amp; \text{-} \\
  &amp; \vdots &amp;   \\
\text{-} &amp; \mathbf{x}_m^T &amp; \text{-}
\end{bmatrix},
\]</span></p>
<p>where each row <span class="math inline">\(\mathbf{x}_i \in
\mathbb{R}^n\)</span> is a feature vector. Linear algebra provides tools
to analyze, compress, and transform this data. 其中每行 <span
class="math inline">\(\mathbf{x}_i \in \mathbb{R}^n\)</span>
是一个特征向量。线性代数提供了分析、压缩和转换此类数据的工具。</p>
<h3 id="linear-models">Linear Models</h3>
<p>线性模型</p>
<p>At the heart of machine learning are linear predictors:
机器学习的核心是线性预测器：</p>
<p><span class="math inline">\(\hat{y} = X\mathbf{w},\)</span></p>
<p>where <span class="math inline">\(\mathbf{w}\)</span> is the weight
vector. Training often involves solving a least squares problem or a
regularized variant such as ridge regression: 其中 <span
class="math inline">\(\mathbf{w}\)</span>
是权重向量。训练通常涉及求解最小二乘问题或正则化变体，例如岭回归：</p>
<p><span class="math inline">\(\min_{\mathbf{w}} \|X\mathbf{w} -
\mathbf{y}\|^2 + \lambda \|\mathbf{w}\|^2.\)</span></p>
<p>This is solved efficiently using matrix factorizations.
使用矩阵分解可以有效地解决这个问题。</p>
<h3 id="singular-value-decomposition-svd">Singular Value Decomposition
(SVD)</h3>
<p>奇异值分解（SVD）</p>
<p>The SVD of a matrix <span class="math inline">\(X\)</span> is 矩阵
<span class="math inline">\(X\)</span> 的 SVD 为</p>
<p><span class="math inline">\(X = U \Sigma V^T,\)</span></p>
<p>where <span class="math inline">\(U, V\)</span> are orthogonal and
<span class="math inline">\(\Sigma\)</span> is diagonal with nonnegative
entries (singular values). 其中 <span class="math inline">\(U,
V\)</span> 是正交的， <span class="math inline">\(\Sigma\)</span>
是对角的，具有非负项（奇异值）。</p>
<ul>
<li>Singular values measure the importance of directions in feature
space. 奇异值衡量特征空间中方向的重要性。</li>
<li>SVD is used for dimensionality reduction (low-rank approximations),
topic modeling, and recommender systems. SVD
用于降维（低秩近似）、主题建模和推荐系统。</li>
</ul>
<h3 id="eigenvalues-in-machine-learning">Eigenvalues in Machine
Learning</h3>
<p>机器学习中的特征值</p>
<ul>
<li>PCA (Principal Component Analysis): diagonalization of the
covariance matrix identifies directions of maximal variance.
PCA（主成分分析）：协方差矩阵的对角化确定了最大方差的方向。</li>
<li>Spectral clustering: uses eigenvectors of the Laplacian to group
data points into clusters.
谱聚类：使用拉普拉斯算子的特征向量将数据点分组成聚类。</li>
<li>Stability analysis: eigenvalues of Hessian matrices determine
whether optimization converges to a minimum. 稳定性分析：Hessian
矩阵的特征值决定优化是否收敛到最小值。</li>
</ul>
<h3 id="neural-networks">Neural Networks</h3>
<p>神经网络</p>
<p>Even deep learning, though nonlinear, uses linear algebra at its
core: 即使是深度学习，尽管是非线性的，其核心也使用线性代数：</p>
<ul>
<li>Each layer is a matrix multiplication followed by a nonlinear
activation. 每一层都是矩阵乘法，然后是非线性激活。</li>
<li>Training requires computing gradients, which are expressed in terms
of matrix calculus. 训练需要计算梯度，以矩阵微积分来表示。</li>
<li>Backpropagation is essentially repeated applications of the chain
rule with linear algebra.
反向传播本质上是链式法则与线性代数的重复应用。</li>
</ul>
<h3 id="why-this-matters-38">Why this matters</h3>
<p>为什么这很重要</p>
<p>Machine learning models often involve datasets with millions of
features and parameters. Linear algebra provides the algorithms and
abstractions that make training and inference possible. Without it,
large-scale computation in AI would be intractable.
机器学习模型通常涉及具有数百万个特征和参数的数据集。线性代数提供了使训练和推理成为可能的算法和抽象。如果没有它，人工智能中的大规模计算将变得难以处理。</p>
<h3 id="exercises-10.4">Exercises 10.4</h3>
<p>练习10.4</p>
<ol type="1">
<li>Show that ridge regression leads to the normal equations
证明岭回归可以得出正态方程</li>
</ol>
<p><span class="math display">\[
(X^T X + \lambda I)\mathbf{w} = X^T \mathbf{y}.
\]</span></p>
<ol start="2" type="1">
<li><p>Explain how SVD can be used to compress an image represented as a
matrix of pixel intensities. 解释如何使用 SVD
来压缩以像素强度矩阵表示的图像。</p></li>
<li><p>For a covariance matrix <span
class="math inline">\(\Sigma\)</span>, show why its eigenvalues
represent variances along principal components. 对于协方差矩阵 <span
class="math inline">\(\Sigma\)</span>
，说明为什么它的特征值表示沿主成分的方差。</p></li>
<li><p>Give an example of how eigenvectors of the Laplacian matrix can
be used for clustering a small graph.
举例说明如何使用拉普拉斯矩阵的特征向量对小图进行聚类。</p></li>
<li><p>In a neural network with one hidden layer, write the forward pass
in matrix form.
在具有一个隐藏层的神经网络中，以矩阵形式写出前向传递。</p></li>
</ol>
</article>
</body>
</html>