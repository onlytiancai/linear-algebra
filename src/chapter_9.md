# Chapter 9. Quadratic Forms and Spectral Theorems
ç¬¬ä¹ç« äºŒæ¬¡å‹å’Œè°±å®šç†

## 9.1 Quadratic Forms
9.1 äºŒæ¬¡å‹

A quadratic form is a polynomial of degree two in several variables, expressed neatly using matrices. Quadratic forms appear throughout mathematics: in optimization, geometry of conic sections, statistics (variance), and physics (energy functions).
äºŒæ¬¡å‹æ˜¯å¤šå…ƒäºŒæ¬¡å¤šé¡¹å¼ï¼Œå¯ä»¥ç”¨çŸ©é˜µç®€æ´åœ°è¡¨ç¤ºã€‚äºŒæ¬¡å‹åœ¨æ•°å­¦ä¸­éšå¤„å¯è§ï¼šä¼˜åŒ–ã€åœ†é”¥æ›²çº¿å‡ ä½•ã€ç»Ÿè®¡å­¦ï¼ˆæ–¹å·®ï¼‰å’Œç‰©ç†å­¦ï¼ˆèƒ½é‡å‡½æ•°ï¼‰ã€‚

### Definition
å®šä¹‰

Let $A$ be an $n \times n$ symmetric matrix and $\mathbf{x} \in \mathbb{R}^n$. The quadratic form associated with $A$ is
ä»¤ $A$ ä¸º $n \times n$ å¯¹ç§°çŸ©é˜µï¼Œ $\mathbf{x} \in \mathbb{R}^n$ ã€‚ä¸ $A$ ç›¸å…³çš„äºŒæ¬¡å¼ä¸º

$$
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}.
$$

Expanded,
æ‰©å±•ï¼Œ

$$
Q(\mathbf{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j.
$$

Because $A$ is symmetric ($a_{ij} = a_{ji}$), the cross-terms can be grouped naturally.
å› ä¸º $A$ æ˜¯å¯¹ç§°çš„ (ğ‘ ğ‘– ğ‘— = ğ‘ ğ‘— ğ‘– a ä¼Šå¥‡ â€‹ =a å§¬ â€‹ )ï¼Œäº¤å‰é¡¹å¯ä»¥è‡ªç„¶åˆ†ç»„ã€‚

### Examples
ç¤ºä¾‹

Example 9.1.1. For
ä¾‹ 9.1.1. å¯¹äº

$$
A = \begin{bmatrix}2 & 1 \\1 & 3 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix}x \\y \end{bmatrix},
$$

 

$$
Q(x,y) = \begin{bmatrix} x & y \end{bmatrix}\begin{bmatrix}2 & 1 \\1 & 3 \end{bmatrix}\begin{bmatrix}x \\y \end{bmatrix}= 2x^2 + 2xy + 3y^2.
$$

Example 9.1.2. The quadratic form
ä¾‹ 9.1.2. äºŒæ¬¡å‹

$$
Q(x,y) = x^2 + y^2
$$

corresponds to the matrix $A = I_2$. It measures squared Euclidean distance from the origin.
å¯¹åº”äºçŸ©é˜µğ´ = ğ¼ 2 A=I 2 â€‹ . å®ƒæµ‹é‡è·ç¦»åŸç‚¹çš„å¹³æ–¹æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚

Example 9.1.3. The conic section equation
ä¾‹ 9.1.3 åœ†é”¥æ›²çº¿æ–¹ç¨‹

$$
4x^2 + 2xy + 5y^2 = 1
$$

is described by the quadratic form $\mathbf{x}^T A \mathbf{x} = 1$ with
ç”±äºŒæ¬¡å‹ $\mathbf{x}^T A \mathbf{x} = 1$ æè¿°

$$
A = \begin{bmatrix}4 & 1 \\1 & 5\end{bmatrix}.
$$

### Diagonalization of Quadratic Forms
äºŒæ¬¡å‹çš„å¯¹è§’åŒ–

By choosing a new basis consisting of eigenvectors of $A$, we can rewrite the quadratic form without cross terms. If $A = PDP^{-1}$ with $D$ diagonal, then
é€šè¿‡é€‰æ‹©ç”± $A$ çš„ç‰¹å¾å‘é‡ç»„æˆçš„æ–°åŸºï¼Œæˆ‘ä»¬å¯ä»¥é‡å†™æ²¡æœ‰äº¤å‰é¡¹çš„äºŒæ¬¡å‹ã€‚å¦‚æœ $A = PDP^{-1}$ ä»¥ $D$ ä¸ºå¯¹è§’çº¿ï¼Œåˆ™

$$
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} = (P^{-1}\mathbf{x})^T D (P^{-1}\mathbf{x}).
$$

Thus quadratic forms can always be expressed as a sum of weighted squares:
å› æ­¤äºŒæ¬¡å‹æ€»æ˜¯å¯ä»¥è¡¨ç¤ºä¸ºåŠ æƒå¹³æ–¹å’Œï¼š

$$
Q(\mathbf{y}) = \lambda_1 y_1^2 + \cdots + \lambda_n y_n^2,
$$

where $\lambda_i$ are the eigenvalues of $A$.
å…¶ä¸­ğœ† ğ‘– Î» i â€‹ æ˜¯ $A$ çš„ç‰¹å¾å€¼ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Quadratic forms describe geometric shapes:
äºŒæ¬¡å‹æè¿°å‡ ä½•å½¢çŠ¶ï¼š

*   In 2D: ellipses, parabolas, hyperbolas.
    äºŒç»´ï¼šæ¤­åœ†ã€æŠ›ç‰©çº¿ã€åŒæ›²çº¿ã€‚
*   In 3D: ellipsoids, paraboloids, hyperboloids.
    åœ¨ 3D ä¸­ï¼šæ¤­åœ†ä½“ã€æŠ›ç‰©é¢ã€åŒæ›²é¢ã€‚
*   In higher dimensions: generalizations of ellipsoids.
    åœ¨æ›´é«˜ç»´åº¦ä¸­ï¼šæ¤­åœ†ä½“çš„æ¦‚æ‹¬ã€‚

Diagonalization aligns the coordinate axes with the principal axes of the shape.
å¯¹è§’åŒ–å°†åæ ‡è½´ä¸å½¢çŠ¶çš„ä¸»è½´å¯¹é½ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Quadratic forms unify geometry and algebra. They are central in optimization (minimizing energy functions), statistics ( covariance matrices and variance), mechanics (kinetic energy), and numerical analysis. Understanding quadratic forms leads directly to the spectral theorem.
äºŒæ¬¡å‹ç»Ÿä¸€äº†å‡ ä½•å’Œä»£æ•°ã€‚å®ƒä»¬åœ¨ä¼˜åŒ–ï¼ˆæœ€å°åŒ–èƒ½é‡å‡½æ•°ï¼‰ã€ç»Ÿè®¡å­¦ï¼ˆåæ–¹å·®çŸ©é˜µå’Œæ–¹å·®ï¼‰ã€åŠ›å­¦ï¼ˆåŠ¨èƒ½ï¼‰å’Œæ•°å€¼åˆ†æä¸­éƒ½è‡³å…³é‡è¦ã€‚ç†è§£äºŒæ¬¡å‹å¯ä»¥ç›´æ¥å¼•å‡ºè°±å®šç†ã€‚

### Exercises 9.1
ç»ƒä¹  9.1

1.  Write the quadratic form $Q(x,y) = 3x^2 + 4xy + y^2$ as $\mathbf{x}^T A \mathbf{x}$ for some symmetric matrix $A$.
    å¯¹äºæŸäº›å¯¹ç§°çŸ©é˜µ $A$ ï¼Œå°†äºŒæ¬¡å‹ $Q(x,y) = 3x^2 + 4xy + y^2$ å†™ä¸º $\mathbf{x}^T A \mathbf{x}$ ã€‚
2.  For $A = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$, compute $Q(x,y)$ explicitly.
    å¯¹äº $A = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$ ï¼Œæ˜ç¡®è®¡ç®— $Q(x,y)$ ã€‚
3.  Diagonalize the quadratic form $Q(x,y) = 2x^2 + 2xy + 3y^2$.
    å°†äºŒæ¬¡å‹ $Q(x,y) = 2x^2 + 2xy + 3y^2$ å¯¹è§’åŒ–ã€‚
4.  Identify the conic section given by $Q(x,y) = x^2 - y^2$.
    ç¡®å®šç”± $Q(x,y) = x^2 - y^2$ ç»™å‡ºçš„åœ†é”¥æˆªé¢ã€‚
5.  Show that if $A$ is symmetric, quadratic forms defined by $A$ and $A^T$ are identical.
    è¯æ˜å¦‚æœ $A$ æ˜¯å¯¹ç§°çš„ï¼Œåˆ™ç”± $A$ å’Œ $A^T$ å®šä¹‰çš„äºŒæ¬¡å‹æ˜¯ç›¸åŒçš„ã€‚

## 9.2 Positive Definite Matrices
9.2 æ­£å®šçŸ©é˜µ

Quadratic forms are especially important when their associated matrices are positive definite, since these guarantee positivity of energy, distance, or variance. Positive definiteness is a cornerstone in optimization, numerical analysis, and statistics.
å½“äºŒæ¬¡å‹çš„ç›¸å…³çŸ©é˜µä¸ºæ­£å®šçŸ©é˜µæ—¶ï¼Œå®ƒä»¬å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ä¿è¯èƒ½é‡ã€è·ç¦»æˆ–æ–¹å·®çš„æ­£æ€§ã€‚æ­£å®šæ€§æ˜¯ä¼˜åŒ–ã€æ•°å€¼åˆ†æå’Œç»Ÿè®¡å­¦çš„åŸºçŸ³ã€‚

### Definition
å®šä¹‰

A symmetric matrix $A \in \mathbb{R}^{n \times n}$ is called:
å¯¹ç§°çŸ©é˜µ $A \in \mathbb{R}^{n \times n}$ ç§°ä¸ºï¼š

*   Positive definite if
    æ­£å®šå¦‚æœ
    
    $$
    \mathbf{x}^T A \mathbf{x} > 0 \quad \text{for all nonzero } \mathbf{x} \in \mathbb{R}^n.
    $$
    
*   Positive semidefinite if
    æ­£åŠå®šçš„ï¼Œå¦‚æœ
    
    $$
    \mathbf{x}^T A \mathbf{x} \geq 0 \quad \text{for all } \mathbf{x}.
    $$
    

Similarly, negative definite (always < 0) and indefinite (can be both < 0 and > 0) matrices are defined.
ç±»ä¼¼åœ°ï¼Œå®šä¹‰äº†è´Ÿå®šï¼ˆå§‹ç»ˆ < 0ï¼‰å’Œä¸å®šï¼ˆå¯ä»¥åŒæ—¶ < 0 å’Œ > 0ï¼‰çŸ©é˜µã€‚

### Examples
ç¤ºä¾‹

Example 9.2.1.
ä¾‹ 9.2.1ã€‚

$$
A = \begin{bmatrix}2 & 0 \\0 & 3 \end{bmatrix}
$$

is positive definite, since
æ˜¯æ­£å®šçš„ï¼Œå› ä¸º

$$
Q(x,y) = 2x^2 + 3y^2 > 0
$$

for all $(x,y) \neq (0,0)$.
å¯¹äºæ‰€æœ‰ $(x,y) \neq (0,0)$ ã€‚

Example 9.2.2.
ä¾‹ 9.2.2ã€‚

$$
A = \begin{bmatrix}1 & 2 \\2 & 1 \end{bmatrix}
$$

has quadratic form
å…·æœ‰äºŒæ¬¡å½¢å¼

$$
Q(x,y) = x^2 + 4xy + y^2.
$$

This matrix is not positive definite, since $Q(1,-1) = -2 < 0$.
è¯¥çŸ©é˜µä¸æ˜¯æ­£å®šçš„ï¼Œå› ä¸º $Q(1,-1) = -2 < 0$ ã€‚

### Characterizations
ç‰¹å¾

For a symmetric matrix $A$:
å¯¹äºå¯¹ç§°çŸ©é˜µ $A$ ï¼š

1.  Eigenvalue test: $A$ is positive definite if and only if all eigenvalues of $A$ are positive.
    ç‰¹å¾å€¼æ£€éªŒï¼šå½“ä¸”ä»…å½“ $A$ çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½ä¸ºæ­£æ—¶ï¼Œ $A$ æ‰æ˜¯æ­£å®šçš„ã€‚
    
2.  Principal minors test (Sylvesterâ€™s criterion): $A$ is positive definite if and only if all leading principal minors ( determinants of top-left $k \times k$ submatrices) are positive.
    ä¸»å­å¼æ£€éªŒï¼ˆè¥¿å°”ç»´æ–¯ç‰¹æ ‡å‡†ï¼‰ï¼šå½“ä¸”ä»…å½“æ‰€æœ‰é¦–é¡¹ä¸»å­å¼ï¼ˆå·¦ä¸Šè§’ $k \times k$ å­çŸ©é˜µçš„è¡Œåˆ—å¼ï¼‰å‡ä¸ºæ­£æ—¶ï¼Œ $A$ æ‰æ˜¯æ­£å®šçš„ã€‚
    
3.  Cholesky factorization: $A$ is positive definite if and only if it can be written as
    Cholesky åˆ†è§£ï¼š $A$ ä¸ºæ­£å®šå½“ä¸”ä»…å½“å®ƒå¯ä»¥å†™æˆ
    
    $$
    A = R^T R,
    $$
    
    where $R$ is an upper triangular matrix with positive diagonal entries.
    å…¶ä¸­ $R$ æ˜¯å…·æœ‰æ­£å¯¹è§’çº¿é¡¹çš„ä¸Šä¸‰è§’çŸ©é˜µã€‚
    

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   Positive definite matrices correspond to quadratic forms that define ellipsoids centered at the origin.
    æ­£å®šçŸ©é˜µå¯¹åº”äºå®šä¹‰ä»¥åŸç‚¹ä¸ºä¸­å¿ƒçš„æ¤­åœ†ä½“çš„äºŒæ¬¡å‹ã€‚
*   Positive semidefinite matrices define flattened ellipsoids (possibly degenerate).
    æ­£åŠå®šçŸ©é˜µå®šä¹‰æ‰å¹³çš„æ¤­çƒä½“ï¼ˆå¯èƒ½æ˜¯é€€åŒ–çš„ï¼‰ã€‚
*   Indefinite matrices define hyperbolas or saddle-shaped surfaces.
    ä¸å®šçŸ©é˜µå®šä¹‰åŒæ›²çº¿æˆ–é©¬éå½¢æ›²é¢ã€‚

### Applications
åº”ç”¨

*   Optimization: Hessians of convex functions are positive semidefinite; strict convexity corresponds to positive definite Hessians.
    ä¼˜åŒ–ï¼šå‡¸å‡½æ•°çš„ Hessian çŸ©é˜µæ˜¯æ­£åŠå®šçš„ï¼›ä¸¥æ ¼å‡¸æ€§å¯¹åº”äºæ­£å®šçš„ Hessian çŸ©é˜µã€‚
*   Statistics: Covariance matrices are positive semidefinite.
    ç»Ÿè®¡ï¼šåæ–¹å·®çŸ©é˜µæ˜¯æ­£åŠå®šçš„ã€‚
*   Numerical methods: Cholesky decomposition is widely used to solve systems with positive definite matrices efficiently.
    æ•°å€¼æ–¹æ³•ï¼šCholesky åˆ†è§£è¢«å¹¿æ³›ç”¨äºæœ‰æ•ˆåœ°è§£å†³å…·æœ‰æ­£å®šçŸ©é˜µçš„ç³»ç»Ÿã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Positive definiteness provides stability and guarantees in mathematics and computation. It ensures energy functions are bounded below, optimization problems have unique solutions, and statistical models are meaningful.
æ­£å®šæ€§åœ¨æ•°å­¦å’Œè®¡ç®—ä¸­æä¾›äº†ç¨³å®šæ€§å’Œä¿è¯ã€‚å®ƒç¡®ä¿èƒ½é‡å‡½æ•°æœ‰ç•Œï¼Œä¼˜åŒ–é—®é¢˜æœ‰å”¯ä¸€è§£ï¼Œç»Ÿè®¡æ¨¡å‹æœ‰æ„ä¹‰ã€‚

### Exercises 9.2
ç»ƒä¹  9.2

1.  Use Sylvesterâ€™s criterion to check whether
    ä½¿ç”¨ Sylvester æ ‡å‡†æ£€æŸ¥
    
    $$
    A = \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}
    $$
    
    is positive definite.
    æ˜¯æ­£å®šçš„ã€‚
    
2.  Determine whether
    ç¡®å®šæ˜¯å¦
    
    $$
    A = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
    $$
    
    is positive definite, semidefinite, or indefinite.
    æ˜¯æ­£å®šçš„ã€åŠå®šçš„æˆ–ä¸å®šçš„ã€‚
    
3.  Find the eigenvalues of
    æ‰¾åˆ°ç‰¹å¾å€¼
    
    $$
    A = \begin{bmatrix} 4 & 2 \\ 2 & 3 \end{bmatrix},
    $$
    
    and use them to classify definiteness.
    å¹¶ç”¨å®ƒä»¬æ¥å¯¹ç¡®å®šæ€§è¿›è¡Œåˆ†ç±»ã€‚
    
4.  Prove that all diagonal matrices with positive entries are positive definite.
    è¯æ˜æ‰€æœ‰å…·æœ‰æ­£é¡¹çš„å¯¹è§’çŸ©é˜µéƒ½æ˜¯æ­£å®šçš„ã€‚
    
5.  Show that if $A$ is positive definite, then so is $P^T A P$ for any invertible matrix $P$.
    è¯æ˜å¦‚æœ $A$ ä¸ºæ­£å®šçŸ©é˜µï¼Œåˆ™å¯¹äºä»»ä½•å¯é€†çŸ©é˜µ $P$ ï¼Œ $P^T A P$ ä¹Ÿä¸ºæ­£å®šçŸ©é˜µã€‚
    

## 9.3 Spectral Theorem
9.3 è°±å®šç†

The spectral theorem is one of the most powerful results in linear algebra. It states that symmetric matrices can always be diagonalized by an orthogonal basis of eigenvectors. This links algebra (eigenvalues), geometry (orthogonal directions), and applications (stability, optimization, statistics).
è°±å®šç†æ˜¯çº¿æ€§ä»£æ•°ä¸­æœ€æœ‰åŠ›çš„ç»“è®ºä¹‹ä¸€ã€‚å®ƒæŒ‡å‡ºå¯¹ç§°çŸ©é˜µæ€»æ˜¯å¯ä»¥é€šè¿‡ç‰¹å¾å‘é‡çš„æ­£äº¤åŸºå¯¹è§’åŒ–ã€‚è¿™è¿æ¥äº†ä»£æ•°ï¼ˆç‰¹å¾å€¼ï¼‰ã€å‡ ä½•ï¼ˆæ­£äº¤æ–¹å‘ï¼‰å’Œåº”ç”¨ï¼ˆç¨³å®šæ€§ã€ä¼˜åŒ–ã€ç»Ÿè®¡ï¼‰ã€‚

### Statement of the Spectral Theorem
è°±å®šç†è¡¨è¿°

If $A \in \mathbb{R}^{n \times n}$ is symmetric ($A^T = A$), then:
å¦‚æœ $A \in \mathbb{R}^{n \times n}$ æ˜¯å¯¹ç§°çš„ï¼ˆ $A^T = A$ ï¼‰ï¼Œåˆ™ï¼š

1.  All eigenvalues of $A$ are real.
    $A$ çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯å®æ•°ã€‚
    
2.  There exists an orthonormal basis of $\mathbb{R}^n$ consisting of eigenvectors of $A$.
    å­˜åœ¨ç”± $A$ çš„ç‰¹å¾å‘é‡ç»„æˆçš„ $\mathbb{R}^n$ æ­£äº¤åŸºã€‚
    
3.  Thus, $A$ can be written as
    å› æ­¤ï¼Œ $A$ å¯ä»¥å†™æˆ
    
    $$
    A = Q \Lambda Q^T,
    $$
    
    where $Q$ is an orthogonal matrix ($Q^T Q = I$) and $\Lambda$ is diagonal with eigenvalues of $A$ on the diagonal.
    å…¶ä¸­ $Q$ æ˜¯æ­£äº¤çŸ©é˜µ ( $Q^T Q = I$ )ï¼Œ $\Lambda$ æ˜¯å¯¹è§’çŸ©é˜µï¼Œå…¶ç‰¹å¾å€¼ $A$ ä½äºå¯¹è§’çº¿ä¸Šã€‚
    

### Consequences
ç»“æœ

*   Symmetric matrices are always diagonalizable, and the diagonalization is numerically stable.
    å¯¹ç§°çŸ©é˜µæ€»æ˜¯å¯å¯¹è§’åŒ–çš„ï¼Œå¹¶ä¸”å¯¹è§’åŒ–åœ¨æ•°å€¼ä¸Šæ˜¯ç¨³å®šçš„ã€‚
*   Quadratic forms $\mathbf{x}^T A \mathbf{x}$ can be expressed in terms of eigenvalues and eigenvectors, showing ellipsoids aligned with eigen-directions.
    äºŒæ¬¡å‹ $\mathbf{x}^T A \mathbf{x}$ å¯ä»¥ç”¨ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ¥è¡¨ç¤ºï¼Œæ˜¾ç¤ºä¸ç‰¹å¾æ–¹å‘å¯¹é½çš„æ¤­åœ†ä½“ã€‚
*   Positive definiteness can be checked by confirming that all eigenvalues are positive.
    å¯ä»¥é€šè¿‡ç¡®è®¤æ‰€æœ‰ç‰¹å¾å€¼éƒ½ä¸ºæ­£æ¥æ£€æŸ¥æ­£å®šæ€§ã€‚

### Example 9.3.1
ä¾‹ 9.3.1

Let
è®©

$$
A = \begin{bmatrix}2 & 1 \\1 & 2 \end{bmatrix}.
$$

1.  Characteristic polynomial:
    ç‰¹å¾å¤šé¡¹å¼ï¼š

$$
p(\lambda) = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
$$

Eigenvalues: $\lambda_1 = 1, \ \lambda_2 = 3$.
ç‰¹å¾å€¼ï¼š $\lambda_1 = 1, \ \lambda_2 = 3$ ã€‚

2.  Eigenvectors:
    ç‰¹å¾å‘é‡ï¼š

*   For $\lambda=1$: solve $(A-I)\mathbf{v} = 0$, giving $(1,-1)$.
    å¯¹äº $\lambda=1$ ï¼šæ±‚è§£ $(A-I)\mathbf{v} = 0$ ï¼Œå¾—åˆ° $(1,-1)$ ã€‚
*   For $\lambda=3$: solve $(A-3I)\mathbf{v} = 0$, giving $(1,1)$.
    å¯¹äº $\lambda=3$ ï¼šæ±‚è§£ $(A-3I)\mathbf{v} = 0$ ï¼Œå¾—åˆ° $(1,1)$ ã€‚

3.  Normalize eigenvectors:
    å½’ä¸€åŒ–ç‰¹å¾å‘é‡ï¼š

$$
\mathbf{u}_1 = \tfrac{1}{\sqrt{2}}(1,-1), \quad \mathbf{u}_2 = \tfrac{1}{\sqrt{2}}(1,1).
$$

4.  Then
    ç„¶å

$$
Q =\begin{bmatrix}\tfrac{1}{\sqrt{2}} & \tfrac{1}{\sqrt{2}} \\[6pt] -\tfrac{1}{\sqrt{2}} & \tfrac{1}{\sqrt{2}}\end{bmatrix}, \quad\Lambda =\begin{bmatrix}1 & 0 \\0 & 3\end{bmatrix}.
$$

So
æ‰€ä»¥

$$
A = Q \Lambda Q^T.
$$

### Geometric Interpretation
å‡ ä½•è§£é‡Š

The spectral theorem says every symmetric matrix acts like independent scaling along orthogonal directions. In geometry, this corresponds to stretching space along perpendicular axes.
è°±å®šç†æŒ‡å‡ºï¼Œæ¯ä¸ªå¯¹ç§°çŸ©é˜µéƒ½åƒæ²¿æ­£äº¤æ–¹å‘çš„ç‹¬ç«‹ç¼©æ”¾ä¸€æ ·ã€‚åœ¨å‡ ä½•å­¦ä¸­ï¼Œè¿™ç›¸å½“äºæ²¿å‚ç›´è½´æ‹‰ä¼¸ç©ºé—´ã€‚

*   Ellipses, ellipsoids, and quadratic surfaces can be fully understood via eigenvalues and eigenvectors.
    é€šè¿‡ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡å¯ä»¥å……åˆ†ç†è§£æ¤­åœ†ã€æ¤­åœ†ä½“å’ŒäºŒæ¬¡æ›²é¢ã€‚
*   Orthogonality ensures directions remain perpendicular after transformation.
    æ­£äº¤æ€§ç¡®ä¿æ–¹å‘åœ¨å˜æ¢åä¿æŒå‚ç›´ã€‚

### Applications
åº”ç”¨

*   Optimization: The spectral theorem underlies classification of critical points via eigenvalues of the Hessian.
    ä¼˜åŒ–ï¼šè°±å®šç†æ˜¯é€šè¿‡ Hessian çš„ç‰¹å¾å€¼å¯¹ä¸´ç•Œç‚¹è¿›è¡Œåˆ†ç±»çš„åŸºç¡€ã€‚
*   PCA (Principal Component Analysis): Data covariance matrices are symmetric, and PCA finds orthogonal directions of maximum variance.
    PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ï¼šæ•°æ®åæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼ŒPCA æ‰¾åˆ°æœ€å¤§æ–¹å·®çš„æ­£äº¤æ–¹å‘ã€‚
*   Differential equations & physics: Symmetric operators correspond to measurable quantities with real eigenvalues ( stability, energy).
    å¾®åˆ†æ–¹ç¨‹å’Œç‰©ç†å­¦ï¼šå¯¹ç§°ç®—å­å¯¹åº”äºå…·æœ‰å®ç‰¹å¾å€¼ï¼ˆç¨³å®šæ€§ã€èƒ½é‡ï¼‰çš„å¯æµ‹é‡é‡ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

The spectral theorem guarantees that symmetric matrices are as simple as possible: they can always be analyzed in terms of real, orthogonal eigenvectors. This provides both deep theoretical insight and powerful computational tools.
è°±å®šç†ä¿è¯å¯¹ç§°çŸ©é˜µå°½å¯èƒ½ç®€å•ï¼šå®ƒä»¬æ€»æ˜¯å¯ä»¥ç”¨å®æ•°æ­£äº¤ç‰¹å¾å‘é‡æ¥åˆ†æã€‚è¿™æ—¢æä¾›äº†æ·±åˆ»çš„ç†è®ºè§è§£ï¼Œä¹Ÿæä¾›äº†å¼ºå¤§çš„è®¡ç®—å·¥å…·ã€‚

### Exercises 9.3
ç»ƒä¹  9.3

1.  Diagonalize
    å¯¹è§’åŒ–
    
    $$
    A = \begin{bmatrix} 4 & 2 \\ 2 & 3 \end{bmatrix}
    $$
    
    using the spectral theorem.
    ä½¿ç”¨è°±å®šç†ã€‚
    
2.  Prove that all eigenvalues of a real symmetric matrix are real.
    è¯æ˜å®å¯¹ç§°çŸ©é˜µçš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯å®æ•°ã€‚
    
3.  Show that eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal.
    è¯æ˜å¯¹ç§°çŸ©é˜µçš„ä¸åŒç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡æ˜¯æ­£äº¤çš„ã€‚
    
4.  Explain geometrically how the spectral theorem describes ellipsoids defined by quadratic forms.
    ä»å‡ ä½•è§’åº¦è§£é‡Šè°±å®šç†å¦‚ä½•æè¿°ç”±äºŒæ¬¡å‹å®šä¹‰çš„æ¤­çƒä½“ã€‚
    
5.  Apply the spectral theorem to the covariance matrix
    å°†è°±å®šç†åº”ç”¨äºåæ–¹å·®çŸ©é˜µ
    
    $$
    \Sigma = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix},
    $$
    
    and interpret the eigenvectors as principal directions of variance.
    å¹¶å°†ç‰¹å¾å‘é‡è§£é‡Šä¸ºæ–¹å·®çš„ä¸»æ–¹å‘ã€‚
    

## 9.4 Principal Component Analysis (PCA)
9.4 ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰

Principal Component Analysis (PCA) is a widely used technique in data science, machine learning, and statistics. At its core, PCA is an application of the spectral theorem to covariance matrices: it finds orthogonal directions (principal components) that capture the maximum variance in data.
ä¸»æˆåˆ†åˆ†æ (PCA) æ˜¯æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡å­¦ä¸­å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ã€‚PCA çš„æ ¸å¿ƒæ˜¯è°±å®šç†åœ¨åæ–¹å·®çŸ©é˜µä¸­çš„åº”ç”¨ï¼šå®ƒæ‰¾åˆ°èƒ½å¤Ÿæ•æ‰æ•°æ®ä¸­æœ€å¤§æ–¹å·®çš„æ­£äº¤æ–¹å‘ï¼ˆä¸»æˆåˆ†ï¼‰ã€‚

### The Idea
ç†å¿µ

Given a dataset of vectors $\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_m \in \mathbb{R}^n$:
ç»™å®šå‘é‡æ•°æ®é›† $\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_m \in \mathbb{R}^n$ ï¼š

1.  Center the data by subtracting the mean vector $\bar{\mathbf{x}}$.
    é€šè¿‡å‡å»å¹³å‡å‘é‡ $\bar{\mathbf{x}}$ ä½¿æ•°æ®å±…ä¸­ã€‚
    
2.  Form the covariance matrix
    å½¢æˆåæ–¹å·®çŸ©é˜µ
    
    $$
    \Sigma = \frac{1}{m} \sum_{i=1}^m (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T.
    $$
    
3.  Apply the spectral theorem: $\Sigma = Q \Lambda Q^T$.
    åº”ç”¨è°±å®šç†ï¼š $\Sigma = Q \Lambda Q^T$ ã€‚
    
    *   Columns of $Q$ are orthonormal eigenvectors (principal directions).
        $Q$ çš„åˆ—æ˜¯æ­£äº¤ç‰¹å¾å‘é‡ï¼ˆä¸»æ–¹å‘ï¼‰ã€‚
    *   Eigenvalues in $\Lambda$ measure variance explained by each direction.
        $\Lambda$ ä¸­çš„ç‰¹å¾å€¼æµ‹é‡æ¯ä¸ªæ–¹å‘è§£é‡Šçš„æ–¹å·®ã€‚

The first principal component is the eigenvector corresponding to the largest eigenvalue; it is the direction of maximum variance.
ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ˜¯æœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œæ˜¯æ–¹å·®æœ€å¤§çš„æ–¹å‘ã€‚

### Example 9.4.1
ä¾‹ 9.4.1

Suppose we have two-dimensional data points roughly aligned along the line $y = x$. The covariance matrix is approximately
å‡è®¾æˆ‘ä»¬æœ‰äºŒç»´æ•°æ®ç‚¹å¤§è‡´æ²¿ç€ç›´çº¿ $y = x$ æ’åˆ—ã€‚åæ–¹å·®çŸ©é˜µå¤§çº¦ä¸º

$$
\Sigma =\begin{bmatrix}2 & 1.9 \\1.9 & 2\end{bmatrix}.
$$

Eigenvalues are about $3.9 and \\0.1$. The eigenvector for $\\lambda = 3.9$is approximately$(1,1)/\\sqrt{2}$.
ç‰¹å¾å€¼çº¦ä¸º $3.9 å’Œ \\ 0.1 $. The eigenvector for $ \\lambda = 3.9 $is approximately$ (1,1)/\\sqrt{2}$ã€‚

*   First principal component: the line $y = x$.
    ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ï¼šçº¿ $y = x$ ã€‚
*   Most variance lies along this direction.
    å¤§éƒ¨åˆ†å·®å¼‚éƒ½å‘ç”Ÿåœ¨è¿™ä¸ªæ–¹å‘ã€‚
*   Second component is nearly orthogonal ($y = -x$), but variance there is tiny.
    ç¬¬äºŒä¸ªæˆåˆ†å‡ ä¹æ­£äº¤ï¼ˆ $y = -x$ ï¼‰ï¼Œä½†é‚£é‡Œçš„æ–¹å·®å¾ˆå°ã€‚

Thus PCA reduces the data to essentially one dimension.
å› æ­¤ï¼ŒPCA å°†æ•°æ®ç®€åŒ–ä¸ºä¸€ä¸ªç»´åº¦ã€‚

### Applications of PCA
PCA çš„åº”ç”¨

1.  Dimensionality reduction: Represent data with fewer features while retaining most variance.
    é™ç»´ï¼šç”¨è¾ƒå°‘çš„ç‰¹å¾è¡¨ç¤ºæ•°æ®ï¼ŒåŒæ—¶ä¿ç•™å¤§éƒ¨åˆ†çš„æ–¹å·®ã€‚
2.  Noise reduction: Small eigenvalues correspond to noise; discarding them filters data.
    é™å™ªï¼šè¾ƒå°çš„ç‰¹å¾å€¼å¯¹åº”å™ªå£°ï¼›ä¸¢å¼ƒå®ƒä»¬å¯ä»¥è¿‡æ»¤æ•°æ®ã€‚
3.  Visualization: Projecting high-dimensional data onto top 2 or 3 principal components reveals structure.
    å¯è§†åŒ–ï¼šå°†é«˜ç»´æ•°æ®æŠ•å½±åˆ°å‰ 2 ä¸ªæˆ– 3 ä¸ªä¸»æˆåˆ†ä¸Šå¯ä»¥æ­ç¤ºç»“æ„ã€‚
4.  Compression: PCA is used in image and signal compression.
    å‹ç¼©ï¼šPCA ç”¨äºå›¾åƒå’Œä¿¡å·å‹ç¼©ã€‚

### Connection to the Spectral Theorem
ä¸è°±å®šç†çš„è”ç³»

The covariance matrix $\Sigma$ is always symmetric and positive semidefinite. Hence by the spectral theorem, it has an orthonormal basis of eigenvectors and nonnegative real eigenvalues. PCA is nothing more than re-expressing data in this eigenbasis.
åæ–¹å·®çŸ©é˜µ $\Sigma$ å§‹ç»ˆæ˜¯å¯¹ç§°çš„ï¼Œä¸”ä¸ºåŠæ­£å®šçŸ©é˜µã€‚å› æ­¤ï¼Œæ ¹æ®è°±å®šç†ï¼Œå®ƒæœ‰ä¸€ä¸ªç”±ç‰¹å¾å‘é‡å’Œéè´Ÿå®ç‰¹å¾å€¼ç»„æˆçš„æ­£äº¤åŸºã€‚PCA åªä¸è¿‡æ˜¯åœ¨è¿™ä¸ªç‰¹å¾åŸºä¸Šé‡æ–°è¡¨è¾¾æ•°æ®ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

PCA demonstrates how abstract linear algebra directly powers modern applications. Eigenvalues and eigenvectors give a practical method for simplifying data, revealing patterns, and reducing complexity. It is one of the most important algorithms derived from the spectral theorem.
PCA å±•ç¤ºäº†æŠ½è±¡çº¿æ€§ä»£æ•°å¦‚ä½•ç›´æ¥é©±åŠ¨ç°ä»£åº”ç”¨ã€‚ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æä¾›äº†ä¸€ç§ç®€åŒ–æ•°æ®ã€æ­ç¤ºæ¨¡å¼å’Œé™ä½å¤æ‚æ€§çš„å®ç”¨æ–¹æ³•ã€‚å®ƒæ˜¯ä»è°±å®šç†ä¸­æ¨å¯¼å‡ºçš„æœ€é‡è¦çš„ç®—æ³•ä¹‹ä¸€ã€‚

### Exercises 9.4
ç»ƒä¹  9.4

1.  Show that the covariance matrix is symmetric and positive semidefinite.
    è¯æ˜åæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„å’ŒåŠæ­£å®šçš„ã€‚
2.  Compute the covariance matrix of the dataset $(1,2), (2,3), (3,4)$, and find its eigenvalues and eigenvectors.
    è®¡ç®—æ•°æ®é›† $(1,2), (2,3), (3,4)$ çš„åæ–¹å·®çŸ©é˜µï¼Œå¹¶æ‰¾åˆ°å…¶ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚
3.  Explain why the first principal component captures the maximum variance.
    è§£é‡Šä¸ºä»€ä¹ˆç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ•è·æœ€å¤§æ–¹å·®ã€‚
4.  In image compression, explain how PCA can reduce storage by keeping only the top $k$ principal components.
    åœ¨å›¾åƒå‹ç¼©ä¸­ï¼Œè§£é‡Š PCA å¦‚ä½•é€šè¿‡ä»…ä¿ç•™å‰ $k$ ä¸ªä¸»æˆåˆ†æ¥å‡å°‘å­˜å‚¨ã€‚
5.  Prove that the sum of the eigenvalues of the covariance matrix equals the total variance of the dataset.
    è¯æ˜åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼ä¹‹å’Œç­‰äºæ•°æ®é›†çš„æ€»æ–¹å·®ã€‚