
https://github.com/chizkidd/chizkidd.github.io/blob/main/_posts/2025-05-30-machine-learning-key-math-eqns.md


The Most Important Machine Learning Equations: A Comprehensive Guide

æœ€é‡è¦çš„æœºå™¨å­¦ä¹ æ–¹ç¨‹ï¼šå…¨é¢æŒ‡å—

A comprehensive guide to the most critical machine learning mathematical equations, from probability theory to advanced concepts like diffusion models and attention mechanisms. It includes theoretical explanations and practical Python implementations.  

ä¸€éƒ¨å…¨é¢æŒ‡å—ï¼Œæ¶µç›–æœºå™¨å­¦ä¹ ä¸­æœ€å…³é”®çš„æ•°å­¦æ–¹ç¨‹ï¼Œä»æ¦‚ç‡è®ºåˆ°æ‰©æ•£æ¨¡å‹å’Œæ³¨æ„åŠ›æœºåˆ¶ç­‰é«˜çº§æ¦‚å¿µã€‚å†…å®¹åŒ…å«ç†è®ºé˜é‡Šä¸å®ç”¨çš„Pythonå®ç°æ–¹æ¡ˆã€‚


## Motivation
åŠ¨æœº

Machine learning (ML) is a powerful field driven by mathematics. Whether you're building models, optimizing algorithms, or simply trying to understand how ML works under the hood, mastering the core equations is essential. This blog post is designed to be your go-to resource, covering the most critical and "mind-breaking" ML equationsâ€”enough to grasp most of the core math behind ML. Each section includes theoretical insights, the equations themselves, and practical implementations in Python, so you can see the math in action.

æœºå™¨å­¦ä¹  (ML) æ˜¯ä¸€ä¸ªç”±æ•°å­¦é©±åŠ¨çš„å¼ºå¤§é¢†åŸŸã€‚æ— è®ºæ‚¨æ˜¯æ„å»ºæ¨¡å‹ã€ä¼˜åŒ–ç®—æ³•ï¼Œè¿˜æ˜¯ä»…ä»…æƒ³äº†è§£ ML çš„åº•å±‚å·¥ä½œåŸç†ï¼ŒæŒæ¡æ ¸å¿ƒæ–¹ç¨‹å¼éƒ½è‡³å…³é‡è¦ã€‚è¿™ç¯‡åšæ–‡æ—¨åœ¨æˆä¸ºæ‚¨çš„é¦–é€‰èµ„æºï¼Œæ¶µç›–æœ€å…³é”®ã€æœ€â€œä»¤äººè´¹è§£â€çš„ ML æ–¹ç¨‹å¼ï¼Œè¶³ä»¥å¸®åŠ©æ‚¨æŒæ¡ ML èƒŒåçš„å¤§éƒ¨åˆ†æ ¸å¿ƒæ•°å­¦åŸç†ã€‚æ¯ä¸ªéƒ¨åˆ†éƒ½åŒ…å«ç†è®ºè§è§£ã€æ–¹ç¨‹å¼æœ¬èº«ä»¥åŠ Python ä¸­çš„å®é™…å®ç°ï¼Œè®©æ‚¨èƒ½å¤Ÿäº²çœ¼è§è¯æ•°å­¦çš„è¿ä½œã€‚

This guide is for anyone with a basic background in math and programming who wants to deepen their understanding of ML and is inspired by this [tweet from @goyal\_\_pramod](https://x.com/goyal__pramod/status/1923064911501914216). Let's dive into the equations that power this fascinating field!

æœ¬æŒ‡å—é€‚ç”¨äºä»»ä½•å…·æœ‰æ•°å­¦å’Œç¼–ç¨‹åŸºç¡€ï¼Œå¹¶å¸Œæœ›åŠ æ·±å¯¹æœºå™¨å­¦ä¹ ç†è§£ï¼Œå¹¶å—åˆ° [@goyal\_\_pramod è¿™æ¡æ¨æ–‡](https://x.com/goyal__pramod/status/1923064911501914216)å¯å‘çš„è¯»è€…ã€‚è®©æˆ‘ä»¬æ·±å…¥æ¢ç©¶é©±åŠ¨è¿™ä¸ªè¿·äººé¢†åŸŸçš„æ–¹ç¨‹å¼ï¼

* * *

## Table of Contents
ç›®å½•

*   Introduction    
    ä»‹ç»
    
*   Probability and Information Theory

    æ¦‚ç‡ä¸ä¿¡æ¯è®º
    
    *   Bayes Theorem
        
        è´å¶æ–¯å®šç†
    *   Entropy
        
        ç†µ
    *   Joint and Conditional Probability
        
        è”åˆæ¦‚ç‡å’Œæ¡ä»¶æ¦‚ç‡
    *   Kullback-Leibler Divergence (KLD)
        
        åº“å°”è´å…‹-è±å¸ƒå‹’æ•£åº¦ï¼ˆKLDï¼‰
    *   Cross-Entropy
        
        äº¤å‰ç†µ
*   Linear Algebra
    
    çº¿æ€§ä»£æ•°
    
    *   Linear Transformation
        
        çº¿æ€§å˜æ¢
    *   Eigenvalues and Eigenvectors
        
        ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
    *   Singular Value Decomposition (SVD)
        
        å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰

*   Optimization
    
    ä¼˜åŒ–
    
    *   Gradient Descent
        
        æ¢¯åº¦ä¸‹é™

    *   Backpropagation
        
        åå‘ä¼ æ’­

*   Loss Functions
    
    æŸå¤±å‡½æ•°
    
    *   Mean Squared Error (MSE)
        
        å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰

    *   Cross-Entropy Loss
        
        äº¤å‰ç†µæŸå¤±

*   Advanced ML Concepts
    é«˜çº§æœºå™¨å­¦ä¹ æ¦‚å¿µ
    
    *   Diffusion Process
        
        æ‰©æ•£è¿‡ç¨‹
    *   Convolution Operation
        
        å·ç§¯è¿ç®—        
    *   Softmax Function
        
        Softmax å‡½æ•°
    *   Attention Mechanism
        
        æ³¨æ„åŠ›æœºåˆ¶
*   Conclusion
    
    ç»“è®º
    
*   Further Reading
    
    è¿›ä¸€æ­¥é˜…è¯»
    

* * *

## Introduction
ä»‹ç»

Mathematics is the language of machine learning. From probability to linear algebra, optimization to advanced generative models, equations define how ML algorithms learn from data and make predictions. This blog post compiles the most essential equations, explains their significance, and provides practical examples using Python libraries like NumPy, scikit-learn, TensorFlow, and PyTorch. Whether you're a beginner or an experienced practitioner, this guide will equip you with the tools to understand and apply ML math effectively.

æ•°å­¦æ˜¯æœºå™¨å­¦ä¹ çš„è¯­è¨€ã€‚ä»æ¦‚ç‡åˆ°çº¿æ€§ä»£æ•°ï¼Œä»ä¼˜åŒ–åˆ°é«˜çº§ç”Ÿæˆæ¨¡å‹ï¼Œæ–¹ç¨‹å¼å®šä¹‰äº†æœºå™¨å­¦ä¹ ç®—æ³•å¦‚ä½•ä»æ•°æ®ä¸­å­¦ä¹ å¹¶è¿›è¡Œé¢„æµ‹ã€‚è¿™ç¯‡åšæ–‡æ±‡ç¼–äº†æœ€é‡è¦çš„æ–¹ç¨‹å¼ï¼Œè§£é‡Šäº†å®ƒä»¬çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä½¿ç”¨ NumPyã€scikit-learnã€TensorFlow å’Œ PyTorch ç­‰ Python åº“çš„å®è·µç¤ºä¾‹ã€‚æ— è®ºæ‚¨æ˜¯åˆå­¦è€…è¿˜æ˜¯ç»éªŒä¸°å¯Œçš„ä»ä¸šè€…ï¼Œæœ¬æŒ‡å—éƒ½å°†ä¸ºæ‚¨æä¾›ç†è§£å’Œæœ‰æ•ˆåº”ç”¨æœºå™¨å­¦ä¹ æ•°å­¦çš„å·¥å…·ã€‚

* * *

## Probability and Information Theory

æ¦‚ç‡ä¸ä¿¡æ¯è®º

Probability and information theory provide the foundation for reasoning about uncertainty and measuring differences between distributions.

æ¦‚ç‡å’Œä¿¡æ¯è®ºä¸ºæ¨ç†ä¸ç¡®å®šæ€§å’Œæµ‹é‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚æä¾›äº†åŸºç¡€ã€‚

### Bayes' Theorem
è´å¶æ–¯å®šç†

**Equation:**
æ–¹ç¨‹ï¼š

$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
$$

**Explanation:** Bayes' Theorem describes how to update the probability of a hypothesis ($A$) given new evidence ($B$). Itâ€™s a cornerstone of probabilistic reasoning and is widely used in machine learning for tasks like classification and inference.

**è§£é‡Šï¼š** è´å¶æ–¯å®šç†æè¿°äº†å¦‚ä½•åœ¨ç»™å®šæ–°è¯æ®ï¼ˆ $B$ ï¼‰çš„æƒ…å†µä¸‹æ›´æ–°å‡è®¾ï¼ˆ $A$ ï¼‰çš„æ¦‚ç‡ã€‚å®ƒæ˜¯æ¦‚ç‡æ¨ç†çš„åŸºçŸ³ï¼Œå¹¿æ³›åº”ç”¨äºæœºå™¨å­¦ä¹ ä¸­çš„åˆ†ç±»å’Œæ¨ç†ç­‰ä»»åŠ¡ã€‚

**Practical Use:** Applied in Naive Bayes classifiers, Bayesian networks, and Bayesian optimization.

**å®é™…ç”¨é€”ï¼š** åº”ç”¨äºæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ã€è´å¶æ–¯ç½‘ç»œå’Œè´å¶æ–¯ä¼˜åŒ–ã€‚

**Implementation:**

å®ç°ï¼š

```python
def bayes_theorem(p_d, p_t_given_d, p_t_given_not_d):
    """
    Calculate P(D|T+) using Bayes' Theorem.
    
    Parameters:
    p_d: P(D), probability of having the disease
    p_t_given_d: P(T+|D), probability of testing positive given disease
    p_t_given_not_d: P(T+|D'), probability of testing positive given no disease
    
    Returns:
    P(D|T+), probability of having the disease given a positive test
    """
    p_not_d = 1 - p_d
    p_t = p_t_given_d * p_d + p_t_given_not_d * p_not_d
    p_d_given_t = (p_t_given_d * p_d) / p_t
    return p_d_given_t

# Example usage
p_d = 0.01  # 1% of population has the disease
p_t_given_d = 0.99  # Test is 99% sensitive
p_t_given_not_d = 0.02  # Test has 2% false positive rate
result = bayes_theorem(p_d, p_t_given_d, p_t_given_not_d) 
print(f"P(D|T+) = {result:.4f}")  # Output: P(D|T+) = 0.3333 
```

### Entropy

ç†µ

**Equation:ï¼š**

æ–¹ç¨‹

$$
H(X) = -\sum_{x \in X} P(x) \log P(x)
$$

**Explanation:** Entropy measures the uncertainty or randomness in a probability distribution. It quantifies the amount of information required to describe the distribution and is fundamental in understanding concepts like information gain and decision trees.

**è§£é‡Šï¼š** ç†µè¡¡é‡æ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸ç¡®å®šæ€§æˆ–éšæœºæ€§ã€‚å®ƒé‡åŒ–äº†æè¿°åˆ†å¸ƒæ‰€éœ€çš„ä¿¡æ¯é‡ï¼Œæ˜¯ç†è§£ä¿¡æ¯å¢ç›Šå’Œå†³ç­–æ ‘ç­‰æ¦‚å¿µçš„åŸºç¡€ã€‚

**Practical Use:** Used in decision trees, information gain calculations, and as a basis for other information-theoretic measures.

**å®é™…ç”¨é€”ï¼š** ç”¨äºå†³ç­–æ ‘ã€ä¿¡æ¯å¢ç›Šè®¡ç®—ï¼Œå¹¶ä½œä¸ºå…¶ä»–ä¿¡æ¯ç†è®ºæµ‹é‡çš„åŸºç¡€ã€‚

**Implementation:**

å®ç°ï¼š

```python
import numpy as np

def entropy(p):
    """
    Calculate entropy of a probability distribution.
    
    Parameters:
    p: Probability distribution array
    
    Returns:
    Entropy value
    """
    return -np.sum(p * np.log(p, where=p > 0))

# Example usage
fair_coin = np.array([0.5, 0.5])  # fair coin has the same probability of heads and tails
print(f"Entropy of fair coin: {entropy(fair_coin)}")  # Output: 0.6931471805599453 

biased_coin = np.array([0.9, 0.1])  # biased coin has a higher probability of heads
print(f"Entropy of biased coin: {entropy(biased_coin)}")  # Output: 0.4698716731013394 
```

### Joint and Conditional Probability

è”åˆæ¦‚ç‡å’Œæ¡ä»¶æ¦‚ç‡

**Equations:**

æ–¹ç¨‹å¼ï¼š

*   Joint Probability:

    è”åˆæ¦‚ç‡ï¼š
    
    $$
    P(A, B) = P(A|B) P(B) = P(B|A) P(A)
    $$
    
*   Conditional Probability:

    æ¡ä»¶æ¦‚ç‡ï¼š
    
    $$
    P(A|B) = \frac{P(A, B)}{P(B)}
    $$
    

**Explanation:** Joint probability describes the likelihood of two events occurring together, while conditional probability measures the probability of one event given another. These are the building blocks of Bayesian methods and probabilistic models.

**è§£é‡Šï¼š** è”åˆæ¦‚ç‡æè¿°çš„æ˜¯ä¸¤ä¸ªäº‹ä»¶åŒæ—¶å‘ç”Ÿçš„å¯èƒ½æ€§ï¼Œè€Œæ¡ä»¶æ¦‚ç‡åˆ™è¡¡é‡ä¸€ä¸ªäº‹ä»¶åœ¨å¦ä¸€ä¸ªäº‹ä»¶çš„æ¡ä»¶ä¸‹å‘ç”Ÿçš„æ¦‚ç‡ã€‚å®ƒä»¬æ˜¯è´å¶æ–¯æ–¹æ³•å’Œæ¦‚ç‡æ¨¡å‹çš„åŸºçŸ³ã€‚

**Practical Use:** Used in Naive Bayes classifiers and probabilistic graphical models.

**å®é™…ç”¨é€”ï¼š** ç”¨äºæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨å’Œæ¦‚ç‡å›¾æ¨¡å‹ã€‚

**Implementation:**

å®ç°ï¼š

```python
from sklearn.naive_bayes import GaussianNB
import numpy as np

X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
y = np.array([0, 0, 1, 1])
model = GaussianNB().fit(X, y)
print(model.predict([[2.5, 3.5]]))  # Output: [1]
```

### Kullback-Leibler Divergence (KLD)

åº“å°”è´å…‹-è±å¸ƒå‹’æ•£åº¦ï¼ˆKLDï¼‰

**Equation:**

æ–¹ç¨‹ï¼š

$$
D_{KL}(P \| Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right)
$$

**Explanation:** KLD measures how much one probability distribution $P$ diverges from another $Q$. Itâ€™s asymmetric and foundational in information theory and generative models.

**è§£é‡Šï¼š** KLD è¡¡é‡ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ $P$ ä¸å¦ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ $Q$ çš„åç¦»ç¨‹åº¦ã€‚å®ƒæ˜¯éå¯¹ç§°çš„ï¼Œæ˜¯ä¿¡æ¯è®ºå’Œç”Ÿæˆæ¨¡å‹çš„åŸºç¡€ã€‚

**Practical Use:** Used in variational autoencoders (VAEs) and model evaluation.

**å®é™…ç”¨é€”ï¼š** ç”¨äºå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰å’Œæ¨¡å‹è¯„ä¼°ã€‚

**Implementation:**

å®ç°ï¼š

```python
import numpy as np

P = np.array([0.7, 0.3])
Q = np.array([0.5, 0.5])
kl_div = np.sum(P * np.log(P / Q))
print(f"KL Divergence: {kl_div}")  # Output: 0.08228287850505156
```

### Cross-Entropy

äº¤å‰ç†µ

**Equation:**

æ–¹ç¨‹ï¼š

$$
H(P, Q) = -\sum_{x \in \mathcal{X}} P(x) \log Q(x)
$$

**Explanation:** Cross-entropy quantifies the difference between the true distribution $P$ and the predicted distribution $Q$. Itâ€™s a widely used loss function in classification.

**è§£é‡Šï¼š** äº¤å‰ç†µé‡åŒ–äº†çœŸå®åˆ†å¸ƒ $P$ ä¸é¢„æµ‹åˆ†å¸ƒ $Q$ ä¹‹é—´çš„å·®å¼‚ã€‚å®ƒæ˜¯åˆ†ç±»ä¸­å¹¿æ³›ä½¿ç”¨çš„æŸå¤±å‡½æ•°ã€‚

**Practical Use:** Drives training in logistic regression and neural networks.

**å®é™…ç”¨é€”ï¼š** æ¨åŠ¨é€»è¾‘å›å½’å’Œç¥ç»ç½‘ç»œçš„è®­ç»ƒã€‚

**Implementation:**

å®ç°

```python
import numpy as np

y_true = np.array([1, 0, 1])
y_pred = np.array([0.9, 0.1, 0.8])
cross_entropy = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
print(f"Cross-Entropy: {cross_entropy}")  # Output: 0.164252033486018
```

* * *

## Linear Algebra

çº¿æ€§ä»£æ•°

Linear algebra powers the transformations and structures in ML models.

çº¿æ€§ä»£æ•°ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„è½¬æ¢å’Œç»“æ„æä¾›æ”¯æŒã€‚

### Linear Transformation

çº¿æ€§å˜æ¢

**Equation:**

æ–¹ç¨‹ï¼š

$$
y = Ax + b \quad \text{where } A \in \mathbb{R}^{m \times n}, x \in \mathbb{R}^n, y \in \mathbb{R}^m, b \in \mathbb{R}^m
$$

**Explanation:** This equation represents a linear mapping of input $x$ to output $y$ via matrix $A$ and bias $b$. Itâ€™s the core operation in neural network layers.

**è§£é‡Šï¼š** è¯¥æ–¹ç¨‹è¡¨ç¤ºè¾“å…¥ $x$ é€šè¿‡çŸ©é˜µ $A$ å’Œåå·® $b$ åˆ°è¾“å‡º $y$ çš„çº¿æ€§æ˜ å°„ã€‚å®ƒæ˜¯ç¥ç»ç½‘ç»œå±‚ä¸­çš„æ ¸å¿ƒæ“ä½œã€‚

**Practical Use:** Foundational for linear regression and neural networks.

**å®é™…ç”¨é€”ï¼š** çº¿æ€§å›å½’å’Œç¥ç»ç½‘ç»œçš„åŸºç¡€ã€‚

**Implementation:**

å®ç°ï¼š

```python
import numpy as np

A = np.array([[2, 1], [1, 3]])
x = np.array([1, 2])
b = np.array([0, 1])
y = A @ x + b
print(y)  # Output: [4 7]
```

### Eigenvalues and Eigenvectors

ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡

**Equation:**

æ–¹ç¨‹ï¼š

$$
Av = \lambda v \quad \text{where } \lambda \in \mathbb{R}, v \in \mathbb{R}^n, v \neq 0
$$

**Explanation:** Eigenvalues $\lambda$ and eigenvectors $v$ describe how a matrix $A$ scales and rotates space, crucial for understanding data variance.

**è§£é‡Šï¼š** ç‰¹å¾å€¼ $\lambda$ å’Œç‰¹å¾å‘é‡ $v$ æè¿°çŸ©é˜µ $A$ å¦‚ä½•ç¼©æ”¾å’Œæ—‹è½¬ç©ºé—´ï¼Œè¿™å¯¹äºç†è§£æ•°æ®æ–¹å·®è‡³å…³é‡è¦ã€‚

**Practical Use:** Used in Principal Component Analysis (PCA).

**å®é™…ç”¨é€”ï¼š** ç”¨äºä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã€‚

**Implementation:**

å®ç°ï¼š

```python
import numpy as np

A = np.array([[4, 2], [1, 3]])
eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"Eigenvalues: {eigenvalues}")
print(f"Eigenvectors:\n{eigenvectors}")
```

### Singular Value Decomposition (SVD)

å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰

**Equation:ï¼š**

æ–¹ç¨‹

$$
A = U \Sigma V^T
$$

**Explanation:** SVD breaks down a matrix $A$ into orthogonal matrices $U$ and $V$ and a diagonal matrix $\Sigma$ of singular values. It reveals the intrinsic structure of data.

**è§£é‡Šï¼š** SVD å°†çŸ©é˜µ $A$ åˆ†è§£ä¸ºæ­£äº¤çŸ©é˜µ $U$ å’Œ $V$ ä»¥åŠå¥‡å¼‚å€¼çš„å¯¹è§’çŸ©é˜µ $\Sigma$ ã€‚å®ƒæ­ç¤ºäº†æ•°æ®çš„å†…åœ¨ç»“æ„ã€‚

**Practical Use:** Applied in dimensionality reduction and recommendation systems.

**å®é™…ç”¨é€”ï¼š** åº”ç”¨äºé™ç»´å’Œæ¨èç³»ç»Ÿã€‚

**Implementation:**

æ‰§è¡Œï¼š

```python
import numpy as np

A = np.array([[1, 2], [3, 4], [5, 6]])
U, S, Vt = np.linalg.svd(A)
print(f"U:\n{U}\nS: {S}\nVt:\n{Vt}")
```

* * *

## Optimization

ä¼˜åŒ–

Optimization is how ML models learn from data.

ä¼˜åŒ–æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ çš„æ–¹å¼ã€‚

### Gradient Descent

æ¢¯åº¦ä¸‹é™

**Equation:**

æ–¹ç¨‹ï¼š

$$
\theta_{t+1} = \theta_t - \eta \nabla_{\theta} L(\theta)
$$

**Explanation:** Gradient descent updates parameters $\theta$ by moving opposite to the gradient of the loss function $L$, scaled by learning rate $\eta$.

**è§£é‡Šï¼š** æ¢¯åº¦ä¸‹é™é€šè¿‡ä¸æŸå¤±å‡½æ•° $L$ çš„æ¢¯åº¦ç›¸åçš„æ–¹å‘ç§»åŠ¨æ¥æ›´æ–°å‚æ•° $\theta$ ï¼Œå¹¶é€šè¿‡å­¦ä¹ ç‡ $\eta$ è¿›è¡Œç¼©æ”¾ã€‚

**Practical Use:** The backbone of training most ML models.

**å®é™…ç”¨é€”ï¼š** è®­ç»ƒå¤§å¤šæ•° ML æ¨¡å‹çš„æ”¯æŸ±ã€‚

**Implementation:
æ‰§è¡Œï¼š**

```python
import numpy as np

def gradient_descent(X, y, lr=0.01, epochs=1000):
    m, n = X.shape
    theta = np.zeros(n)
    for _ in range(epochs):
        gradient = (1/m) * X.T @ (X @ theta - y)
        theta -= lr * gradient
    return theta

X = np.array([[1, 1], [1, 2], [1, 3]])
y = np.array([1, 2, 3])
theta = gradient_descent(X, y)
print(theta)  # Output: ~[0., 1.]
```

### Backpropagation

åå‘ä¼ æ’­

**Equation:**

æ–¹ç¨‹ï¼š

$$
\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial a_j} \cdot \frac{\partial a_j}{\partial z_j} \cdot \frac{\partial z_j}{\partial w_{ij}}
$$

**Explanation:** Backpropagation applies the chain rule to compute gradients of the loss $L$ with respect to weights $w_{ij}$ in neural networks.

**è§£é‡Šï¼š** åå‘ä¼ æ’­åº”ç”¨é“¾å¼æ³•åˆ™æ¥è®¡ç®—æŸå¤± $L$ ç›¸å¯¹äºæƒé‡ ğ‘¤ çš„æ¢¯åº¦ ğ‘– ğ‘— w ij â€‹ åœ¨ç¥ç»ç½‘ç»œä¸­ã€‚

**Practical Use:** Enables efficient training of deep networks.

**å®é™…ç”¨é€”ï¼š** å®ç°æ·±åº¦ç½‘ç»œçš„æœ‰æ•ˆè®­ç»ƒã€‚

**Implementation:**

å®ç°ï¼š

```python
import torch
import torch.nn as nn

model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())
loss_fn = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

X = torch.tensor([[0., 0.], [1., 1.]], dtype=torch.float32)
y = torch.tensor([[0.], [1.]], dtype=torch.float32)

optimizer.zero_grad()
output = model(X)
loss = loss_fn(output, y)
loss.backward()
optimizer.step()
print(f"Loss: {loss.item()}")
```

* * *

## Loss Functions

æŸå¤±å‡½æ•°

Loss functions measure model performance and guide optimization.

æŸå¤±å‡½æ•°è¡¡é‡æ¨¡å‹æ€§èƒ½å¹¶æŒ‡å¯¼ä¼˜åŒ–ã€‚

### Mean Squared Error (MSE)

å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰

**Equation:**

æ–¹ç¨‹ï¼š

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

**Explanation:** MSE calculates the average squared difference between true $y_i$ and predicted $\hat{y}_i$ values, penalizing larger errors more heavily.
**è§£é‡Šï¼š** MSE è®¡ç®—çœŸå® ğ‘¦ ä¹‹é—´çš„å¹³å‡å¹³æ–¹å·® ğ‘– y i â€‹ å¹¶é¢„æµ‹ğ‘¦ ^ ğ‘– y ^ â€‹ i â€‹ å€¼ï¼Œå¯¹è¾ƒå¤§çš„é”™è¯¯è¿›è¡Œæ›´ä¸¥å‰çš„æƒ©ç½šã€‚

**Practical Use:** Common in regression tasks.

**å®é™…ç”¨é€”ï¼š** åœ¨å›å½’ä»»åŠ¡ä¸­å¾ˆå¸¸è§ã€‚

**Implementation:**

å®ç°ï¼š

```python
import numpy as np

y_true = np.array([1, 2, 3])
y_pred = np.array([1.1, 1.9, 3.2])
mse = np.mean((y_true - y_pred)**2)
print(f"MSE: {mse}")  # Output: 0.01
```

### Cross-Entropy Loss

äº¤å‰ç†µæŸå¤±

(See Cross-Entropy above for details.)

æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ä¸Šé¢çš„[äº¤å‰ç†µ](#cross-entropy) ã€‚ï¼‰

* * *

## Advanced ML Concepts

é«˜çº§æœºå™¨å­¦ä¹ æ¦‚å¿µ

These equations power cutting-edge ML techniques.

è¿™äº›æ–¹ç¨‹ä¸ºå°–ç«¯çš„æœºå™¨å­¦ä¹ æŠ€æœ¯æä¾›äº†åŠ¨åŠ›ã€‚

### Diffusion Process

æ‰©æ•£è¿‡ç¨‹

**Equation:**

æ–¹ç¨‹ï¼š

$$
x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - \alpha_t} \epsilon \quad \text{where} \quad \epsilon \sim \mathcal{N}(0, I)
$$

**Explanation:** This describes a forward diffusion process where data $x_0$ is gradually noised over time $t$, a key idea in diffusion models.

**è§£é‡Šï¼š** è¿™æè¿°äº†ä¸€ä¸ªå‰å‘æ‰©æ•£è¿‡ç¨‹ï¼Œå…¶ä¸­æ•°æ®ğ‘¥ 0 x 0 â€‹ éšç€æ—¶é—´çš„æ¨ç§»é€æ¸äº§ç”Ÿå™ªå£° $t$ ï¼Œè¿™æ˜¯æ‰©æ•£æ¨¡å‹ä¸­çš„ä¸€ä¸ªå…³é”®æ€æƒ³ã€‚

**Practical Use:** Used in generative AI like image synthesis.

**å®é™…ç”¨é€”ï¼š** ç”¨äºå›¾åƒåˆæˆç­‰ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ã€‚

**Implementation:**

å®ç°ï¼š

```python
import torch

x_0 = torch.tensor([1.0])
alpha_t = 0.9
noise = torch.randn_like(x_0)
x_t = torch.sqrt(torch.tensor(alpha_t)) * x_0 + torch.sqrt(torch.tensor(1 - alpha_t)) * noise
print(f"x_t: {x_t}")
```

* * *

### Convolution Operation

å·ç§¯è¿ç®—

**Equation:**

æ–¹ç¨‹ï¼š

$$
(f * g)(t) = \int f(\tau) g(t - \tau) \, d\tau
$$

**Explanation:** Convolution combines two functions by sliding one over the other, extracting features in data like images.

**è§£é‡Šï¼š** å·ç§¯é€šè¿‡å°†ä¸€ä¸ªå‡½æ•°æ»‘åŠ¨åˆ°å¦ä¸€ä¸ªå‡½æ•°ä¸Šæ¥ç»„åˆä¸¤ä¸ªå‡½æ•°ï¼Œä»è€Œæå–å›¾åƒç­‰æ•°æ®ä¸­çš„ç‰¹å¾ã€‚

**Practical Use:** Core to convolutional neural networks (CNNs).

**å®é™…ç”¨é€”ï¼š** å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ ¸å¿ƒã€‚

**Implementation:**

æ‰§è¡Œï¼š

```python
import torch
import torch.nn as nn

conv = nn.Conv2d(1, 1, kernel_size=3)
image = torch.randn(1, 1, 28, 28)
output = conv(image)
print(output.shape)  # Output: torch.Size([1, 1, 26, 26])
```

* * *

### Softmax Function

Softmax å‡½æ•°

**Equation:**

æ–¹ç¨‹ï¼š

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

**Explanation:** Softmax converts raw scores $z_i$ into probabilities, summing to 1, ideal for multi-class classification.

**è§£é‡Šï¼š** Softmax å°†åŸå§‹åˆ†æ•°ğ‘§ ğ‘– z i â€‹ è½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œæ€»å’Œä¸º 1ï¼Œéå¸¸é€‚åˆå¤šç±»åˆ†ç±»ã€‚

**Practical Use:** Used in neural network outputs.

**å®é™…ç”¨é€”ï¼š** ç”¨äºç¥ç»ç½‘ç»œè¾“å‡ºã€‚

**Implementation:**

å®ç°ï¼š

```python
import numpy as np

z = np.array([1.0, 2.0, 3.0])
softmax = np.exp(z) / np.sum(np.exp(z))
print(f"Softmax: {softmax}")  # Output: [0.09003057 0.24472847 0.66524096]
```

* * *

### Attention Mechanism

æ³¨æ„åŠ›æœºåˆ¶

**Equation:**

æ–¹ç¨‹ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
$$

**Explanation:** Attention computes a weighted sum of values $V$ based on the similarity between queries $Q$ and keys $K$, scaled by $\sqrt{d_k}$.

**è§£é‡Šï¼š** æ³¨æ„åŠ›æœºåˆ¶æ ¹æ®æŸ¥è¯¢ $Q$ å’Œé”® $K$ ä¹‹é—´çš„ç›¸ä¼¼æ€§è®¡ç®—å€¼ $V$ çš„åŠ æƒå’Œï¼Œå¹¶ä»¥ ğ‘‘ ä¸ºæ¯”ä¾‹ ğ‘˜ d k â€‹ â€‹ .

**Practical Use:** Powers transformers in NLP and beyond.

**å®é™…ç”¨é€”ï¼š** ä¸º NLP åŠå…¶ä»–é¢†åŸŸçš„å˜å‹å™¨æä¾›åŠ¨åŠ›ã€‚

**Implementation:**

å®ç°ï¼š

```python
import torch

def attention(Q, K, V):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    attn = torch.softmax(scores, dim=-1)
    return torch.matmul(attn, V)

Q = torch.tensor([[1., 0.], [0., 1.]])
K = torch.tensor([[1., 1.], [1., 0.]])
V = torch.tensor([[0., 1.], [1., 0.]])
output = attention(Q, K, V)
print(output)
```

* * *

## Conclusion

ç»“è®º

This blog post has explored the most critical equations in machine learning, from foundational probability and linear algebra to advanced concepts like diffusion and attention. With theoretical explanations, practical implementations, and visualizations, you now have a comprehensive resource to understand and apply ML math. Point anyone asking about core ML math hereâ€”they'll learn 95% of what they need in one place!

è¿™ç¯‡åšæ–‡æ¢è®¨äº†æœºå™¨å­¦ä¹ ä¸­æœ€å…³é”®çš„æ–¹ç¨‹å¼ï¼Œä»åŸºç¡€æ¦‚ç‡å’Œçº¿æ€§ä»£æ•°åˆ°æ‰©æ•£å’Œæ³¨æ„åŠ›ç­‰é«˜çº§æ¦‚å¿µã€‚é€šè¿‡ç†è®ºè§£é‡Šã€å®é™…å®ç°å’Œå¯è§†åŒ–ï¼Œæ‚¨ç°åœ¨æ‹¥æœ‰äº†ç†è§£å’Œåº”ç”¨æœºå™¨å­¦ä¹ æ•°å­¦çš„å…¨é¢èµ„æºã€‚å¦‚æœæœ‰äººæƒ³äº†è§£æœºå™¨å­¦ä¹ æ ¸å¿ƒæ•°å­¦ï¼Œå¯ä»¥å‚è€ƒè¿™é‡Œâ€”â€”ä»–ä»¬å°†åœ¨ä¸€ä¸ªåœ°æ–¹å­¦åˆ° 95% æ‰€éœ€çš„çŸ¥è¯†ï¼

* * *

## Further Reading

è¿›ä¸€æ­¥é˜…è¯»

*   *Pattern Recognition and Machine Learning* by Christopher Bishop

    *æ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ * ï¼ˆä½œè€…ï¼šChristopher Bishopï¼‰
*   *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville

    *ã€Šæ·±åº¦å­¦ä¹ ã€‹* ï¼ˆä½œè€…ï¼šIan Goodfellowã€Yoshua Bengio å’Œ Aaron Courvilleï¼‰
*   [Stanford CS229: Machine Learning](https://cs229.stanford.edu/)

    æ–¯å¦ç¦ CS229ï¼šæœºå™¨å­¦ä¹ 
*   [PyTorch Tutorials](https://pytorch.org/tutorials/)

    PyTorch æ•™ç¨‹