# Chapter 10. Linear Algebra in Practice
ç¬¬ 10 ç«  çº¿æ€§ä»£æ•°å®è·µ

## 10.1 Computer Graphics (Rotations, Projections)
10.1 è®¡ç®—æœºå›¾å½¢å­¦ï¼ˆæ—‹è½¬ã€æŠ•å½±ï¼‰

Linear algebra is the language of modern computer graphics. Every image rendered on a screen, every 3D model rotated or projected, is ultimately the result of applying matrices to vectors. Rotations, reflections, scalings, and projections are all linear transformations, making matrices the natural tool for manipulating geometry.
çº¿æ€§ä»£æ•°æ˜¯ç°ä»£è®¡ç®—æœºå›¾å½¢å­¦çš„è¯­è¨€ã€‚å±å¹•ä¸Šæ¸²æŸ“çš„æ¯ä¸€å¹…å›¾åƒï¼Œä»¥åŠæ—‹è½¬æˆ–æŠ•å½±çš„æ¯ä¸€ä¸ª 3D æ¨¡å‹ï¼Œæœ€ç»ˆéƒ½æ˜¯å°†çŸ©é˜µåº”ç”¨äºå‘é‡çš„ç»“æœã€‚æ—‹è½¬ã€åå°„ã€ç¼©æ”¾å’ŒæŠ•å½±éƒ½æ˜¯çº¿æ€§å˜æ¢ï¼Œè¿™ä½¿å¾—çŸ©é˜µæˆä¸ºå¤„ç†å‡ ä½•å›¾å½¢çš„å¤©ç„¶å·¥å…·ã€‚

### Rotations in 2D
äºŒç»´æ—‹è½¬

A counterclockwise rotation by an angle $\theta$ in the plane is represented by
åœ¨å¹³é¢ä¸Šé€†æ—¶é’ˆæ—‹è½¬è§’åº¦ $\theta$ è¡¨ç¤ºä¸º

$$
R_\theta =\begin{bmatrix}\cos\theta & -\sin\theta \\\sin\theta & \cos\theta\end{bmatrix}.
$$

For any vector $\mathbf{v} \in \mathbb{R}^2$, the rotated vector is
å¯¹äºä»»æ„å‘é‡ $\mathbf{v} \in \mathbb{R}^2$ ï¼Œæ—‹è½¬åçš„å‘é‡ä¸º

$$
\mathbf{v}' = R_\theta \mathbf{v}.
$$

This preserves lengths and angles, since $R_\theta$ is orthogonal with determinant 1.
è¿™ä¿ç•™äº†é•¿åº¦å’Œè§’åº¦ï¼Œå› ä¸ºğ‘… ğœƒ R Î¸ â€‹ ä¸è¡Œåˆ—å¼ 1 æ­£äº¤ã€‚

### Rotations in 3D
3D æ—‹è½¬

In three dimensions, rotations are represented by $3 \\times 3 orthogonal matrices with determinant \\1$. For example, arotation about the $z$-axis is
åœ¨ä¸‰ç»´ç©ºé—´ä¸­ï¼Œæ—‹è½¬ç”±$3 \\times 3 æ­£äº¤çŸ©é˜µè¡¨ç¤ºï¼Œå…¶è¡Œåˆ—å¼ä¸º \\ 1 $. For example, arotation about the $ z$è½´ä¸º

$$
R_z(\theta) =\begin{bmatrix}\cos\theta & -\sin\theta & 0 \\\sin\theta & \cos\theta & 0 \\0 & 0 & 1\end{bmatrix}.
$$

Similar formulas exist for rotations about the $x$\- and $y$\-axes.
å¯¹äºç»• $x$ è½´å’Œ $y$ è½´çš„æ—‹è½¬ä¹Ÿå­˜åœ¨ç±»ä¼¼çš„å…¬å¼ã€‚

More general 3D rotations can be described by axisâ€“angle representation or quaternions, but the underlying idea is still linear transformations represented by matrices.
æ›´ä¸€èˆ¬çš„ 3D æ—‹è½¬å¯ä»¥ç”¨è½´è§’è¡¨ç¤ºæˆ–å››å…ƒæ•°æ¥æè¿°ï¼Œä½†å…¶åŸºæœ¬æ€æƒ³ä»ç„¶æ˜¯çŸ©é˜µè¡¨ç¤ºçš„çº¿æ€§å˜æ¢ã€‚

### Projections
é¢„æµ‹

To display 3D objects on a 2D screen, we use projections:
ä¸ºäº†åœ¨ 2D å±å¹•ä¸Šæ˜¾ç¤º 3D å¯¹è±¡ï¼Œæˆ‘ä»¬ä½¿ç”¨æŠ•å½±ï¼š

1.  Orthogonal projection: drops the $z$\-coordinate, mapping $(x,y,z) \mapsto (x,y)$.
    æ­£äº¤æŠ•å½±ï¼šåˆ é™¤ $z$ åæ ‡ï¼Œæ˜ å°„ $(x,y,z) \mapsto (x,y)$ ã€‚
    
    $$
    P = \begin{bmatrix}1 & 0 & 0 \\0 & 1 & 0\end{bmatrix}.
    $$
    
2.  Perspective projection: mimics the effect of a camera. A point $(x,y,z)$ projects to
    é€è§†æŠ•å½±ï¼šæ¨¡æ‹Ÿç›¸æœºçš„æ•ˆæœã€‚ç‚¹ $(x,y,z)$ æŠ•å½±åˆ°
    
    $$
    \left(\frac{x}{z}, \frac{y}{z}\right),
    $$
    
    capturing how distant objects appear smaller.
    æ•æ‰è¿œå¤„ç‰©ä½“å¦‚ä½•æ˜¾å¾—æ›´å°ã€‚
    

These operations are linear (orthogonal projection) or nearly linear (perspective projection becomes linear in homogeneous coordinates).
è¿™äº›æ“ä½œæ˜¯çº¿æ€§çš„ï¼ˆæ­£äº¤æŠ•å½±ï¼‰æˆ–è¿‘ä¼¼çº¿æ€§çš„ï¼ˆé€è§†æŠ•å½±åœ¨é½æ¬¡åæ ‡ä¸­å˜ä¸ºçº¿æ€§ï¼‰ã€‚

### Homogeneous Coordinates
é½æ¬¡åæ ‡

To unify translations and projections with linear transformations, computer graphics uses homogeneous coordinates. A 3D point $(x,y,z)$ is represented as a 4D vector $(x,y,z,1)$. Transformations are then 4Ã—4 matrices, which can represent rotations, scalings, and translations in a single framework.
ä¸ºäº†å°†å¹³ç§»å’ŒæŠ•å½±ä¸çº¿æ€§å˜æ¢ç»Ÿä¸€èµ·æ¥ï¼Œè®¡ç®—æœºå›¾å½¢å­¦ä½¿ç”¨é½æ¬¡åæ ‡ã€‚3D ç‚¹ $(x,y,z)$ è¡¨ç¤ºä¸ºå››ç»´å‘é‡ $(x,y,z,1)$ ã€‚å˜æ¢åˆ™è¡¨ç¤ºä¸ºçŸ©é˜µ 4Ã—4 ï¼Œå¯ä»¥åœ¨å•ä¸ªæ¡†æ¶ä¸­è¡¨ç¤ºæ—‹è½¬ã€ç¼©æ”¾å’Œå¹³ç§»ã€‚

Example: Translation by $(a,b,c)$:
ä¾‹å¦‚ï¼š $(a,b,c)$ ç¿»è¯‘ï¼š

$$
T = \begin{bmatrix}1 & 0 & 0 & a \\0 & 1 & 0 & b \\0 & 0 & 1 & c \\0 & 0 & 0 & 1\end{bmatrix}.
$$

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   Rotations preserve shape and size, only changing orientation.
    æ—‹è½¬ä¿æŒå½¢çŠ¶å’Œå¤§å°ï¼Œä»…æ”¹å˜æ–¹å‘ã€‚
*   Projections reduce dimension: from 3D world space to 2D screen space.
    æŠ•å½±å‡å°‘ç»´åº¦ï¼šä» 3D ä¸–ç•Œç©ºé—´åˆ° 2D å±å¹•ç©ºé—´ã€‚
*   Homogeneous coordinates allow us to combine multiple transformations (rotation + translation + projection) into a single matrix multiplication.
    é½æ¬¡åæ ‡å…è®¸æˆ‘ä»¬å°†å¤šä¸ªå˜æ¢ï¼ˆæ—‹è½¬+å¹³ç§»+æŠ•å½±ï¼‰ç»„åˆæˆå•ä¸ªçŸ©é˜µä¹˜æ³•ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Linear algebra enables all real-time graphics: video games, simulations, CAD software, and movie effects. By chaining simple matrix operations, complex transformations are applied efficiently to millions of points per second.
çº¿æ€§ä»£æ•°æ”¯æŒæ‰€æœ‰å®æ—¶å›¾å½¢ï¼šè§†é¢‘æ¸¸æˆã€æ¨¡æ‹Ÿã€CAD è½¯ä»¶å’Œç”µå½±ç‰¹æ•ˆã€‚é€šè¿‡é“¾æ¥ç®€å•çš„çŸ©é˜µè¿ç®—ï¼Œå¤æ‚çš„å˜æ¢å¯ä»¥é«˜æ•ˆåœ°åº”ç”¨äºæ¯ç§’æ•°ç™¾ä¸‡ä¸ªç‚¹ã€‚

### Exercises 10.1
ç»ƒä¹ 10.1

1.  Write the rotation matrix for a 90Â° counterclockwise rotation in $\mathbb{R}^2$. Apply it to $(1,0)$.
    åœ¨ $\mathbb{R}^2$ ä¸­å†™å‡ºé€†æ—¶é’ˆæ—‹è½¬ 90Â° çš„æ—‹è½¬çŸ©é˜µã€‚å°†å…¶åº”ç”¨åˆ° $(1,0)$ ã€‚
2.  Rotate the point $(1,1,0)$ about the $z$\-axis by 180Â°.
    å°†ç‚¹ $(1,1,0)$ ç»• $z$ è½´æ—‹è½¬ 180Â°ã€‚
3.  Show that the determinant of any 2D or 3D rotation matrix is 1.
    è¯æ˜ä»»ä½•äºŒç»´æˆ–ä¸‰ç»´æ—‹è½¬çŸ©é˜µçš„è¡Œåˆ—å¼ä¸º 1ã€‚
4.  Derive the orthogonal projection matrix from $\mathbb{R}^3$ to the $xy$\-plane.
    æ¨å¯¼ä» $\mathbb{R}^3$ åˆ° $xy$ å¹³é¢çš„æ­£äº¤æŠ•å½±çŸ©é˜µã€‚
5.  Explain how homogeneous coordinates allow translations to be represented as matrix multiplications.
    è§£é‡Šé½æ¬¡åæ ‡å¦‚ä½•å…è®¸å¹³ç§»è¡¨ç¤ºä¸ºçŸ©é˜µä¹˜æ³•ã€‚

## 10.2 Data Science (Dimensionality Reduction, Least Squares)
10.2 æ•°æ®ç§‘å­¦ï¼ˆé™ç»´ã€æœ€å°äºŒä¹˜ï¼‰

Linear algebra provides the foundation for many data science techniques. Two of the most important are dimensionality reduction, where high-dimensional datasets are compressed while preserving essential information, and the least squares method, which underlies regression and model fitting.
çº¿æ€§ä»£æ•°ä¸ºè®¸å¤šæ•°æ®ç§‘å­¦æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚å…¶ä¸­æœ€é‡è¦çš„ä¸¤ä¸ªæŠ€æœ¯æ˜¯é™ç»´ï¼ˆåœ¨ä¿ç•™åŸºæœ¬ä¿¡æ¯çš„åŒæ—¶å‹ç¼©é«˜ç»´æ•°æ®é›†ï¼‰å’Œæœ€å°äºŒä¹˜æ³•ï¼ˆå›å½’å’Œæ¨¡å‹æ‹Ÿåˆçš„åŸºç¡€ï¼‰ã€‚

### Dimensionality Reduction
é™ç»´

High-dimensional data often contains redundancy: many features are correlated, meaning the data essentially lies near a lower-dimensional subspace. Dimensionality reduction identifies these subspaces.
é«˜ç»´æ•°æ®é€šå¸¸åŒ…å«å†—ä½™ï¼šè®¸å¤šç‰¹å¾ç›¸äº’å…³è”ï¼Œè¿™æ„å‘³ç€æ•°æ®æœ¬è´¨ä¸Šä½äºä½ç»´å­ç©ºé—´é™„è¿‘ã€‚é™ç»´å¯ä»¥è¯†åˆ«è¿™äº›å­ç©ºé—´ã€‚

*   PCA (Principal Component Analysis): As introduced earlier, PCA diagonalizes the covariance matrix of the data.
    PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ï¼šå¦‚å‰æ‰€è¿°ï¼ŒPCA å°†æ•°æ®çš„åæ–¹å·®çŸ©é˜µå¯¹è§’åŒ–ã€‚
    
    *   Eigenvectors (principal components) define orthogonal directions of maximum variance.
        ç‰¹å¾å‘é‡ï¼ˆä¸»æˆåˆ†ï¼‰å®šä¹‰æœ€å¤§æ–¹å·®çš„æ­£äº¤æ–¹å‘ã€‚
    *   Eigenvalues measure how much variance lies along each direction.
        ç‰¹å¾å€¼è¡¡é‡æ¯ä¸ªæ–¹å‘ä¸Šçš„æ–¹å·®ã€‚
    *   Keeping only the top $k$ components reduces data from $n$\-dimensional space to $k$\-dimensional space while retaining most variability.
        ä»…ä¿ç•™å‰ $k$ ä¸ªæˆåˆ†å¯å°†æ•°æ®ä» $n$ ç»´ç©ºé—´å‡å°‘åˆ° $k$ ç»´ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å¤§éƒ¨åˆ†å¯å˜æ€§ã€‚

Example 10.2.1. A dataset of 1000 images, each with 1024 pixels, may have most variance captured by just 50 eigenvectors of the covariance matrix. Projecting onto these components compresses the data while preserving essential features.
ä¾‹ 10.2.1ã€‚ä¸€ä¸ªåŒ…å« 1000 å¹…å›¾åƒçš„æ•°æ®é›†ï¼Œæ¯å¹…å›¾åƒæœ‰ 1024 ä¸ªåƒç´ ï¼Œå…¶å¤§éƒ¨åˆ†æ–¹å·®å¯èƒ½ä»…ç”±åæ–¹å·®çŸ©é˜µçš„ 50 ä¸ªç‰¹å¾å‘é‡æ•è·ã€‚æŠ•å½±åˆ°è¿™äº›åˆ†é‡ä¸Šå¯ä»¥å‹ç¼©æ•°æ®ï¼ŒåŒæ—¶ä¿ç•™åŸºæœ¬ç‰¹å¾ã€‚

### Least Squares
æœ€å°äºŒä¹˜æ³•

Often, we have more equations than unknowns-an overdetermined system:
é€šå¸¸ï¼Œæˆ‘ä»¬çš„æ–¹ç¨‹æ¯”æœªçŸ¥æ•°è¿˜å¤šâ€”â€”ä¸€ä¸ªè¶…å®šç³»ç»Ÿï¼š

$$
A\mathbf{x} \approx \mathbf{b}, \quad A \in \mathbb{R}^{m \times n}, \ m > n.
$$

An exact solution may not exist. Instead, we seek $\mathbf{x}$ that minimizes the error
ç²¾ç¡®è§£å¯èƒ½ä¸å­˜åœ¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯»æ±‚æœ€å°åŒ–è¯¯å·®çš„ $\mathbf{x}$

$$
\|A\mathbf{x} - \mathbf{b}\|^2.
$$

This leads to the normal equations:
è¿™å¯¼è‡´äº†æ­£è§„æ–¹ç¨‹ï¼š

$$
A^T A \mathbf{x} = A^T \mathbf{b}.
$$

The solution is the orthogonal projection of $\mathbf{b}$ onto the column space of $A$.
è§£å†³æ–¹æ¡ˆæ˜¯å°† $\mathbf{b}$ æ­£äº¤æŠ•å½±åˆ° $A$ çš„åˆ—ç©ºé—´ä¸Šã€‚

### Example 10.2.2
ä¾‹ 10.2.2

Fit a line $y = mx + c$ to data points $(x_i, y_i)$.
å°†çº¿ $y = mx + c$ ä¸æ•°æ®ç‚¹ $(x_i, y_i)$ æ‹Ÿåˆã€‚

Matrix form:
çŸ©é˜µå½¢å¼ï¼š

$$
A = \begin{bmatrix}x_1 & 1 \\x_2 & 1 \\\vdots & \vdots \\x_m & 1\end{bmatrix},\quad\mathbf{b} =\begin{bmatrix}y_1 \\y_2 \\\vdots \\y_m \end{bmatrix},\quad\mathbf{x} =\begin{bmatrix}m \\c \end{bmatrix}.
$$

Solve $A^T A \mathbf{x} = A^T \mathbf{b}$. This yields the best-fit line in the least squares sense.
æ±‚è§£ $A^T A \mathbf{x} = A^T \mathbf{b}$ ã€‚è¿™å°†å¾—å‡ºæœ€å°äºŒä¹˜æ„ä¹‰ä¸Šçš„æœ€ä½³æ‹Ÿåˆçº¿ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   Dimensionality reduction: Find the best subspace capturing most variance.
    é™ç»´ï¼šæ‰¾åˆ°æ•è·æœ€å¤šæ–¹å·®çš„æœ€ä½³å­ç©ºé—´ã€‚
*   Least squares: Project the target vector onto the subspace spanned by predictors.
    æœ€å°äºŒä¹˜ï¼šå°†ç›®æ ‡å‘é‡æŠ•å½±åˆ°é¢„æµ‹å˜é‡æ‰€è·¨è¶Šçš„å­ç©ºé—´ä¸Šã€‚

Both are projection problems, solved using inner products and orthogonality.
ä¸¤è€…éƒ½æ˜¯æŠ•å½±é—®é¢˜ï¼Œä½¿ç”¨å†…ç§¯å’Œæ­£äº¤æ€§æ¥è§£å†³ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Dimensionality reduction makes large datasets tractable, filters noise, and reveals structure. Least squares fitting powers regression, statistics, and machine learning. Both rely directly on eigenvalues, eigenvectors, and projections-core tools of linear algebra.
é™ç»´ä½¿å¤§å‹æ•°æ®é›†æ›´æ˜“äºå¤„ç†ï¼Œè¿‡æ»¤å™ªå£°å¹¶æ­ç¤ºç»“æ„ã€‚æœ€å°äºŒä¹˜æ‹Ÿåˆä¸ºå›å½’ã€ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ æä¾›æ”¯æŒã€‚ä¸¤è€…éƒ½ç›´æ¥ä¾èµ–äºç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡å’ŒæŠ•å½±â€”â€”çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒå·¥å…·ã€‚

### Exercises 10.2
ç»ƒä¹ 10.2

1.  Explain why PCA reduces noise in datasets by discarding small eigenvalue components.
    è§£é‡Šä¸ºä»€ä¹ˆ PCA é€šè¿‡ä¸¢å¼ƒè¾ƒå°çš„ç‰¹å¾å€¼åˆ†é‡æ¥å‡å°‘æ•°æ®é›†ä¸­çš„å™ªå£°ã€‚
2.  Compute the least squares solution to fitting a line through $(0,0), (1,1), (2,2)$.
    è®¡ç®—é€šè¿‡ $(0,0), (1,1), (2,2)$ æ‹Ÿåˆç›´çº¿çš„æœ€å°äºŒä¹˜è§£ã€‚
3.  Show that the least squares solution is unique if and only if $A^T A$ is invertible.
    è¯æ˜æœ€å°äºŒä¹˜è§£æ˜¯å”¯ä¸€çš„å½“ä¸”ä»…å½“ $A^T A$ å¯é€†ã€‚
4.  Prove that the least squares solution minimizes the squared error by projection arguments.
    è¯æ˜æœ€å°äºŒä¹˜è§£é€šè¿‡æŠ•å½±å‚æ•°æœ€å°åŒ–å¹³æ–¹è¯¯å·®ã€‚
5.  Apply PCA to the data points $(1,0), (2,1), (3,2)$ and find the first principal component.
    å°† PCA åº”ç”¨äºæ•°æ®ç‚¹ $(1,0), (2,1), (3,2)$ å¹¶æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ã€‚

## 10.3 Networks and Markov Chains
10.3 ç½‘ç»œå’Œé©¬å°”å¯å¤«é“¾

Graphs and networks provide a natural setting where linear algebra comes to life. From modeling flows and connectivity to predicting long-term behavior, matrices translate network structure into algebraic form. Markov chains, already introduced in Section 8.4, are a central example of networks evolving over time.
å›¾å’Œç½‘ç»œä¸ºçº¿æ€§ä»£æ•°çš„è¿ç”¨æä¾›äº†è‡ªç„¶çš„å¹³å°ã€‚ä»å»ºæ¨¡æµå’Œè¿æ¥åˆ°é¢„æµ‹é•¿æœŸè¡Œä¸ºï¼ŒçŸ©é˜µå°†ç½‘ç»œç»“æ„è½¬åŒ–ä¸ºä»£æ•°å½¢å¼ã€‚é©¬å°”å¯å¤«é“¾ï¼ˆå·²åœ¨ 8.4 èŠ‚ä»‹ç»ï¼‰æ˜¯ç½‘ç»œéšæ—¶é—´æ¼”åŒ–çš„ä¸€ä¸ªå…¸å‹ä¾‹å­ã€‚

### Adjacency Matrices
é‚»æ¥çŸ©é˜µ

A network (graph) with $n$ nodes can be represented by an adjacency matrix $A \in \mathbb{R}^{n \times n}$:
å…·æœ‰ $n$ ä¸ªèŠ‚ç‚¹çš„ç½‘ç»œï¼ˆå›¾ï¼‰å¯ä»¥ç”¨é‚»æ¥çŸ©é˜µ $A \in \mathbb{R}^{n \times n}$ è¡¨ç¤ºï¼š

$$
A_{ij} =\begin{cases}1 & \text{if there is an edge from node \(i\) to node \(j\)} \\0 & \text{otherwise.}\end{cases}
$$

For weighted graphs, entries may be positive weights instead of 0/1.
å¯¹äºåŠ æƒå›¾ï¼Œæ¡ç›®å¯èƒ½æ˜¯æ­£æƒé‡è€Œä¸æ˜¯ 0/1 ã€‚

*   The number of walks of length $k$ from node $i$ to node $j$ is given by the entry $(A^k)_{ij}$.
    ä»èŠ‚ç‚¹ $i$ åˆ°èŠ‚ç‚¹ $j$ çš„é•¿åº¦ä¸º $k$ çš„æ­¥è¡Œæ¬¡æ•°ç”±æ¡ç›® ( ğ´ ğ‘˜ ) ğ‘– ğ‘— ï¼ˆä¸€ä¸ª k ) ä¼Šå¥‡ â€‹ .
*   Powers of adjacency matrices thus encode connectivity over time.
    å› æ­¤ï¼Œé‚»æ¥çŸ©é˜µçš„å¹‚å¯ä»¥å¯¹éšæ—¶é—´å˜åŒ–çš„è¿é€šæ€§è¿›è¡Œç¼–ç ã€‚

### Laplacian Matrices
æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ

Another important matrix is the graph Laplacian:
å¦ä¸€ä¸ªé‡è¦çš„çŸ©é˜µæ˜¯å›¾æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼š

$$
L = D - A,
$$

where $D$ is the diagonal degree matrix ( $D_{ii} = \text{degree}(i)$ ).
å…¶ä¸­ $D$ æ˜¯å¯¹è§’åº¦çŸ©é˜µ ( $D_{ii} = \text{degree}(i)$ )ã€‚

*   $L$ is symmetric and positive semidefinite.
    $L$ æ˜¯å¯¹ç§°çš„å¹¶ä¸”æ˜¯æ­£åŠå®šçš„ã€‚
*   The smallest eigenvalue is always $0$, with eigenvector $(1,1,\\dots,1)$.
    æœ€å°ç‰¹å¾å€¼å§‹ç»ˆæ˜¯ $0 $, with eigenvector $ (1,1,\\dots,1)$ã€‚
*   The multiplicity of eigenvalue 0 equals the number of connected components in the graph.
    ç‰¹å¾å€¼ 0 çš„å¤šé‡æ€§ç­‰äºå›¾ä¸­è¿é€šåˆ†é‡çš„æ•°é‡ã€‚

This connection between eigenvalues and connectivity forms the basis of spectral graph theory.
ç‰¹å¾å€¼å’Œè¿é€šæ€§ä¹‹é—´çš„è¿™ç§è”ç³»æ„æˆäº†è°±å›¾ç†è®ºçš„åŸºç¡€ã€‚

### Markov Chains on Graphs
å›¾ä¸Šçš„é©¬å°”å¯å¤«é“¾

A Markov chain can be viewed as a random walk on a graph. If $P$ is the transition matrix where $P_{ij}$ is the probability of moving from node $i$ to node $j$, then
é©¬å°”å¯å¤«é“¾å¯ä»¥çœ‹ä½œå›¾ä¸Šçš„éšæœºæ¸¸åŠ¨ã€‚è®¾ $P$ ä¸ºè½¬ç§»çŸ©é˜µï¼Œå…¶ä¸­ ğ‘ƒ ğ‘– ğ‘— P ä¼Šå¥‡ â€‹ æ˜¯ä»èŠ‚ç‚¹ $i$ ç§»åŠ¨åˆ°èŠ‚ç‚¹ $j$ çš„æ¦‚ç‡ï¼Œé‚£ä¹ˆ

$$
\mathbf{x}_{k+1} = P \mathbf{x}_k
$$

describes the distribution of positions after $k$ steps.
æè¿° $k$ æ­¥ä¹‹åçš„ä½ç½®åˆ†å¸ƒã€‚

*   The steady-state distribution is given by the eigenvector of $P$ with eigenvalue 1.
    ç¨³æ€åˆ†å¸ƒç”±ç‰¹å¾å‘é‡ $P$ ç»™å‡ºï¼Œç‰¹å¾å€¼ä¸º 1 ã€‚
*   The speed of convergence depends on the gap between the largest eigenvalue (which is always 1) and the second largest eigenvalue.
    æ”¶æ•›é€Ÿåº¦å–å†³äºæœ€å¤§ç‰¹å¾å€¼ï¼ˆå§‹ç»ˆä¸º 1 ï¼‰ä¸ç¬¬äºŒå¤§ç‰¹å¾å€¼ä¹‹é—´çš„å·®è·ã€‚

### Example 10.3.1
ä¾‹ 10.3.1

Consider a simple 3-node cycle graph:
è€ƒè™‘ä¸€ä¸ªç®€å•çš„ 3 èŠ‚ç‚¹å¾ªç¯å›¾ï¼š

$$
P = \begin{bmatrix}0 & 1 & 0 \\0 & 0 & 1 \\1 & 0 & 0\end{bmatrix}.
$$

```
This Markov chain cycles deterministically among the nodes. Eigenvalues are the cube roots of unity: $1, e^{2\\pi i/3}, e^{4\\pi i/3}. The eigenvalue \\1$corresponds to the steady state, which is the uniformdistribution$(1/3,1/3,1/3)$.

è¿™ä¸ªé©¬å°”å¯å¤«é“¾åœ¨èŠ‚ç‚¹ä¹‹é—´ç¡®å®šæ€§åœ°å¾ªç¯ã€‚ç‰¹å¾å€¼æ˜¯ ç»Ÿä¸€ï¼š$1,e^{2\\pi i/3},e^{4\\pi i/3} ã€‚

ç‰¹å¾å€¼ \\ 1 $corresponds to the steady state, which is the uniformdistribution$ (1/3,1/3,1/3)$ã€‚
```

### Applications
åº”ç”¨

*   Search engines: Googleâ€™s PageRank algorithm models the web as a Markov chain, where steady-state probabilities rank pages.
    æœç´¢å¼•æ“ï¼šGoogle çš„ PageRank ç®—æ³•å°†ç½‘ç»œå»ºæ¨¡ä¸ºé©¬å°”å¯å¤«é“¾ï¼Œå…¶ä¸­ç¨³æ€æ¦‚ç‡å¯¹ç½‘é¡µè¿›è¡Œæ’åã€‚
*   Network analysis: Eigenvalues of adjacency or Laplacian matrices reveal communities, bottlenecks, and robustness.
    ç½‘ç»œåˆ†æï¼šé‚»æ¥çŸ©é˜µæˆ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å€¼æ­ç¤ºç¤¾åŒºã€ç“¶é¢ˆå’Œç¨³å¥æ€§ã€‚
*   Epidemiology and information flow: Random walks model how diseases or ideas spread through networks.
    æµè¡Œç—…å­¦å’Œä¿¡æ¯æµï¼šéšæœºæ¸¸åŠ¨æ¨¡æ‹Ÿç–¾ç—…æˆ–æ€æƒ³å¦‚ä½•é€šè¿‡ç½‘ç»œä¼ æ’­ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Linear algebra transforms network problems into matrix problems. Eigenvalues and eigenvectors reveal connectivity, flow, stability, and long-term dynamics. Networks are everywhere-social media, biology, finance, and the internet-so these tools are indispensable.
çº¿æ€§ä»£æ•°å°†ç½‘ç»œé—®é¢˜è½¬åŒ–ä¸ºçŸ©é˜µé—®é¢˜ã€‚ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ­ç¤ºäº†è¿é€šæ€§ã€æµåŠ¨ã€ç¨³å®šæ€§å’Œé•¿æœŸåŠ¨æ€ã€‚ç½‘ç»œæ— å¤„ä¸åœ¨â€”â€”ç¤¾äº¤åª’ä½“ã€ç”Ÿç‰©ã€é‡‘èå’Œäº’è”ç½‘â€”â€”å› æ­¤è¿™äº›å·¥å…·ä¸å¯æˆ–ç¼ºã€‚

### Exercises 10.3
ç»ƒä¹ 10.3

1.  Write the adjacency matrix of a square graph with 4 nodes. Compute $A^2$ and interpret the entries.
    å†™å‡ºä¸€ä¸ªæœ‰ 4 ä¸ªèŠ‚ç‚¹çš„æ­£æ–¹å½¢å›¾çš„é‚»æ¥çŸ©é˜µã€‚è®¡ç®— $A^2$ å¹¶è§£é‡Šå…¶ä¸­çš„å…ƒç´ ã€‚
    
2.  Show that the Laplacian of a connected graph has exactly one zero eigenvalue.
    è¯æ˜è¿é€šå›¾çš„æ‹‰æ™®æ‹‰æ–¯ç®—å­æ°å¥½æœ‰ä¸€ä¸ªé›¶ç‰¹å¾å€¼ã€‚
    
3.  Find the steady-state distribution of the Markov chain with
    æ‰¾åˆ°é©¬å°”å¯å¤«é“¾çš„ç¨³æ€åˆ†å¸ƒ
    
    $$
    P = \begin{bmatrix} 0.5 & 0.5 \\ 0.4 & 0.6 \end{bmatrix}.
    $$
    
4.  Explain how eigenvalues of the Laplacian can detect disconnected components of a graph.
    è§£é‡Šæ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å€¼å¦‚ä½•æ£€æµ‹å›¾ä¸­ä¸è¿ç»­çš„ç»„æˆéƒ¨åˆ†ã€‚
    
5.  Describe how PageRank modifies the transition matrix of the web graph to ensure a unique steady-state distribution.
    æè¿° PageRank å¦‚ä½•ä¿®æ”¹ç½‘ç»œå›¾çš„è½¬æ¢çŸ©é˜µä»¥ç¡®ä¿å”¯ä¸€çš„ç¨³æ€åˆ†å¸ƒã€‚
    

## 10.4 Machine Learning Connections
10.4 æœºå™¨å­¦ä¹ è¿æ¥

Modern machine learning is built on linear algebra. From the representation of data as matrices to the optimization of large-scale models, nearly every step relies on concepts such as vector spaces, projections, eigenvalues, and matrix decompositions.
ç°ä»£æœºå™¨å­¦ä¹ å»ºç«‹åœ¨çº¿æ€§ä»£æ•°çš„åŸºç¡€ä¸Šã€‚ä»æ•°æ®çŸ©é˜µè¡¨ç¤ºåˆ°å¤§è§„æ¨¡æ¨¡å‹çš„ä¼˜åŒ–ï¼Œå‡ ä¹æ¯ä¸€æ­¥éƒ½ä¾èµ–äºå‘é‡ç©ºé—´ã€æŠ•å½±ã€ç‰¹å¾å€¼å’ŒçŸ©é˜µåˆ†è§£ç­‰æ¦‚å¿µã€‚

### Data as Matrices
æ•°æ®ä½œä¸ºçŸ©é˜µ

A dataset with $m$ examples and $n$ features is represented as a matrix $X \in \mathbb{R}^{m \times n}$:
å…·æœ‰ $m$ ä¸ªç¤ºä¾‹å’Œ $n$ ä¸ªç‰¹å¾çš„æ•°æ®é›†è¡¨ç¤ºä¸ºçŸ©é˜µ $X \in \mathbb{R}^{m \times n}$ ï¼š


$$
X = \begin{bmatrix}
\text{-} & \mathbf{x}_1^T & \text{-} \\
\text{-} & \mathbf{x}_2^T & \text{-} \\
  & \vdots &   \\
\text{-} & \mathbf{x}_m^T & \text{-}
\end{bmatrix},
$$


where each row $\mathbf{x}_i \in \mathbb{R}^n$ is a feature vector. Linear algebra provides tools to analyze, compress, and transform this data.
å…¶ä¸­æ¯è¡Œ $\mathbf{x}_i \in \mathbb{R}^n$ æ˜¯ä¸€ä¸ªç‰¹å¾å‘é‡ã€‚çº¿æ€§ä»£æ•°æä¾›äº†åˆ†æã€å‹ç¼©å’Œè½¬æ¢æ­¤ç±»æ•°æ®çš„å·¥å…·ã€‚

### Linear Models
çº¿æ€§æ¨¡å‹

At the heart of machine learning are linear predictors:
æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ˜¯çº¿æ€§é¢„æµ‹å™¨ï¼š

$\hat{y} = X\mathbf{w},$

where $\mathbf{w}$ is the weight vector. Training often involves solving a least squares problem or a regularized variant such as ridge regression:
å…¶ä¸­ $\mathbf{w}$ æ˜¯æƒé‡å‘é‡ã€‚è®­ç»ƒé€šå¸¸æ¶‰åŠæ±‚è§£æœ€å°äºŒä¹˜é—®é¢˜æˆ–æ­£åˆ™åŒ–å˜ä½“ï¼Œä¾‹å¦‚å²­å›å½’ï¼š

$\min_{\mathbf{w}} \|X\mathbf{w} - \mathbf{y}\|^2 + \lambda \|\mathbf{w}\|^2.$

This is solved efficiently using matrix factorizations.
ä½¿ç”¨çŸ©é˜µåˆ†è§£å¯ä»¥æœ‰æ•ˆåœ°è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

### Singular Value Decomposition (SVD)
å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰

The SVD of a matrix $X$ is
çŸ©é˜µ $X$ çš„ SVD ä¸º

$X = U \Sigma V^T,$

where $U, V$ are orthogonal and $\Sigma$ is diagonal with nonnegative entries (singular values).
å…¶ä¸­ $U, V$ æ˜¯æ­£äº¤çš„ï¼Œ $\Sigma$ æ˜¯å¯¹è§’çš„ï¼Œå…·æœ‰éè´Ÿé¡¹ï¼ˆå¥‡å¼‚å€¼ï¼‰ã€‚

*   Singular values measure the importance of directions in feature space.
    å¥‡å¼‚å€¼è¡¡é‡ç‰¹å¾ç©ºé—´ä¸­æ–¹å‘çš„é‡è¦æ€§ã€‚
*   SVD is used for dimensionality reduction (low-rank approximations), topic modeling, and recommender systems.
    SVD ç”¨äºé™ç»´ï¼ˆä½ç§©è¿‘ä¼¼ï¼‰ã€ä¸»é¢˜å»ºæ¨¡å’Œæ¨èç³»ç»Ÿã€‚

### Eigenvalues in Machine Learning
æœºå™¨å­¦ä¹ ä¸­çš„ç‰¹å¾å€¼

*   PCA (Principal Component Analysis): diagonalization of the covariance matrix identifies directions of maximal variance.
    PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ï¼šåæ–¹å·®çŸ©é˜µçš„å¯¹è§’åŒ–ç¡®å®šäº†æœ€å¤§æ–¹å·®çš„æ–¹å‘ã€‚
*   Spectral clustering: uses eigenvectors of the Laplacian to group data points into clusters.
    è°±èšç±»ï¼šä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å‘é‡å°†æ•°æ®ç‚¹åˆ†ç»„æˆèšç±»ã€‚
*   Stability analysis: eigenvalues of Hessian matrices determine whether optimization converges to a minimum.
    ç¨³å®šæ€§åˆ†æï¼šHessian çŸ©é˜µçš„ç‰¹å¾å€¼å†³å®šä¼˜åŒ–æ˜¯å¦æ”¶æ•›åˆ°æœ€å°å€¼ã€‚

### Neural Networks
ç¥ç»ç½‘ç»œ

Even deep learning, though nonlinear, uses linear algebra at its core:
å³ä½¿æ˜¯æ·±åº¦å­¦ä¹ ï¼Œå°½ç®¡æ˜¯éçº¿æ€§çš„ï¼Œå…¶æ ¸å¿ƒä¹Ÿä½¿ç”¨çº¿æ€§ä»£æ•°ï¼š

*   Each layer is a matrix multiplication followed by a nonlinear activation.
    æ¯ä¸€å±‚éƒ½æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œç„¶åæ˜¯éçº¿æ€§æ¿€æ´»ã€‚
*   Training requires computing gradients, which are expressed in terms of matrix calculus.
    è®­ç»ƒéœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œä»¥çŸ©é˜µå¾®ç§¯åˆ†æ¥è¡¨ç¤ºã€‚
*   Backpropagation is essentially repeated applications of the chain rule with linear algebra.
    åå‘ä¼ æ’­æœ¬è´¨ä¸Šæ˜¯é“¾å¼æ³•åˆ™ä¸çº¿æ€§ä»£æ•°çš„é‡å¤åº”ç”¨ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Machine learning models often involve datasets with millions of features and parameters. Linear algebra provides the algorithms and abstractions that make training and inference possible. Without it, large-scale computation in AI would be intractable.
æœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸æ¶‰åŠå…·æœ‰æ•°ç™¾ä¸‡ä¸ªç‰¹å¾å’Œå‚æ•°çš„æ•°æ®é›†ã€‚çº¿æ€§ä»£æ•°æä¾›äº†ä½¿è®­ç»ƒå’Œæ¨ç†æˆä¸ºå¯èƒ½çš„ç®—æ³•å’ŒæŠ½è±¡ã€‚å¦‚æœæ²¡æœ‰å®ƒï¼Œäººå·¥æ™ºèƒ½ä¸­çš„å¤§è§„æ¨¡è®¡ç®—å°†å˜å¾—éš¾ä»¥å¤„ç†ã€‚

### Exercises 10.4
ç»ƒä¹ 10.4

1.  Show that ridge regression leads to the normal equations
    è¯æ˜å²­å›å½’å¯ä»¥å¾—å‡ºæ­£æ€æ–¹ç¨‹

$$
(X^T X + \lambda I)\mathbf{w} = X^T \mathbf{y}.
$$

2.  Explain how SVD can be used to compress an image represented as a matrix of pixel intensities.
    è§£é‡Šå¦‚ä½•ä½¿ç”¨ SVD æ¥å‹ç¼©ä»¥åƒç´ å¼ºåº¦çŸ©é˜µè¡¨ç¤ºçš„å›¾åƒã€‚
    
3.  For a covariance matrix $\Sigma$, show why its eigenvalues represent variances along principal components.
    å¯¹äºåæ–¹å·®çŸ©é˜µ $\Sigma$ ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆå®ƒçš„ç‰¹å¾å€¼è¡¨ç¤ºæ²¿ä¸»æˆåˆ†çš„æ–¹å·®ã€‚
    
4.  Give an example of how eigenvectors of the Laplacian matrix can be used for clustering a small graph.
    ä¸¾ä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å‘é‡å¯¹å°å›¾è¿›è¡Œèšç±»ã€‚
    
5.  In a neural network with one hidden layer, write the forward pass in matrix form.
    åœ¨å…·æœ‰ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œä¸­ï¼Œä»¥çŸ©é˜µå½¢å¼å†™å‡ºå‰å‘ä¼ é€’ã€‚
