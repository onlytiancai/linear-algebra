# Chapter 7. Inner Product Spaces

ç¬¬ 7 ç« å†…ç§¯ç©ºé—´

## 7.1 Inner Products and Norms

7.1 å†…ç§¯å’ŒèŒƒæ•°

To extend the geometric ideas of length, distance, and angle beyond $\mathbb{R}^2$ and $\mathbb{R}^3$, we introduce inner products. Inner products provide a way of measuring similarity between vectors, while norms derived from them measure length. These concepts are the foundation of geometry inside vector spaces.

ä¸ºäº†å°†é•¿åº¦ã€è·ç¦»å’Œè§’åº¦çš„å‡ ä½•æ¦‚å¿µæ‰©å±•åˆ° $\mathbb{R}^2$ å’Œ $\mathbb{R}^3$ ä¹‹å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å†…ç§¯ã€‚å†…ç§¯æä¾›äº†ä¸€ç§åº¦é‡å‘é‡ä¹‹é—´ç›¸ä¼¼æ€§çš„æ–¹æ³•ï¼Œè€Œç”±å†…ç§¯å¯¼å‡ºçš„èŒƒæ•°åˆ™ç”¨äºåº¦é‡é•¿åº¦ã€‚è¿™äº›æ¦‚å¿µæ˜¯å‘é‡ç©ºé—´å‡ ä½•çš„åŸºç¡€ã€‚

### Inner Product

å†…ç§¯

An inner product on a real vector space $V$ is a function

å®å‘é‡ç©ºé—´ $V$ ä¸Šçš„å†…ç§¯æ˜¯ä¸€ä¸ªå‡½æ•°

$$
\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}
$$

that assigns to each pair of vectors $(\mathbf{u}, \mathbf{v})$ a real number, subject to the following properties:

ä¸ºæ¯å¯¹å‘é‡ $(\mathbf{u}, \mathbf{v})$ åˆ†é…ä¸€ä¸ªå®æ•°ï¼Œå¹¶éµå¾ªä»¥ä¸‹å±æ€§ï¼š

1.  Symmetry: $\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.$

    å¯¹ç§°æ€§ï¼š $\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.$
    
2.  Linearity in the first argument: $\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v} \rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle \mathbf{w}, \mathbf{v} \rangle.$

    ç¬¬ä¸€ä¸ªå‚æ•°çš„çº¿æ€§ï¼š $\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v} \rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle \mathbf{w}, \mathbf{v} \rangle.$
    
3.  Positive-definiteness: $\langle \mathbf{v}, \mathbf{v} \rangle \geq 0$, and equality holds if and only if $\mathbf{v} = \mathbf{0}$.

    æ­£å®šæ€§ï¼š $\langle \mathbf{v}, \mathbf{v} \rangle \geq 0$ ï¼Œä¸”ä»…å½“ $\mathbf{v} = \mathbf{0}$ æ—¶ç­‰å¼æˆç«‹ã€‚
    

The standard inner product on $\mathbb{R}^n$ is the dot product:

$\mathbb{R}^n$ ä¸Šçš„æ ‡å‡†å†…ç§¯æ˜¯ç‚¹ç§¯ï¼š

$$
\langle \mathbf{u}, \mathbf{v} \rangle = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
$$

### Norms
èŒƒæ•°

The norm of a vector is its length, defined in terms of the inner product:

å‘é‡çš„èŒƒæ•°æ˜¯å…¶é•¿åº¦ï¼Œæ ¹æ®å†…ç§¯å®šä¹‰ï¼š

$$
\|\mathbf{v}\| = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}.
$$

For the dot product in $\mathbb{R}^n$:

å¯¹äº $\mathbb{R}^n$ ä¸­çš„ç‚¹ç§¯ï¼š

$$
\|(x_1, x_2, \dots, x_n)\| = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}.
$$

### Angles Between Vectors

å‘é‡ä¹‹é—´çš„è§’åº¦

The inner product allows us to define the angle $\theta$ between two nonzero vectors $\mathbf{u}, \mathbf{v}$ by

é€šè¿‡å†…ç§¯ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸¤ä¸ªéé›¶å‘é‡ $\mathbf{u}, \mathbf{v}$ ä¹‹é—´çš„è§’åº¦ $\theta$ ï¼Œå³

$$
\cos \theta = \frac{\langle \mathbf{u}, \mathbf{v} \rangle}{\|\mathbf{u}\| \, \|\mathbf{v}\|}.
$$

Thus, two vectors are orthogonal if $\langle \mathbf{u}, \mathbf{v} \rangle = 0$.

å› æ­¤ï¼Œè‹¥ $\langle \mathbf{u}, \mathbf{v} \rangle = 0$ ï¼Œåˆ™ä¸¤ä¸ªå‘é‡æ­£äº¤ã€‚

### Examples

ç¤ºä¾‹

Example 7.1.1. In $\mathbb{R}^2$, with $\mathbf{u} = (1,2)$, $\mathbf{v} = (3,4)$:

ä¾‹ 7.1.1ã€‚ åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œä½¿ç”¨ $\mathbf{u} = (1,2)$ ã€ $\mathbf{v} = (3,4)$ ï¼š

$$
\langle \mathbf{u}, \mathbf{v} \rangle = 1\cdot 3 + 2\cdot 4 = 11.
$$

 

$$
\|\mathbf{u}\| = \sqrt{1^2 + 2^2} = \sqrt{5}, \quad \|\mathbf{v}\| = \sqrt{3^2 + 4^2} = 5.
$$

So,

æ‰€ä»¥ï¼Œ

$$
\cos \theta = \frac{11}{\sqrt{5}\cdot 5}.
$$

Example 7.1.2. In the function space $C[0,1]$, the inner product

ä¾‹ 7.1.2ã€‚ åœ¨å‡½æ•°ç©ºé—´ $C[0,1]$ ä¸­ï¼Œå†…ç§¯

$$
\langle f, g \rangle = \int_0^1 f(x) g(x)\, dx
$$

defines a length

å®šä¹‰é•¿åº¦

$$
\|f\| = \sqrt{\int_0^1 f(x)^2 dx}.
$$

This generalizes geometry to infinite-dimensional spaces.

è¿™å°†å‡ ä½•å­¦æ¨å¹¿åˆ°æ— é™ç»´ç©ºé—´ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   Inner product: measures similarity between vectors.

    å†…ç§¯ï¼šæµ‹é‡å‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
*   Norm: length of a vector.

    èŒƒæ•°ï¼šå‘é‡çš„é•¿åº¦ã€‚
*   Angle: measure of alignment between two directions.

    è§’åº¦ï¼šä¸¤ä¸ªæ–¹å‘ä¹‹é—´çš„å¯¹é½åº¦é‡ã€‚

These concepts unify algebraic operations with geometric intuition.

è¿™äº›æ¦‚å¿µå°†ä»£æ•°è¿ç®—ä¸å‡ ä½•ç›´è§‰ç»Ÿä¸€èµ·æ¥ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Inner products and norms allow us to extend geometry into abstract vector spaces. They form the basis of orthogonality, projections, Fourier series, least squares approximation, and many applications in physics and machine learning.

å†…ç§¯å’ŒèŒƒæ•°ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†å‡ ä½•æ‰©å±•åˆ°æŠ½è±¡å‘é‡ç©ºé—´ã€‚å®ƒä»¬æ„æˆäº†æ­£äº¤æ€§ã€æŠ•å½±ã€å‚…é‡Œå¶çº§æ•°ã€æœ€å°äºŒä¹˜è¿‘ä¼¼ä»¥åŠç‰©ç†å­¦å’Œæœºå™¨å­¦ä¹ ä¸­è®¸å¤šåº”ç”¨çš„åŸºç¡€ã€‚

### Exercises 7.1
ç»ƒä¹  7.1

1.  Compute $\langle (2,-1,3), (1,4,0) \rangle$. Then find the angle between them.

    è®¡ç®— $\langle (2,-1,3), (1,4,0) \rangle$ ã€‚ç„¶åæ±‚å‡ºå®ƒä»¬ä¹‹é—´çš„è§’åº¦ã€‚
    
2.  Show that $\|(x,y)\| = \sqrt{x^2+y^2}$ satisfies the properties of a norm.

    è¯æ˜$\|(x,y)\| = \sqrt{x^2+y^2}$â€‹ æ»¡è¶³èŒƒæ•°çš„æ€§è´¨ã€‚
    
3.  In $\mathbb{R}^3$, verify that $(1,1,0)$ and $(1,-1,0)$ are orthogonal.

    åœ¨ $\mathbb{R}^3$ ä¸­ï¼ŒéªŒè¯ $(1,1,0)$ å’Œ $(1,-1,0)$ æ˜¯å¦æ­£äº¤ã€‚
    
4.  In $C[0,1]$, compute $\langle f,g \rangle$ for $f(x)=x$, $g(x)=1$.

    åœ¨ $C[0,1]$ ä¸­ï¼Œè®¡ç®— $f(x)=x$ ã€ $g(x)=1$ çš„ $\langle f,g \rangle$ ã€‚
    
5.  Prove the Cauchyâ€“Schwarz inequality:

    è¯æ˜æŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼ï¼š
    
    $$
    |\langle \mathbf{u}, \mathbf{v} \rangle| \leq \|\mathbf{u}\| \, \|\mathbf{v}\|.
    $$
    

## 7.2 Orthogonal Projections

7.2 æ­£äº¤æŠ•å½±

One of the most useful applications of inner products is the notion of orthogonal projection. Projection allows us to approximate a vector by another lying in a subspace, minimizing error in the sense of distance. This idea underpins geometry, statistics, and numerical analysis.

å†…ç§¯æœ€æœ‰ç”¨çš„åº”ç”¨ä¹‹ä¸€æ˜¯æ­£äº¤æŠ•å½±çš„æ¦‚å¿µã€‚æŠ•å½±ä½¿æˆ‘ä»¬èƒ½å¤Ÿç”¨å­ç©ºé—´ä¸­çš„å¦ä¸€ä¸ªå‘é‡æ¥è¿‘ä¼¼ä¸€ä¸ªå‘é‡ï¼Œä»è€Œæœ€å°åŒ–è·ç¦»æ–¹å‘ä¸Šçš„è¯¯å·®ã€‚è¿™ä¸€æ€æƒ³æ˜¯å‡ ä½•ã€ç»Ÿè®¡å­¦å’Œæ•°å€¼åˆ†æçš„åŸºç¡€ã€‚

### Projection onto a Line

æŠ•å½±åˆ°çº¿ä¸Š

Let $\mathbf{u} \in \mathbb{R}^n$ be a nonzero vector. The line spanned by $\mathbf{u}$ is

ä»¤ $\mathbf{u} \in \mathbb{R}^n$ ä¸ºéé›¶å‘é‡ã€‚ $\mathbf{u}$ æ‰€å¼ æˆçš„ç›´çº¿ä¸º

$$
L = \{ c\mathbf{u} \mid c \in \mathbb{R} \}.
$$

Given a vector $\mathbf{v}$, the projection of $\mathbf{v}$ onto $\mathbf{u}$ is the vector in $L$ closest to $\mathbf{v}$. Geometrically, it is the shadow of $\mathbf{v}$ on the line.

ç»™å®šå‘é‡ $\mathbf{v}$ ï¼Œ $\mathbf{v}$ åœ¨ $\mathbf{u}$ ä¸Šçš„æŠ•å½±æ˜¯ $L$ ä¸­è·ç¦» $\mathbf{v}$ æœ€è¿‘çš„å‘é‡ã€‚ä»å‡ ä½•å­¦ä¸Šè®²ï¼Œå®ƒæ˜¯ $\mathbf{v}$ åœ¨çº¿ä¸Šçš„é˜´å½±ã€‚

The formula is

å…¬å¼æ˜¯

$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\langle \mathbf{v}, \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle} \, \mathbf{u}.
$$

The error vector $\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$ is orthogonal to $\mathbf{u}$.

è¯¯å·®å‘é‡ $\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$ ä¸ $\mathbf{u}$ æ­£äº¤ã€‚

### Example 7.2.1
ä¾‹ 7.2.1

Let $\mathbf{u} = (1,2)$, $\mathbf{v} = (3,1)$.

ä»¤ $\mathbf{u} = (1,2)$ ï¼Œ $\mathbf{v} = (3,1)$ ã€‚

$$
\langle \mathbf{v}, \mathbf{u} \rangle = 3\cdot 1 + 1\cdot 2 = 5, \quad\langle \mathbf{u}, \mathbf{u} \rangle = 1^2 + 2^2 = 5.
$$

So

æ‰€ä»¥

$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{5}{5}(1,2) = (1,2).
$$

The error vector is $(3,1) - (1,2) = (2,-1)$, which is orthogonal to $(1,2)$.

è¯¯å·®å‘é‡ä¸º $(3,1) - (1,2) = (2,-1)$ ï¼Œä¸ $(1,2)$ æ­£äº¤ã€‚

### Projection onto a Subspace

æŠ•å½±åˆ°å­ç©ºé—´

Suppose $W \subseteq \mathbb{R}^n$ is a subspace with orthonormal basis $\{ \mathbf{w}_1, \dots, \mathbf{w}_k \}$. The projection of a vector $\mathbf{v}$ onto $W$ is

å‡è®¾ $W \subseteq \mathbb{R}^n$ æ˜¯ä¸€ä¸ªå…·æœ‰æ­£äº¤åŸº $\{ \mathbf{w}_1, \dots, \mathbf{w}_k \}$ çš„å­ç©ºé—´ã€‚å‘é‡ $\mathbf{v}$ åœ¨ $W$ ä¸Šçš„æŠ•å½±ä¸º

$$
\text{proj}_{W}(\mathbf{v}) = \langle \mathbf{v}, \mathbf{w}_1 \rangle \mathbf{w}_1 + \cdots + \langle \mathbf{v}, \mathbf{w}_k \rangle \mathbf{w}_k.
$$

This is the unique vector in $W$ closest to $\mathbf{v}$. The difference $\mathbf{v} - \text{proj}_{W}(\mathbf{v})$ is orthogonal to all of $W$.

è¿™æ˜¯ $W$ ä¸­ä¸ $\mathbf{v}$ æœ€æ¥è¿‘çš„å”¯ä¸€å‘é‡ã€‚å·®å€¼ $\mathbf{v} - \text{proj}_{W}(\mathbf{v})$ ä¸æ‰€æœ‰ $W$ æ­£äº¤ã€‚

### Least Squares Approximation

æœ€å°äºŒä¹˜è¿‘ä¼¼

Orthogonal projection explains the method of least squares. To solve an overdetermined system $A\mathbf{x} \approx \mathbf{b}$, we seek the $\mathbf{x}$ that makes $A\mathbf{x}$ the projection of $\mathbf{b}$ onto the column space of $A$. This gives the normal equations

æ­£äº¤æŠ•å½±è§£é‡Šäº†æœ€å°äºŒä¹˜æ³•ã€‚ä¸ºäº†è§£å†³è¶…å®šé—®é¢˜ ç³»ç»Ÿ $A\mathbf{x} \approx \mathbf{b}$ ï¼Œæˆ‘ä»¬å¯»æ‰¾ $\mathbf{x}$ ï¼Œä½¿å¾— $A\mathbf{x}$ æˆä¸º $\mathbf{b}$ åœ¨ $A$ çš„åˆ—ç©ºé—´ä¸Šçš„æŠ•å½±ã€‚è¿™ç»™å‡ºäº†æ­£åˆ™æ–¹ç¨‹

$$
A^T A \mathbf{x} = A^T \mathbf{b}.
$$

Thus, least squares is just projection in disguise.

å› æ­¤ï¼Œæœ€å°äºŒä¹˜æ³•åªæ˜¯ä¼ªè£…çš„æŠ•å½±ã€‚

### Geometric Interpretation

å‡ ä½•è§£é‡Š

*   Projection finds the closest point in a subspace to a given vector.

    æŠ•å½±æ‰¾åˆ°å­ç©ºé—´ä¸­è·ç¦»ç»™å®šå‘é‡æœ€è¿‘çš„ç‚¹ã€‚

*   It minimizes distance (error) in the sense of Euclidean norm.

    å®ƒæŒ‰ç…§æ¬§å‡ é‡Œå¾—èŒƒæ•°çš„æ„ä¹‰æœ€å°åŒ–è·ç¦»ï¼ˆè¯¯å·®ï¼‰ã€‚
*   Orthogonality ensures the error vector points directly away from the subspace.

    æ­£äº¤æ€§ç¡®ä¿è¯¯å·®å‘é‡ç›´æ¥æŒ‡å‘è¿œç¦»å­ç©ºé—´çš„æ–¹å‘ã€‚

### Why this matters

ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Orthogonal projection is central in both pure and applied mathematics. It underlies the geometry of subspaces, the theory of Fourier series, regression in statistics, and approximation methods in numerical linear algebra. Whenever we fit data with a simpler model, projection is at work.

æ­£äº¤æŠ•å½±åœ¨çº¯æ•°å­¦å’Œåº”ç”¨æ•°å­¦ä¸­éƒ½è‡³å…³é‡è¦ã€‚å®ƒæ˜¯å­ç©ºé—´å‡ ä½•ã€å‚…é‡Œå¶çº§æ•°ç†è®ºã€ç»Ÿè®¡å­¦ä¸­çš„å›å½’ä»¥åŠæ•°å€¼çº¿æ€§ä»£æ•°ä¸­çš„è¿‘ä¼¼æ–¹æ³•çš„åŸºç¡€ã€‚æ¯å½“æˆ‘ä»¬ç”¨æ›´ç®€å•çš„æ¨¡å‹æ‹Ÿåˆæ•°æ®æ—¶ï¼ŒæŠ•å½±å°±ä¼šå‘æŒ¥ä½œç”¨ã€‚

### Exercises 7.2
ç»ƒä¹  7.2

1.  Compute the projection of $(2,3)$ onto the vector $(1,1)$.

    è®¡ç®— $(2,3)$ åˆ°å‘é‡ $(1,1)$ çš„æŠ•å½±ã€‚
2.  Show that $\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$ is orthogonal to $\mathbf{u}$.

    è¯æ˜ $\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$ ä¸ $\mathbf{u}$ æ­£äº¤ã€‚
3.  Let $W = \text{span}\{(1,0,0), (0,1,0)\} \subseteq \mathbb{R}^3$. Find the projection of $(1,2,3)$ onto $W$.

    ä»¤ $W = \text{span}\{(1,0,0), (0,1,0)\} \subseteq \mathbb{R}^3$ ã€‚æ±‚ $(1,2,3)$ åˆ° $W$ çš„æŠ•å½±ã€‚
4.  Explain why least squares fitting corresponds to projection onto the column space of $A$.

    è§£é‡Šä¸ºä»€ä¹ˆæœ€å°äºŒä¹˜æ‹Ÿåˆå¯¹åº”äº $A$ çš„åˆ—ç©ºé—´ä¸Šçš„æŠ•å½±ã€‚
5.  Prove that projection onto a subspace $W$ is unique: there is exactly one closest vector in $W$ to a given $\mathbf{v}$.

    è¯æ˜æŠ•å½±åˆ°å­ç©ºé—´ $W$ æ˜¯å”¯ä¸€çš„ï¼šåœ¨ $W$ ä¸­ï¼Œæœ‰ä¸”ä»…æœ‰ä¸€ä¸ªä¸ç»™å®š $\mathbf{v}$ æœ€æ¥è¿‘çš„å‘é‡ã€‚

## 7.3 Gramâ€“Schmidt Process

7.3 æ ¼æ‹‰å§†-æ–½å¯†ç‰¹è¿‡ç¨‹

The Gramâ€“Schmidt process is a systematic way to turn any linearly independent set of vectors into an orthonormal basis. This is especially useful because orthonormal bases simplify computations: inner products become simple coordinate comparisons, and projections take clean forms.

æ ¼æ‹‰å§†-æ–½å¯†ç‰¹è¿‡ç¨‹æ˜¯ä¸€ç§å°†ä»»æ„çº¿æ€§æ— å…³çš„å‘é‡é›†è½¬åŒ–ä¸ºæ­£äº¤åŸºçš„ç³»ç»Ÿæ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å°¤å…¶æœ‰ç”¨ï¼Œå› ä¸ºæ­£äº¤åŸºå¯ä»¥ç®€åŒ–è®¡ç®—ï¼šå†…ç§¯å˜æˆäº†ç®€å•çš„åæ ‡æ¯”è¾ƒï¼Œå¹¶ä¸”æŠ•å½±å‘ˆç°å‡ºæ¸…æ™°çš„å½¢å¼ã€‚

### The Idea
ç†å¿µ

Given a linearly independent set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}$ in an inner product space, we want to construct an orthonormal set $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\}$ that spans the same subspace.

ç»™å®šå†…ç§¯ç©ºé—´ä¸­ä¸€ç»„çº¿æ€§æ— å…³çš„å‘é‡ $\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}$ ï¼Œæˆ‘ä»¬æƒ³è¦æ„å»ºä¸€ä¸ªå¼ æˆåŒä¸€å­ç©ºé—´çš„æ­£äº¤é›† $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\}$ ã€‚

We proceed step by step:
æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥ï¼š

1.  Start with $\mathbf{v}_1$, normalize it to get $\mathbf{u}_1$.

    ä» $\mathbf{v}_1$ å¼€å§‹ï¼Œå°†å…¶æ ‡å‡†åŒ–å¾—åˆ°$\mathbf{u}_1$â€‹ .
2.  Subtract from $\mathbf{v}_2$ its projection onto $\mathbf{u}_1$, leaving a vector orthogonal to $\mathbf{u}_1$. Normalize to get $\mathbf{u}_2$.

    ä» $\mathbf{v}_2$ ä¸­å‡å»å®ƒåœ¨$\mathbf{u}_1$ä¸Šçš„æŠ•å½±ï¼Œç•™ä¸‹ä¸€ä¸ªä¸ $\mathbf{u}_1$ æ­£äº¤çš„å‘é‡ï¼Œæ ‡å‡†åŒ–å¾—åˆ° $\mathbf{u}_2$â€‹ .
3.  For each $\mathbf{v}_k$, subtract projections onto all previously constructed $\mathbf{u}_1, \dots, \mathbf{u}_{k-1}$, then normalize.

    å¯¹äºæ¯ä¸ª$\mathbf{v}_k$ â€‹ ï¼Œå‡å»æ‰€æœ‰å…ˆå‰æ„å»ºçš„ğ‘¢ä¸Šçš„æŠ•å½± $\mathbf{u}_1, \dots, \mathbf{u}_{k-1}$ â€‹ ï¼Œç„¶åæ ‡å‡†åŒ–ã€‚

### The Algorithm

ç®—æ³•

For $k = 1, 2, \dots, n$:

å¯¹äº $k = 1, 2, \dots, n$ ï¼š

$$
\mathbf{w}_k = \mathbf{v}_k - \sum_{j=1}^{k-1} \langle \mathbf{v}_k, \mathbf{u}_j \rangle \mathbf{u}_j,
$$

 

$$
\mathbf{u}_k = \frac{\mathbf{w}_k}{\|\mathbf{w}_k\|}.
$$

The result $\{\mathbf{u}_1, \dots, \mathbf{u}_n\}$ is an orthonormal basis of the span of the original vectors.

ç»“æœ $\{\mathbf{u}_1, \dots, \mathbf{u}_n\}$ æ˜¯åŸå§‹å‘é‡è·¨åº¦çš„æ­£äº¤åŸºã€‚

### Example 7.3.1

ä¾‹ 7.3.1

Take $\mathbf{v}_1 = (1,1,0), \ \mathbf{v}_2 = (1,0,1), \ \mathbf{v}_3 = (0,1,1)$ in $\mathbb{R}^3$.

åœ¨ $\mathbb{R}^3$ ä¸­ $\mathbf{v}_1 = (1,1,0), \ \mathbf{v}_2 = (1,0,1), \ \mathbf{v}_3 = (0,1,1)$ ã€‚

1.  Normalize $\mathbf{v}_1$:

    æ ‡å‡†åŒ–$\mathbf{v}_1$â€‹ :

$$
\mathbf{u}_1 = \frac{1}{\sqrt{2}}(1,1,0).
$$

2.  Subtract projection of $\mathbf{v}_2$ on $\mathbf{u}_1$:

    å‡å» $\mathbf{v}_2$ åœ¨$\mathbf{u}_1$ çš„æŠ•å½± :

    $$
    \mathbf{w}_2 = \mathbf{v}_2 - \langle \mathbf{v}_2,\mathbf{u}_1 \rangle \mathbf{u}_1.
    $$

    

    $$
    \langle \mathbf{v}_2,\mathbf{u}_1 \rangle = \frac{1}{\sqrt{2}}(1\cdot 1 + 0\cdot 1 + 1\cdot 0) = \tfrac{1}{\sqrt{2}}.
    $$

    So

    æ‰€ä»¥

    $$
    \mathbf{w}_2 = (1,0,1) - \tfrac{1}{\sqrt{2}}\cdot \tfrac{1}{\sqrt{2}}(1,1,0)= (1,0,1) - \tfrac{1}{2}(1,1,0)= \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
    $$

    Normalize:

    è§„èŒƒåŒ–ï¼š

    $$
    \mathbf{u}_2 = \frac{1}{\sqrt{\tfrac{1}{4}+\tfrac{1}{4}+1}} \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)= \frac{1}{\sqrt{\tfrac{3}{2}}}\left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
    $$

3.  Subtract projections from $\mathbf{v}_3$:

    ä»$\mathbf{v}_3$ä¸­å‡å»æŠ•å½±:

    $$
    \mathbf{w}_3 = \mathbf{v}_3 - \langle \mathbf{v}_3,\mathbf{u}_1 \rangle \mathbf{u}_1 - \langle \mathbf{v}_3,\mathbf{u}_2 \rangle \mathbf{u}_2.
    $$

    After computing, normalize to obtain $\mathbf{u}_3$.

    è®¡ç®—åï¼Œå½’ä¸€åŒ–å¾—åˆ°$\mathbf{u}_3$â€‹ .

    The result is an orthonormal basis of the span of $\{\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3\}$.

    ç»“æœæ˜¯ $\{\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3\}$ å¼ æˆçš„æ­£äº¤åŸºã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Gramâ€“Schmidt is like straightening out a set of vectors: you start with the original directions and adjust each new vector to be perpendicular to all previous ones. Then you scale to unit length. The process ensures orthogonality while preserving the span.

æ ¼æ‹‰å§†-æ–½å¯†ç‰¹å˜æ¢å°±åƒæ‹‰ç›´ä¸€ç»„å‘é‡ï¼šä»åŸå§‹æ–¹å‘å¼€å§‹ï¼Œè°ƒæ•´æ¯ä¸ªæ–°å‘é‡ä½¿å…¶ä¸æ‰€æœ‰å…ˆå‰çš„å‘é‡å‚ç›´ã€‚ç„¶åç¼©æ”¾åˆ°å•ä½é•¿åº¦ã€‚è¿™ä¸ªè¿‡ç¨‹ç¡®ä¿äº†æ­£äº¤æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†è·¨åº¦ã€‚

### Why this matters

ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Orthonormal bases simplify inner products, projections, and computations in general. They make coordinate systems easier to work with and are crucial in numerical methods, QR decomposition, Fourier analysis, and statistics (orthogonal polynomials, principal component analysis).

æ­£äº¤åŸºå¯ä»¥ç®€åŒ–å†…ç§¯ã€æŠ•å½±å’Œä¸€èˆ¬è®¡ç®—ã€‚å®ƒä»¬ä½¿åæ ‡ç³»æ›´æ˜“äºä½¿ç”¨ï¼Œå¹¶ä¸”åœ¨æ•°å€¼æ–¹æ³•ã€QR åˆ†è§£ã€å‚…é‡Œå¶åˆ†æå’Œç»Ÿè®¡å­¦ï¼ˆæ­£äº¤å¤šé¡¹å¼ã€ä¸»æˆåˆ†åˆ†æï¼‰ä¸­è‡³å…³é‡è¦ã€‚

### Exercises 7.3
ç»ƒä¹  7.3

1.  Apply Gramâ€“Schmidt to $(1,0), (1,1)$ in $\mathbb{R}^2$.

    å¯¹ $\mathbb{R}^2$ ä¸­çš„ $(1,0), (1,1)$ åº”ç”¨ Gramâ€“Schmidt å…¬å¼ã€‚
2.  Orthogonalize $(1,1,1), (1,0,1)$ in $\mathbb{R}^3$.

    åœ¨ $\mathbb{R}^3$ ä¸­å¯¹ $(1,1,1), (1,0,1)$ è¿›è¡Œæ­£äº¤åŒ–ã€‚
3.  Prove that each step of Gramâ€“Schmidt yields a vector orthogonal to all previous ones.

    è¯æ˜ Gram-Schmidt çš„æ¯ä¸€æ­¥éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªä¸æ‰€æœ‰å‰é¢çš„å‘é‡æ­£äº¤çš„å‘é‡ã€‚
4.  Show that Gramâ€“Schmidt preserves the span of the original vectors.

    è¯æ˜ Gramâ€“Schmidt ä¿ç•™äº†åŸå§‹å‘é‡çš„è·¨åº¦ã€‚
5.  Explain how Gramâ€“Schmidt leads to the QR decomposition of a matrix.

    è§£é‡Š Gram-Schmidt å¦‚ä½•å¯¼è‡´çŸ©é˜µçš„ QR åˆ†è§£ã€‚

## 7.4 Orthonormal Bases
7.4 æ­£äº¤åŸº

An orthonormal basis is a basis of a vector space in which all vectors are both orthogonal to each other and have unit length. Such bases are the most convenient possible coordinate systems: computations involving inner products, projections, and norms become exceptionally simple.

æ­£äº¤åŸºæ˜¯å‘é‡ç©ºé—´ä¸­çš„ä¸€ç§åŸºï¼Œå…¶ä¸­æ‰€æœ‰å‘é‡å½¼æ­¤æ­£äº¤ä¸”å…·æœ‰å•ä½é•¿åº¦ã€‚è¿™æ ·çš„åŸºæ˜¯æœ€æ–¹ä¾¿çš„åæ ‡ç³»ï¼šæ¶‰åŠå†…ç§¯ã€æŠ•å½±å’ŒèŒƒæ•°çš„è®¡ç®—å˜å¾—å¼‚å¸¸ç®€å•ã€‚

### Definition
å®šä¹‰

A set of vectors $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\}$ in an inner product space $V$ is called an orthonormal basis if

å†…ç§¯ç©ºé—´ $V$ ä¸­çš„ä¸€ç»„å‘é‡ $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\}$ ç§°ä¸ºæ­£äº¤åŸºï¼Œè‹¥

1.  $\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0$ whenever $i \neq j$ (orthogonality),

    $\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0$ æ¯å½“ $i \neq j$ ï¼ˆæ­£äº¤æ€§ï¼‰
2.  $\|\mathbf{u}_i\| = 1$ for all $i$ (normalization),

    å¯¹æ‰€æœ‰ $i$ è¿›è¡Œ $\|\mathbf{u}_i\| = 1$ ï¼ˆè§„èŒƒåŒ–ï¼‰ï¼Œ
3.  The set spans $V$.

    è¯¥é›†åˆå¼ æˆ $V$ ã€‚

### Examples
ç¤ºä¾‹

Example 7.4.1. In $\mathbb{R}^2$, the standard basis

ä¾‹ 7.4.1. åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œæ ‡å‡†åŸºç¡€

$$
\mathbf{e}_1 = (1,0), \quad \mathbf{e}_2 = (0,1)
$$

is orthonormal under the dot product.

åœ¨ç‚¹ç§¯ä¸‹æ˜¯æ­£äº¤çš„ã€‚

Example 7.4.2. In $\mathbb{R}^3$, the standard basis

ä¾‹ 7.4.2. åœ¨ $\mathbb{R}^3$ ä¸­ï¼Œæ ‡å‡†åŸºç¡€

$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$

is orthonormal.

æ˜¯æ­£äº¤çš„ã€‚

Example 7.4.3. Fourier basis on functions:

ä¾‹ 7.4.3. å‡½æ•°çš„å‚…é‡Œå¶åŸºï¼š

$$
\{1, \cos x, \sin x, \cos 2x, \sin 2x, \dots\}
$$

is an orthogonal set in the space of square-integrable functions on $[-\pi,\pi]$ with inner product

æ˜¯ $[-\pi,\pi]$ ä¸Šå¹³æ–¹å¯ç§¯å‡½æ•°ç©ºé—´ä¸­çš„æ­£äº¤é›†ï¼Œå…·æœ‰å†…ç§¯

$$
\langle f,g \rangle = \int_{-\pi}^{\pi} f(x) g(x)\, dx.
$$

After normalization, it becomes an orthonormal basis.

ç»è¿‡å½’ä¸€åŒ–ä¹‹åï¼Œå®ƒå°±å˜æˆäº†æ­£äº¤åŸºã€‚

### Properties
ç‰¹æ€§

1.  Coordinate simplicity: If $\{\mathbf{u}_1,\dots,\mathbf{u}_n\}$ is an orthonormal basis of $V$, then any vector $\mathbf{v}\in V$ has coordinates

    åæ ‡ç®€å•æ€§ï¼šå¦‚æœ $\{\mathbf{u}_1,\dots,\mathbf{u}_n\}$ æ˜¯ $V$ çš„æ­£äº¤åŸºï¼Œåˆ™ä»»ä½•å‘é‡ $\mathbf{v}\in V$ éƒ½æœ‰åæ ‡
    
    $$
    [\mathbf{v}] = \begin{bmatrix} \langle \mathbf{v}, \mathbf{u}_1 \rangle \\ \vdots \\ \langle \mathbf{v}, \mathbf{u}_n \rangle \end{bmatrix}.
    $$
    
    That is, coordinates are just inner products.

    ä¹Ÿå°±æ˜¯è¯´ï¼Œåæ ‡åªæ˜¯å†…ç§¯ã€‚
    
2.  Parsevalâ€™s identity: For any $\mathbf{v} \in V$,

    å¸•å¡ç“¦å°”çš„ identityï¼š å¯¹äºä»»æ„çš„ $\mathbf{v} \in V$ ï¼Œ
    
    $$
    \|\mathbf{v}\|^2 = \sum_{i=1}^n |\langle \mathbf{v}, \mathbf{u}_i \rangle|^2.
    $$
    
3.  Projections: The orthogonal projection onto the span of $\mathbf{u}_1,\dots,\mathbf{u}_k$ is
    
    æŠ•å½±ï¼š $ \mathbf{u}_1,\dots,\mathbf{u}_k$ å¼ æˆçš„æ­£äº¤æŠ•å½±â€‹ æ˜¯
    
    $$
    \text{proj}(\mathbf{v}) = \sum_{i=1}^k \langle \mathbf{v}, \mathbf{u}_i \rangle \mathbf{u}_i.
    $$
    

### Constructing Orthonormal Bases
æ„é€ æ­£äº¤åŸº

*   Start with any linearly independent set, then apply the Gramâ€“Schmidt process to obtain an orthonormal set spanning the same subspace.

    ä»ä»»æ„çº¿æ€§æ— å…³é›†å¼€å§‹ï¼Œç„¶ååº”ç”¨ Gram-Schmidt è¿‡ç¨‹æ¥è·å–å¼ æˆç›¸åŒå­ç©ºé—´çš„æ­£äº¤é›†ã€‚

*   In practice, orthonormal bases are often chosen for numerical stability and simplicity of computation.

    åœ¨å®è·µä¸­ï¼Œé€šå¸¸é€‰æ‹©æ­£äº¤åŸºæ¥å®ç°æ•°å€¼ç¨³å®šæ€§å’Œè®¡ç®—ç®€å•æ€§ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

An orthonormal basis is like a perfectly aligned and equally scaled coordinate system. Distances and angles are computed directly using coordinates without correction factors. They are the ideal rulers of linear algebra.

æ­£äº¤åŸºå°±åƒä¸€ä¸ªå®Œç¾å¯¹é½ä¸”ç­‰æ¯”ä¾‹ç¼©æ”¾çš„åæ ‡ç³»ã€‚è·ç¦»å’Œè§’åº¦ç›´æ¥ä½¿ç”¨åæ ‡è®¡ç®—ï¼Œæ— éœ€æ ¡æ­£å› å­ã€‚å®ƒä»¬æ˜¯çº¿æ€§ä»£æ•°çš„ç†æƒ³æ ‡å°ºã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Orthonormal bases simplify every aspect of linear algebra: solving systems, computing projections, expanding functions, diagonalizing symmetric matrices, and working with Fourier series. In data science, principal component analysis produces orthonormal directions capturing maximum variance.

æ­£äº¤åŸºç®€åŒ–äº†çº¿æ€§ä»£æ•°çš„å„ä¸ªæ–¹é¢ï¼šæ±‚è§£ç³»ç»Ÿã€è®¡ç®—æŠ•å½±ã€å±•å¼€å‡½æ•°ã€å¯¹è§’åŒ–å¯¹ç§°çŸ©é˜µä»¥åŠå¤„ç†å‚…é‡Œå¶çº§æ•°ã€‚åœ¨æ•°æ®ç§‘å­¦ä¸­ï¼Œä¸»æˆåˆ†åˆ†æå¯ä»¥ç”Ÿæˆæ­£äº¤æ–¹å‘ï¼Œä»è€Œæ•æ‰æœ€å¤§æ–¹å·®ã€‚

### Exercises 7.4
ç»ƒä¹  7.4

1.  Verify that $(1/\sqrt{2})(1,1)$ and $(1/\sqrt{2})(1,-1)$ form an orthonormal basis of $\mathbb{R}^2$.

    éªŒè¯ $(1/\sqrt{2})(1,1)$ å’Œ $(1/\sqrt{2})(1,-1)$ æ˜¯å¦æ„æˆ $\mathbb{R}^2$ çš„æ­£äº¤åŸºã€‚
2.  Express $(3,4)$ in terms of the orthonormal basis $\{(1/\sqrt{2})(1,1), (1/\sqrt{2})(1,-1)\}$.
    
    ç”¨æ­£äº¤åŸº $\{(1/\sqrt{2})(1,1), (1/\sqrt{2})(1,-1)\}$ è¡¨ç¤º $(3,4)$ ã€‚
3.  Prove Parsevalâ€™s identity for $\mathbb{R}^n$ with the dot product.
    
    ä½¿ç”¨ç‚¹ç§¯è¯æ˜ $\mathbb{R}^n$ çš„å¸•å¡ç“¦å°”æ’ç­‰å¼ã€‚
4.  Find an orthonormal basis for the plane $x+y+z=0$ in $\mathbb{R}^3$.

    åœ¨ $\mathbb{R}^3$ ä¸­æ‰¾å‡ºå¹³é¢ $x+y+z=0$ çš„æ­£äº¤åŸºã€‚
5.  Explain why orthonormal bases are numerically more stable than arbitrary bases in computations.

    è§£é‡Šä¸ºä»€ä¹ˆæ­£äº¤åŸºåœ¨è®¡ç®—ä¸­æ¯”ä»»æ„åŸºåœ¨æ•°å€¼ä¸Šæ›´ç¨³å®šã€‚