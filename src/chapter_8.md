# Chapter 8. Eigenvalues and eigenvectors
ç¬¬ 8 ç«  ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡

## 8.1 Definitions and Intuition
8.1 å®šä¹‰å’Œç›´è§‰

The concepts of eigenvalues and eigenvectors reveal the most fundamental behavior of linear transformations. They identify the special directions in which a transformation acts by simple stretching or compressing, without rotation or distortion.
ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„æ¦‚å¿µæ­ç¤ºäº†çº¿æ€§å˜æ¢æœ€åŸºæœ¬çš„è¡Œä¸ºã€‚å®ƒä»¬é€šè¿‡ç®€å•çš„æ‹‰ä¼¸æˆ–å‹ç¼©ï¼ˆä¸è¿›è¡Œæ—‹è½¬æˆ–å˜å½¢ï¼‰æ¥è¯†åˆ«å˜æ¢æ‰€ä½œç”¨çš„ç‰¹å®šæ–¹å‘ã€‚

### Definition
å®šä¹‰

Let $T: V \to V$ be a linear transformation on a vector space $V$. A nonzero vector $\mathbf{v} \in V$ is called an eigenvector of $T$ if
ä»¤ $T: V \to V$ ä¸ºå‘é‡ç©ºé—´ $V$ ä¸Šçš„çº¿æ€§å˜æ¢ã€‚éé›¶å‘é‡ $\mathbf{v} \in V$ ç§°ä¸º $T$ çš„ç‰¹å¾å‘é‡ï¼Œè‹¥

$$
T(\mathbf{v}) = \lambda \mathbf{v}
$$

for some scalar $\lambda \in \mathbb{R}$ (or $\mathbb{C}$). The scalar $\lambda$ is the eigenvalue corresponding to $\mathbf{v}$.
æŸä¸ªæ ‡é‡ $\lambda \in \mathbb{R}$ ï¼ˆæˆ– $\mathbb{C}$ ï¼‰ã€‚æ ‡é‡ $\lambda$ æ˜¯å¯¹åº”äº $\mathbf{v}$ çš„ç‰¹å¾å€¼ã€‚

Equivalently, if $A$ is the matrix of $T$, then eigenvalues and eigenvectors satisfy
ç­‰ä»·åœ°ï¼Œå¦‚æœ $A$ æ˜¯ $T$ çš„çŸ©é˜µï¼Œåˆ™ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ»¡è¶³

$$
A\mathbf{v} = \lambda \mathbf{v}.
$$

### Basic Examples
åŸºæœ¬ç¤ºä¾‹

Example 8.1.1. Let
ä¾‹ 8.1.1. è®¾

$$
A = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}.
$$

Then
ç„¶å

$$
A(1,0)^T = 2(1,0)^T, \quad A(0,1)^T = 3(0,1)^T.
$$

So $(1,0)$ is an eigenvector with eigenvalue $2$, and $(0,1) is an eigenvector with eigenvalue \\3$.
å› æ­¤ $(1,0)$ æ˜¯ç‰¹å¾å€¼ä¸º $2 çš„ç‰¹å¾å‘é‡ï¼Œ $, and $ (0,1) æ˜¯ç‰¹å¾å€¼ä¸º \\ 3$ çš„ç‰¹å¾å‘é‡ ã€‚

Example 8.1.2. Rotation matrix in $\mathbb{R}^2$:
ä¾‹ 8.1.2ã€‚ $\mathbb{R}^2$ ä¸­çš„æ—‹è½¬çŸ©é˜µï¼š

$$
R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}.
$$

If $\theta \neq 0, \pi$, $R_\theta$ has no real eigenvalues: every vector is rotated, not scaled. Over $\mathbb{C}$, however, it has eigenvalues $e^{i\theta}, e^{-i\theta}$.
å¦‚æœ $\theta \neq 0, \pi$ ï¼Œğ‘… ğœƒ R Î¸ â€‹ æ²¡æœ‰å®æ•°ç‰¹å¾å€¼ï¼šæ¯ä¸ªå‘é‡éƒ½ç»è¿‡æ—‹è½¬ï¼Œè€Œä¸æ˜¯ç¼©æ”¾ã€‚ç„¶è€Œï¼Œåœ¨ $\mathbb{C}$ ä¸Šï¼Œå®ƒçš„ç‰¹å¾å€¼ä¸º $e^{i\theta}, e^{-i\theta}$ ã€‚

### Algebraic Formulation
ä»£æ•°å…¬å¼

Eigenvalues arise from solving the characteristic equation:
ç‰¹å¾å€¼ç”±æ±‚è§£ç‰¹å¾æ–¹ç¨‹å¾—å‡ºï¼š

$$
\det(A - \lambda I) = 0.
$$

This polynomial in $\lambda$ is the characteristic polynomial. Its roots are the eigenvalues.
$\lambda$ ä¸­çš„è¿™ä¸ªå¤šé¡¹å¼æ˜¯ç‰¹å¾å¤šé¡¹å¼ã€‚å®ƒçš„æ ¹å°±æ˜¯ç‰¹å¾å€¼ã€‚

### Geometric Intuition
å‡ ä½•ç›´è§‰

*   Eigenvectors are directions that remain unchanged in orientation under a transformation; only their length is scaled.
    ç‰¹å¾å‘é‡æ˜¯åœ¨å˜æ¢ä¸‹æ–¹å‘ä¿æŒä¸å˜çš„æ–¹å‘ï¼›åªæœ‰å®ƒä»¬çš„é•¿åº¦è¢«ç¼©æ”¾ã€‚
*   Eigenvalues tell us the scaling factor along those directions.
    ç‰¹å¾å€¼å‘Šè¯‰æˆ‘ä»¬æ²¿è¿™äº›æ–¹å‘çš„ç¼©æ”¾å› å­ã€‚
*   If a matrix has many independent eigenvectors, it can often be simplified (diagonalized) by changing basis.
    å¦‚æœçŸ©é˜µå…·æœ‰è®¸å¤šç‹¬ç«‹çš„ç‰¹å¾å‘é‡ï¼Œåˆ™é€šå¸¸å¯ä»¥é€šè¿‡æ”¹å˜åŸºæ¥ç®€åŒ–ï¼ˆå¯¹è§’åŒ–ï¼‰ã€‚

### Applications in Geometry and Science
å‡ ä½•å’Œç§‘å­¦ä¸­çš„åº”ç”¨

*   Stretching along principal axes of an ellipse (quadratic forms).
    æ²¿æ¤­åœ†çš„ä¸»è½´æ‹‰ä¼¸ï¼ˆäºŒæ¬¡å‹ï¼‰ã€‚
*   Stable directions of dynamical systems.
    åŠ¨åŠ›ç³»ç»Ÿçš„ç¨³å®šæ–¹å‘ã€‚
*   Principal components in statistics and machine learning.
    ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ä¸­çš„ä¸»è¦æˆåˆ†ã€‚
*   Quantum mechanics, where observables correspond to operators with eigenvalues.
    é‡å­åŠ›å­¦ï¼Œå…¶ä¸­å¯è§‚æµ‹é‡å¯¹åº”äºå…·æœ‰ç‰¹å¾å€¼çš„ç®—å­ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Eigenvalues and eigenvectors are a bridge between algebra and geometry. They provide a lens for understanding linear transformations in their simplest form. Nearly every application of linear algebra-differential equations, statistics, physics, computer science-relies on eigen-analysis.
ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ˜¯ä»£æ•°å’Œå‡ ä½•ä¹‹é—´çš„æ¡¥æ¢ã€‚å®ƒä»¬ä¸ºç†è§£æœ€ç®€å½¢å¼çš„çº¿æ€§å˜æ¢æä¾›äº†ä¸€ä¸ªè§†è§’ã€‚å‡ ä¹æ‰€æœ‰çº¿æ€§ä»£æ•°çš„åº”ç”¨â€”â€”å¾®åˆ†æ–¹ç¨‹ã€ç»Ÿè®¡å­¦ã€ç‰©ç†å­¦ã€è®¡ç®—æœºç§‘å­¦â€”â€”éƒ½ä¾èµ–äºç‰¹å¾åˆ†æã€‚

### Exercises 8.1
ç»ƒä¹  8.1

1.  Find the eigenvalues and eigenvectors of $\begin{bmatrix} 4 & 0 \\ 0 & -1 \end{bmatrix}$.
    æ‰¾åˆ°ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ $\begin{bmatrix} 4 & 0 \\ 0 & -1 \end{bmatrix}$ .
2.  Show that every scalar multiple of an eigenvector is again an eigenvector for the same eigenvalue.
    è¯æ˜ç‰¹å¾å‘é‡çš„æ¯ä¸ªæ ‡é‡å€æ•°åˆæ˜¯åŒä¸€ç‰¹å¾å€¼çš„ç‰¹å¾å‘é‡ã€‚
3.  Verify that the rotation matrix $R_\theta$ has no real eigenvalues unless $\theta = 0$ or $\pi$.
    éªŒè¯æ—‹è½¬çŸ©é˜µğ‘… ğœƒ R Î¸ â€‹ é™¤é $\theta = 0$ æˆ– $\pi$ ï¼Œå¦åˆ™æ²¡æœ‰å®æ•°ç‰¹å¾å€¼ã€‚
4.  Compute the characteristic polynomial of $\begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$.
    è®¡ç®—ç‰¹å¾å¤šé¡¹å¼ $\begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$ .
5.  Explain geometrically what eigenvectors and eigenvalues represent for the shear matrix $\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$.
    ä»å‡ ä½•è§’åº¦è§£é‡Šç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼å¯¹äºå‰ªåˆ‡çŸ©é˜µçš„æ„ä¹‰ $\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$ .

## 8.2 Diagonalization
8.2 å¯¹è§’åŒ–

A central goal in linear algebra is to simplify the action of a matrix by choosing a good basis. Diagonalization is the process of rewriting a matrix so that it acts by simple scaling along independent directions. This makes computations such as powers, exponentials, and solving differential equations far easier.
çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒç›®æ ‡æ˜¯é€šè¿‡é€‰æ‹©åˆé€‚çš„åŸºæ¥ç®€åŒ–çŸ©é˜µçš„è¿ç®—ã€‚å¯¹è§’åŒ–æ˜¯å°†çŸ©é˜µé‡å†™ï¼Œä½¿å…¶èƒ½å¤Ÿæ²¿ç‹¬ç«‹æ–¹å‘è¿›è¡Œç®€å•çš„ç¼©æ”¾ã€‚è¿™ä½¿å¾—å¹‚ã€æŒ‡æ•°å’Œå¾®åˆ†æ–¹ç¨‹ç­‰è®¡ç®—å˜å¾—æ›´åŠ å®¹æ˜“ã€‚

### Definition
å®šä¹‰

A square matrix $A \in \mathbb{R}^{n \times n}$ is diagonalizable if there exists an invertible matrix $P$ such that
å¦‚æœå­˜åœ¨å¯é€†çŸ©é˜µ $P$ å¹¶ä¸”æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™æ–¹é˜µ $A \in \mathbb{R}^{n \times n}$ å¯å¯¹è§’åŒ–

$$
P^{-1} A P = D,
$$

where $D$ is a diagonal matrix.
å…¶ä¸­ $D$ æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µã€‚

The diagonal entries of $D$ are eigenvalues of $A$, and the columns of $P$ are the corresponding eigenvectors.
$D$ çš„å¯¹è§’çº¿é¡¹æ˜¯ $A$ çš„ç‰¹å¾å€¼ï¼Œ $P$ çš„åˆ—æ˜¯ç›¸åº”çš„ç‰¹å¾å‘é‡ã€‚

### When is a Matrix Diagonalizable?
çŸ©é˜µä½•æ—¶å¯å¯¹è§’åŒ–ï¼Ÿ

*   A matrix is diagonalizable if it has $n$ linearly independent eigenvectors.
    å¦‚æœçŸ©é˜µå…·æœ‰ $n$ ä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ï¼Œåˆ™è¯¥çŸ©é˜µå¯å¯¹è§’åŒ–ã€‚
*   Equivalently, the sum of the dimensions of its eigenspaces equals $n$.
    ç­‰æ•ˆåœ°ï¼Œå…¶ç‰¹å¾ç©ºé—´çš„ç»´æ•°ä¹‹å’Œç­‰äº $n$ ã€‚
*   Symmetric matrices (over $\mathbb{R}$) are always diagonalizable, with an orthonormal basis of eigenvectors.
    å¯¹ç§°çŸ©é˜µï¼ˆåœ¨ $\mathbb{R}$ ä¸Šï¼‰å§‹ç»ˆå¯å¯¹è§’åŒ–ï¼Œä¸”å…·æœ‰ç‰¹å¾å‘é‡çš„æ­£äº¤åŸºã€‚

### Example 8.2.1
ä¾‹ 8.2.1

Let
è®©

$$
A = \begin{bmatrix} 4 & 1 \\ 0 & 2 \end{bmatrix}.
$$

1.  Characteristic polynomial:
    ç‰¹å¾å¤šé¡¹å¼ï¼š

$$
\det(A - \lambda I) = (4-\lambda)(2-\lambda).
$$

So eigenvalues are $\lambda_1 = 4$, $\lambda_2 = 2$.
æ‰€ä»¥ç‰¹å¾å€¼æ˜¯ $\lambda_1 = 4$ ï¼Œ $\lambda_2 = 2$ ã€‚

2.  Eigenvectors:
    ç‰¹å¾å‘é‡ï¼š

*   For $\lambda = 4$, solve $(A-4I)\mathbf{v}=0$: $\begin{bmatrix} 0 & 1 \\ 0 & -2 \end{bmatrix}\mathbf{v} = 0$, giving $\mathbf{v}_1 = (1,0)$.
    å¯¹äº $\lambda = 4$ ï¼Œæ±‚è§£ $(A-4I)\mathbf{v}=0$ ï¼š $\begin{bmatrix} 0 & 1 \\ 0 & -2 \end{bmatrix}\mathbf{v} = 0$ ï¼Œå¾—åˆ° $\mathbf{v}_1 = (1,0)$ ã€‚
*   For $\lambda = 2$: $(A-2I)\mathbf{v}=0$, giving $\mathbf{v}_2 = (1,-2)$.
    å¯¹äº $\lambda = 2$ ï¼š $(A-2I)\mathbf{v}=0$ ï¼Œç»™å‡º $\mathbf{v}_2 = (1,-2)$ ã€‚

3.  Construct $P = \begin{bmatrix} 1 & 1 \\ 0 & -2 \end{bmatrix}$. Then
    æ„é€  $P = \begin{bmatrix} 1 & 1 \\ 0 & -2 \end{bmatrix}$ ã€‚ç„¶å

$$
P^{-1} A P = \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix}.
$$

Thus, $A$ is diagonalizable.
å› æ­¤ï¼Œ $A$ æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚

### Why Diagonalize?
ä¸ºä»€ä¹ˆè¦å¯¹è§’åŒ–ï¼Ÿ

*   Computing powers: If $A = P D P^{-1}$, then
    è®¡ç®—èƒ½åŠ›ï¼š å¦‚æœ $A = P D P^{-1}$ ï¼Œåˆ™
    
    $$
    A^k = P D^k P^{-1}.
    $$
    
    Since $D$ is diagonal, $D^k$ is easy to compute.
    ç”±äº $D$ æ˜¯å¯¹è§’çº¿ï¼Œå› æ­¤ $D^k$ å¾ˆå®¹æ˜“è®¡ç®—ã€‚
    
*   Matrix exponentials: $e^A = P e^D P^{-1}$, useful in solving differential equations.
    çŸ©é˜µæŒ‡æ•°ï¼š $e^A = P e^D P^{-1}$ ï¼Œæœ‰åŠ©äºè§£å†³å¾®åˆ†æ–¹ç¨‹ã€‚
    
*   Understanding geometry: Diagonalization reveals the directions along which a transformation stretches or compresses space independently.
    ç†è§£å‡ ä½•ï¼šå¯¹è§’åŒ–æ­ç¤ºäº†å˜æ¢ç‹¬ç«‹æ‹‰ä¼¸æˆ–å‹ç¼©ç©ºé—´çš„æ–¹å‘ã€‚
    

### Non-Diagonalizable Example
ä¸å¯å¯¹è§’åŒ–çš„ä¾‹å­

Not all matrices can be diagonalized.
å¹¶éæ‰€æœ‰çŸ©é˜µéƒ½å¯ä»¥å¯¹è§’åŒ–ã€‚

$$
A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}
$$

has only one eigenvalue $\lambda = 1$, with eigenspace dimension 1. Since $n=2$ but we only have 1 independent eigenvector, $A$ is not diagonalizable.
åªæœ‰ä¸€ä¸ªç‰¹å¾å€¼ $\lambda = 1$ ï¼Œç‰¹å¾ç©ºé—´ç»´æ•°ä¸º 1ã€‚ç”±äº $n=2$ ä½†æˆ‘ä»¬åªæœ‰ 1 ä¸ªç‹¬ç«‹ç‰¹å¾å‘é‡ï¼Œå› æ­¤ $A$ ä¸å¯å¯¹è§’åŒ–ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Diagonalization means we have found a basis of eigenvectors. In this basis, the matrix acts by simple scaling along each coordinate axis. It transforms complicated motion into independent 1D motions.
å¯¹è§’åŒ–æ„å‘³ç€æˆ‘ä»¬æ‰¾åˆ°äº†ç‰¹å¾å‘é‡çš„åŸºã€‚åœ¨æ­¤åŸºä¸Šï¼ŒçŸ©é˜µé€šè¿‡æ²¿æ¯ä¸ªåæ ‡è½´è¿›è¡Œç®€å•çš„ç¼©æ”¾æ¥å‘æŒ¥ä½œç”¨ã€‚å®ƒå°†å¤æ‚çš„è¿åŠ¨è½¬åŒ–ä¸ºç‹¬ç«‹çš„ä¸€ç»´è¿åŠ¨ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Diagonalization is a cornerstone of linear algebra. It simplifies computation, reveals structure, and is the starting point for the spectral theorem, Jordan form, and many applications in physics, engineering, and data science.
å¯¹è§’åŒ–æ˜¯çº¿æ€§ä»£æ•°çš„åŸºçŸ³ã€‚å®ƒç®€åŒ–äº†è®¡ç®—ï¼Œæ­ç¤ºäº†ç»“æ„ï¼Œå¹¶ä¸”æ˜¯è°±å®šç†ã€è‹¥å°”å½“å½¢å¼ä»¥åŠç‰©ç†ã€å·¥ç¨‹å’Œæ•°æ®ç§‘å­¦ä¸­è®¸å¤šåº”ç”¨çš„èµ·ç‚¹ã€‚

### Exercises 8.2
ç»ƒä¹  8.2

1.  Diagonalize
    å¯¹è§’åŒ–
    
    $$
    A = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}.
    $$
    
2.  Determine whether
    ç¡®å®šæ˜¯å¦
    
    $$
    A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}
    $$
    
    is diagonalizable. Why or why not?
    æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚ä¸ºä»€ä¹ˆæˆ–ä¸ºä»€ä¹ˆä¸ï¼Ÿ
    
3.  Find $A^5$ for
    æŸ¥æ‰¾ $A^5$
    
    $$
    A = \begin{bmatrix} 4 & 1 \\ 0 & 2 \end{bmatrix}
    $$
    
    using diagonalization.
    ä½¿ç”¨å¯¹è§’åŒ–ã€‚
    
4.  Show that any $n \times n$ matrix with $n$ distinct eigenvalues is diagonalizable.
    è¯æ˜ä»»ä½•å…·æœ‰ $n$ ä¸ªä¸åŒç‰¹å¾å€¼çš„ $n \times n$ çŸ©é˜µéƒ½æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚
    
5.  Explain why real symmetric matrices are always diagonalizable.
    è§£é‡Šä¸ºä»€ä¹ˆå®å¯¹ç§°çŸ©é˜µæ€»æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚
    

## 8.3 Characteristic Polynomials
8.3 ç‰¹å¾å¤šé¡¹å¼

The key to finding eigenvalues is the characteristic polynomial of a matrix. This polynomial encodes the values of $\lambda$ for which the matrix $A - \lambda I$ fails to be invertible.
å¯»æ‰¾ç‰¹å¾å€¼çš„å…³é”®æ˜¯çŸ©é˜µçš„ç‰¹å¾å¤šé¡¹å¼ã€‚è¯¥å¤šé¡¹å¼å¯¹å€¼è¿›è¡Œç¼–ç  çŸ©é˜µ $A - \lambda I$ ä¸å¯é€†ï¼Œå…¶ä¸­ $\lambda$ ã€‚

### Definition
å®šä¹‰

For an $n \times n$ matrix $A$, the characteristic polynomial is
å¯¹äº $n \times n$ çŸ©é˜µ $A$ ï¼Œç‰¹å¾å¤šé¡¹å¼ä¸º

$$
p_A(\lambda) = \det(A - \lambda I).
$$

The roots of $p_A(\lambda)$ are the eigenvalues of $A$.
$p_A(\lambda)$ çš„æ ¹æ˜¯ $A$ çš„ç‰¹å¾å€¼ã€‚

### Examples
ç¤ºä¾‹

Example 8.3.1. Let
ä¾‹ 8.3.1. è®¾

$$
A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}.
$$

Then
ç„¶å

$$
p_A(\lambda) = \det\!\begin{bmatrix} 2-\lambda & 1 \\ 1 & 2-\lambda \end{bmatrix}= (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
$$

Thus eigenvalues are $\lambda = 1, 3$.
å› æ­¤ç‰¹å¾å€¼ä¸º $\lambda = 1, 3$ ã€‚

Example 8.3.2. For
ä¾‹ 8.3.2. å¯¹äº

$$
A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
$$

(rotation by 90Â°),
ï¼ˆæ—‹è½¬ 90Â°ï¼‰ï¼Œ

$$
p_A(\lambda) = \det\!\begin{bmatrix} -\lambda & -1 \\ 1 & -\lambda \end{bmatrix}= \lambda^2 + 1.
$$

Eigenvalues are $\lambda = \pm i$. No real eigenvalues exist, consistent with pure rotation.
ç‰¹å¾å€¼ä¸º $\lambda = \pm i$ ã€‚ä¸å­˜åœ¨å®æ•°ç‰¹å¾å€¼ï¼Œä¸çº¯æ—‹è½¬ä¸€è‡´ã€‚

Example 8.3.3. For a triangular matrix
ä¾‹ 8.3.3. å¯¹äºä¸‰è§’çŸ©é˜µ

$$
A = \begin{bmatrix} 2 & 1 & 0 \\ 0 & 3 & 5 \\ 0 & 0 & 4 \end{bmatrix},
$$

the determinant is simply the product of diagonal entries minus $\lambda$:
è¡Œåˆ—å¼ä»…ä»…æ˜¯å¯¹è§’çº¿é¡¹çš„ä¹˜ç§¯å‡å» $\lambda$ ï¼š

$$
p_A(\lambda) = (2-\lambda)(3-\lambda)(4-\lambda).
$$

So eigenvalues are 2,3,4.
æ‰€ä»¥ç‰¹å¾å€¼ä¸º 2,3,4 ã€‚

### Properties
ç‰¹æ€§

1.  The characteristic polynomial of an $n \times n$ matrix has degree $n$.
    $n \times n$ çŸ©é˜µçš„ç‰¹å¾å¤šé¡¹å¼çš„åº¦ä¸º $n$ ã€‚
    
2.  The sum of the eigenvalues (counted with multiplicity) equals the trace of $A$:
    ç‰¹å¾å€¼ï¼ˆæŒ‰é‡æ•°è®¡ç®—ï¼‰çš„å’Œç­‰äº $A$ çš„è¿¹ï¼š
    
    $$
    \text{tr}(A) = \lambda_1 + \cdots + \lambda_n.
    $$
    
3.  The product of the eigenvalues equals the determinant of $A$:
    ç‰¹å¾å€¼çš„ä¹˜ç§¯ç­‰äº $A$ çš„è¡Œåˆ—å¼ï¼š
    
    $$
    \det(A) = \lambda_1 \cdots \lambda_n.
    $$
    
4.  Similar matrices have the same characteristic polynomial, hence the same eigenvalues.
    ç›¸ä¼¼çš„çŸ©é˜µå…·æœ‰ç›¸åŒçš„ç‰¹å¾å¤šé¡¹å¼ï¼Œå› æ­¤å…·æœ‰ç›¸åŒçš„ç‰¹å¾å€¼ã€‚
    

### Geometric Interpretation
å‡ ä½•è§£é‡Š

The characteristic polynomial captures when $A - \lambda I$ collapses space: its determinant is zero precisely when the transformation $A - \lambda I$ is singular. Thus, eigenvalues mark the critical scalings where the matrix loses invertibility.
ç‰¹å¾å¤šé¡¹å¼æ•æ‰äº† $A - \lambda I$ ä½•æ—¶ä½¿ç©ºé—´åç¼©ï¼šå½“å˜æ¢ $A - \lambda I$ ä¸ºå¥‡å¼‚æ—¶ï¼Œå…¶è¡Œåˆ—å¼æ°å¥½ä¸ºé›¶ã€‚å› æ­¤ï¼Œç‰¹å¾å€¼æ ‡è®°äº†çŸ©é˜µå¤±å»å¯é€†æ€§çš„ä¸´ç•Œå°ºåº¦ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Characteristic polynomials provide the computational tool to extract eigenvalues. They connect matrix invariants (trace and determinant) with geometry, and form the foundation for diagonalization, spectral theorems, and stability analysis in dynamical systems.
ç‰¹å¾å¤šé¡¹å¼æä¾›äº†æå–ç‰¹å¾å€¼çš„è®¡ç®—å·¥å…·ã€‚å®ƒä»¬å°†çŸ©é˜µä¸å˜é‡ï¼ˆè¿¹å’Œè¡Œåˆ—å¼ï¼‰ä¸å‡ ä½•è”ç³»èµ·æ¥ï¼Œå¹¶æ„æˆäº†åŠ¨åŠ›ç³»ç»Ÿä¸­å¯¹è§’åŒ–ã€è°±å®šç†å’Œç¨³å®šæ€§åˆ†æçš„åŸºç¡€ã€‚

### Exercises 8.3
ç»ƒä¹  8.3

1.  Compute the characteristic polynomial of
    è®¡ç®—ç‰¹å¾å¤šé¡¹å¼
    
    $$
    A = \begin{bmatrix} 4 & 2 \\ 1 & 3 \end{bmatrix}.
    $$
    
2.  Verify that the sum of the eigenvalues of $\begin{bmatrix} 5 & 0 \\ 0 & -2 \end{bmatrix}$ equals its trace, and their product equals its determinant.
    éªŒè¯ç‰¹å¾å€¼ä¹‹å’Œ $\begin{bmatrix} 5 & 0 \\ 0 & -2 \end{bmatrix}$ ç­‰äºå®ƒçš„è¿¹ï¼Œå®ƒä»¬çš„ä¹˜ç§¯ç­‰äºå®ƒçš„è¡Œåˆ—å¼ã€‚
    
3.  Show that for any triangular matrix, the eigenvalues are just the diagonal entries.
    è¯æ˜å¯¹äºä»»ä½•ä¸‰è§’çŸ©é˜µï¼Œç‰¹å¾å€¼åªæ˜¯å¯¹è§’çº¿é¡¹ã€‚
    
4.  Prove that if $A$ and $B$ are similar matrices, then $p_A(\lambda) = p_B(\lambda)$.
    è¯æ˜å¦‚æœ $A$ å’Œ $B$ æ˜¯ç›¸ä¼¼çŸ©é˜µï¼Œåˆ™ $p_A(\lambda) = p_B(\lambda)$ ã€‚
    
5.  Compute the characteristic polynomial of $\begin{bmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix}$.
    è®¡ç®—ç‰¹å¾å¤šé¡¹å¼ \[ 1 1 0 0 1 1 0 0 1 \] â€‹ 1 0 0 â€‹ 1 1 0 â€‹ 0 1 1 â€‹ â€‹ .
    

## 8.4 Applications (Differential Equations, Markov Chains)
8.4 åº”ç”¨ï¼ˆå¾®åˆ†æ–¹ç¨‹ã€é©¬å°”å¯å¤«é“¾ï¼‰

Eigenvalues and eigenvectors are not only central to the theory of linear algebra-they are indispensable tools across mathematics and applied science. Two classic applications are solving systems of differential equations and analyzing Markov chains.
ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ä¸ä»…æ˜¯çº¿æ€§ä»£æ•°ç†è®ºçš„æ ¸å¿ƒï¼Œä¹Ÿæ˜¯æ•°å­¦å’Œåº”ç”¨ç§‘å­¦é¢†åŸŸä¸­ä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚ä¸¤ä¸ªç»å…¸çš„åº”ç”¨æ˜¯æ±‚è§£å¾®åˆ†æ–¹ç¨‹ç»„å’Œåˆ†æé©¬å°”å¯å¤«é“¾ã€‚

### Linear Differential Equations
çº¿æ€§å¾®åˆ†æ–¹ç¨‹

Consider the system
è€ƒè™‘ç³»ç»Ÿ

$$
\frac{d\mathbf{x}}{dt} = A \mathbf{x},
$$

where $A$ is an $n \times n$ matrix and $\mathbf{x}(t)$ is a vector-valued function.
å…¶ä¸­ $A$ æ˜¯ $n \times n$ çŸ©é˜µï¼Œ $\mathbf{x}(t)$ æ˜¯çŸ¢é‡å€¼å‡½æ•°ã€‚

If $\mathbf{v}$ is an eigenvector of $A$ with eigenvalue $\lambda$, then the function
å¦‚æœ $\mathbf{v}$ æ˜¯ $A$ çš„ç‰¹å¾å‘é‡ï¼Œå…¶ç‰¹å¾å€¼ä¸º $\lambda$ ï¼Œåˆ™å‡½æ•°

$$
\mathbf{x}(t) = e^{\lambda t}\mathbf{v}
$$

is a solution.
æ˜¯ä¸€ä¸ªè§£å†³æ–¹æ¡ˆã€‚

*   Eigenvalues determine the growth or decay rate:
    ç‰¹å¾å€¼å†³å®šå¢é•¿ç‡æˆ–è¡°å‡ç‡ï¼š
    
    *   If $\lambda < 0$, solutions decay (stable).
        å¦‚æœ $\lambda < 0$ ï¼Œåˆ™è§£å†³æ–¹æ¡ˆè¡°å‡ï¼ˆç¨³å®šï¼‰ã€‚
    *   If $\lambda > 0$, solutions grow (unstable).
        å¦‚æœ $\lambda > 0$ ï¼Œåˆ™è§£å†³æ–¹æ¡ˆä¼šå¢é•¿ï¼ˆä¸ç¨³å®šï¼‰ã€‚
    *   If $\lambda$ is complex, oscillations occur.
        å¦‚æœ $\lambda$ æ˜¯å¤æ•°ï¼Œåˆ™ä¼šå‘ç”ŸæŒ¯è¡ã€‚

By combining eigenvector solutions, we can solve general initial conditions.
é€šè¿‡ç»“åˆç‰¹å¾å‘é‡è§£ï¼Œæˆ‘ä»¬å¯ä»¥è§£å†³ä¸€èˆ¬çš„åˆå§‹æ¡ä»¶ã€‚

Example 8.4.1. Let
ä¾‹ 8.4.1. è®¾

$$
A = \begin{bmatrix}2 & 0 \\0 & -1 \end{bmatrix}.
$$

Then eigenvalues are $2, -1$with eigenvectors$(1,0)$, $(0,1)$. Solutions are
åˆ™ç‰¹å¾å€¼ä¸º $2, -1 $with eigenvectors$ (1,0) $, $ (0,1)$ã€‚è§£ä¸º

$$
\mathbf{x}(t) = c_1 e^{2t}(1,0) + c_2 e^{-t}(0,1).
$$

Thus one component grows exponentially, the other decays.
å› æ­¤ï¼Œä¸€ä¸ªéƒ¨åˆ†å‘ˆæŒ‡æ•°å¢é•¿ï¼Œå¦ä¸€ä¸ªéƒ¨åˆ†åˆ™è¡°å‡ã€‚

### Markov Chains
é©¬å°”å¯å¤«é“¾

A Markov chain is described by a stochastic matrix $P$, where each column sums to 1 and entries are nonnegative. If $\mathbf{x}_k$ represents the probability distribution after $k$ steps, then
é©¬å°”å¯å¤«é“¾å¯ä»¥ç”¨éšæœºçŸ©é˜µ $P$ æ¥æè¿°ï¼Œå…¶ä¸­æ¯åˆ—å’Œä¸º 1ï¼Œä¸”å…ƒç´ ä¸ºéè´Ÿå€¼ã€‚å¦‚æœ ğ‘¥ ğ‘˜ x k â€‹ è¡¨ç¤º $k$ æ­¥åçš„æ¦‚ç‡åˆ†å¸ƒï¼Œåˆ™

$$
\mathbf{x}_{k+1} = P \mathbf{x}_k.
$$

Iterating gives
è¿­ä»£å¾—åˆ°

$$
\mathbf{x}_k = P^k \mathbf{x}_0.
$$

Understanding long-term behavior reduces to analyzing powers of $P$.
ç†è§£é•¿æœŸè¡Œä¸ºå¯ä»¥å½’ç»“ä¸ºåˆ†æ $P$ çš„åŠ›é‡ã€‚

*   The eigenvalue $\lambda = 1$ always exists. Its eigenvector gives the steady-state distribution.
    ç‰¹å¾å€¼ $\lambda = 1$ å§‹ç»ˆå­˜åœ¨ã€‚å…¶ç‰¹å¾å‘é‡ç»™å‡ºäº†ç¨³æ€åˆ†å¸ƒã€‚
*   All other eigenvalues satisfy $|\lambda| \leq 1$. Their influence decays as $k \to \infty$.
    æ‰€æœ‰å…¶ä»–ç‰¹å¾å€¼éƒ½æ»¡è¶³ $|\lambda| \leq 1$ ã€‚å®ƒä»¬çš„å½±å“è¡°å‡ä¸º $k \to \infty$ ã€‚

Example 8.4.2. Consider
ä¾‹ 8.4.2. è€ƒè™‘

$$
P = \begin{bmatrix}0.9 & 0.5 \\0.1 & 0.5 \end{bmatrix}.
$$

Eigenvalues are $\lambda_1 = 1$, $\lambda_2 = 0.4$. The eigenvector for $\lambda = 1$ is proportional to $(5,1)$. Normalizing gives the steady state
ç‰¹å¾å€¼ä¸º $\lambda_1 = 1$ , $\lambda_2 = 0.4$ ã€‚ $\lambda = 1$ çš„ç‰¹å¾å‘é‡ä¸ $(5,1)$ æˆæ­£æ¯”ã€‚å½’ä¸€åŒ–åå¯å¾—åˆ°ç¨³æ€

$$
\pi = \left(\tfrac{5}{6}, \tfrac{1}{6}\right).
$$

Thus, regardless of the starting distribution, the chain converges to $\pi$.
å› æ­¤ï¼Œæ— è®ºèµ·å§‹åˆ†å¸ƒå¦‚ä½•ï¼Œé“¾éƒ½ä¼šæ”¶æ•›åˆ° $\pi$ ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   In differential equations, eigenvalues determine the time evolution: exponential growth, decay, or oscillation.
    åœ¨å¾®åˆ†æ–¹ç¨‹ä¸­ï¼Œç‰¹å¾å€¼å†³å®šæ—¶é—´çš„æ¼”å˜ï¼šæŒ‡æ•°å¢é•¿ã€è¡°å‡æˆ–æŒ¯è¡ã€‚
*   In Markov chains, eigenvalues determine the long-term equilibrium of stochastic processes.
    åœ¨é©¬å°”å¯å¤«é“¾ä¸­ï¼Œç‰¹å¾å€¼å†³å®šäº†éšæœºè¿‡ç¨‹çš„é•¿æœŸå‡è¡¡ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Eigenvalue methods turn complex iterative or dynamical systems into tractable problems. In physics, engineering, and finance, they describe stability and resonance. In computer science and statistics, they power algorithms from Googleâ€™s PageRank to modern machine learning.
ç‰¹å¾å€¼æ–¹æ³•å°†å¤æ‚çš„è¿­ä»£æˆ–åŠ¨æ€ç³»ç»Ÿè½¬åŒ–ä¸ºæ˜“äºå¤„ç†çš„é—®é¢˜ã€‚åœ¨ç‰©ç†å­¦ã€å·¥ç¨‹å­¦å’Œé‡‘èå­¦é¢†åŸŸï¼Œå®ƒä»¬æè¿°ç¨³å®šæ€§å’Œå…±æŒ¯ã€‚åœ¨è®¡ç®—æœºç§‘å­¦å’Œç»Ÿè®¡å­¦é¢†åŸŸï¼Œå®ƒä»¬ä¸ºä»è°·æ­Œçš„ PageRank åˆ°ç°ä»£æœºå™¨å­¦ä¹ ç­‰å„ç§ç®—æ³•æä¾›æ”¯æŒã€‚

### Exercises 8.4
ç»ƒä¹  8.4

1.  Solve $\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 & 0 \\ 0 & -2 \end{bmatrix}\mathbf{x}$.
    è§£å‡º $\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 & 0 \\ 0 & -2 \end{bmatrix}\mathbf{x}$ ã€‚
    
2.  Show that if $A$ has a complex eigenvalue $\alpha \pm i\beta$, then solutions of $\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}$ involve oscillations of frequency $\beta$.
    è¯æ˜å¦‚æœ $A$ å…·æœ‰å¤ç‰¹å¾å€¼ $\alpha \pm i\beta$ ï¼Œåˆ™ $\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}$ çš„è§£æ¶‰åŠé¢‘ç‡ $\beta$ çš„æŒ¯è¡ã€‚
    
3.  Find the steady-state distribution of
    æ‰¾åˆ°ç¨³æ€åˆ†å¸ƒ
    
    $$
    P = \begin{bmatrix} 0.7 & 0.2 \\ 0.3 & 0.8 \end{bmatrix}.
    $$
    
4.  Prove that for any stochastic matrix $P$, 1 is always an eigenvalue.
    è¯æ˜å¯¹äºä»»ä½•éšæœºçŸ©é˜µ $P$ ï¼Œ 1 å§‹ç»ˆæ˜¯ç‰¹å¾å€¼ã€‚
    
5.  Explain why all eigenvalues of a stochastic matrix satisfy $|\lambda| \leq 1$.
    è§£é‡Šä¸ºä»€ä¹ˆéšæœºçŸ©é˜µçš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ»¡è¶³ $|\lambda| \leq 1$ ã€‚
    