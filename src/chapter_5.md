# Chapter 5. Linear Transformations
ç¬¬ 5 ç« çº¿æ€§å˜æ¢

## 5.1 Functions that Preserve Linearity
5.1 ä¿æŒçº¿æ€§çš„å‡½æ•°

A central theme of linear algebra is understanding linear transformations: functions between vector spaces that preserve their algebraic structure. These transformations generalize the idea of matrix multiplication and capture the essence of linear behavior.

çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ç†è§£çº¿æ€§å˜æ¢ï¼šå‘é‡ç©ºé—´ä¹‹é—´ä¿æŒå…¶ä»£æ•°ç»“æ„çš„å‡½æ•°ã€‚è¿™äº›å˜æ¢æ¨å¹¿äº†çŸ©é˜µä¹˜æ³•çš„æ¦‚å¿µï¼Œå¹¶æŠ“ä½äº†çº¿æ€§è¡Œä¸ºçš„æœ¬è´¨ã€‚

### Definition
å®šä¹‰

Let $V$ and $W$ be vector spaces over $\mathbb{R}$. A function
ä»¤ $V$ å’Œ $W$ ä¸º $\mathbb{R}$ ä¸Šçš„å‘é‡ç©ºé—´ã€‚å‡½æ•°

$$
T : V \to W
$$

is called a linear transformation (or linear map) if for all vectors $\mathbf{u}, \mathbf{v} \in V$ and all scalars $c \in \mathbb{R}$:
å¦‚æœå¯¹äºæ‰€æœ‰å‘é‡ $\mathbf{u}, \mathbf{v} \in V$ å’Œæ‰€æœ‰æ ‡é‡ $c \in \mathbb{R}$ ï¼Œåˆ™ç§°ä¸ºçº¿æ€§å˜æ¢ï¼ˆæˆ–çº¿æ€§æ˜ å°„ï¼‰ï¼š

1.  Additivity:
    åŠ æ€§ï¼š

$$
T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}),
$$

2.  Homogeneity:
    åŒè´¨æ€§ï¼š

$$
T(c\mathbf{u}) = cT(\mathbf{u}).
$$

If both conditions hold, then $T$ automatically respects linear combinations:
å¦‚æœä¸¤ä¸ªæ¡ä»¶éƒ½æˆç«‹ï¼Œåˆ™ $T$ è‡ªåŠ¨éµå¾ªçº¿æ€§ç»„åˆï¼š

$$
T(c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k) = c_1 T(\mathbf{v}_1) + \cdots + c_k T(\mathbf{v}_k).
$$

### Examples
ç¤ºä¾‹

Example 5.1.1. Scaling in $\mathbb{R}^2$. Let $T:\mathbb{R}^2 \to \mathbb{R}^2$ be defined by
ä¾‹ 5.1.1. ç¼©æ”¾ $\mathbb{R}^2$ ã€‚ä»¤ $T:\mathbb{R}^2 \to \mathbb{R}^2$ å®šä¹‰ä¸º

$$
T(x,y) = (2x, 2y).
$$

This doubles the length of every vector, preserving direction. It is linear.
è¿™ä¼šä½¿æ¯ä¸ªå‘é‡çš„é•¿åº¦åŠ å€ï¼ŒåŒæ—¶ä¿æŒæ–¹å‘ä¸å˜ã€‚å®ƒæ˜¯çº¿æ€§çš„ã€‚

Example 5.1.2. Rotation.
ä¾‹ 5.1.2. æ—‹è½¬ã€‚

Let $R_\theta: \mathbb{R}^2 \to \mathbb{R}^2$ be
ä»¤ $R_\theta: \mathbb{R}^2 \to \mathbb{R}^2$ ä¸º

$$
R_\theta(x,y) = (x\cos\theta - y\sin\theta, \; x\sin\theta + y\cos\theta).
$$

This rotates vectors by angle $\theta$. It satisfies additivity and homogeneity, hence is linear.
è¿™å°†å‘é‡æ—‹è½¬è§’åº¦ $\theta$ ã€‚å®ƒæ»¡è¶³å¯åŠ æ€§å’Œé½æ¬¡æ€§ï¼Œå› æ­¤æ˜¯çº¿æ€§çš„ã€‚

Example 5.1.3. Differentiation.
ä¾‹ 5.1.3. åŒºåˆ†ã€‚

Let $D: \mathbb{R}[x] \to \mathbb{R}[x]$ be differentiation: $D(p(x)) = p'(x)$.
ä»¤ $D: \mathbb{R}[x] \to \mathbb{R}[x]$ ä¸ºå¾®åˆ†ï¼š $D(p(x)) = p'(x)$ ã€‚

Since derivatives respect addition and scalar multiples, differentiation is a linear transformation.
ç”±äºå¯¼æ•°å°Šé‡åŠ æ³•å’Œæ ‡é‡å€æ•°ï¼Œå› æ­¤å¾®åˆ†æ˜¯ä¸€ç§çº¿æ€§å˜æ¢ã€‚

### Non-Example
éç¤ºä¾‹

The map $S:\mathbb{R}^2 \to \mathbb{R}^2$ defined by
åœ°å›¾ $S:\mathbb{R}^2 \to \mathbb{R}^2$ å®šä¹‰ä¸º

$$
S(x,y) = (x^2, y^2)
$$

is not linear, because $S(\mathbf{u} + \mathbf{v}) \neq S(\mathbf{u}) + S(\mathbf{v})$ in general.
ä¸æ˜¯çº¿æ€§çš„ï¼Œå› ä¸ºä¸€èˆ¬æ¥è¯´ $S(\mathbf{u} + \mathbf{v}) \neq S(\mathbf{u}) + S(\mathbf{v})$ ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Linear transformations are exactly those that preserve the origin, lines through the origin, and proportions along those lines. They include familiar operations: scaling, rotations, reflections, shears, and projections. Nonlinear transformations bend or curve space, breaking these properties.
çº¿æ€§å˜æ¢æ­£æ˜¯é‚£äº›ä¿ç•™åŸç‚¹ã€è¿‡åŸç‚¹çš„ç›´çº¿ä»¥åŠæ²¿è¿™äº›ç›´çº¿çš„æ¯”ä¾‹çš„å˜æ¢ã€‚å®ƒä»¬åŒ…æ‹¬æˆ‘ä»¬ç†Ÿæ‚‰çš„æ“ä½œï¼šç¼©æ”¾ã€æ—‹è½¬ã€åå°„ã€å‰ªåˆ‡å’ŒæŠ•å½±ã€‚éçº¿æ€§å˜æ¢ä¼šå¼¯æ›²ç©ºé—´ï¼Œä»è€Œç ´åè¿™äº›å±æ€§ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Linear transformations unify geometry, algebra, and computation. They explain how matrices act on vectors, how data can be rotated or projected, and how systems evolve under linear rules. Much of linear algebra is devoted to understanding these transformations, their representations, and their invariants.
çº¿æ€§å˜æ¢ç»Ÿä¸€äº†å‡ ä½•ã€ä»£æ•°å’Œè®¡ç®—ã€‚å®ƒè§£é‡Šäº†çŸ©é˜µå¦‚ä½•ä½œç”¨äºå‘é‡ï¼Œæ•°æ®å¦‚ä½•æ—‹è½¬æˆ–æŠ•å½±ï¼Œä»¥åŠç³»ç»Ÿå¦‚ä½•åœ¨çº¿æ€§è§„åˆ™ä¸‹æ¼”åŒ–ã€‚çº¿æ€§ä»£æ•°çš„å¤§éƒ¨åˆ†å†…å®¹è‡´åŠ›äºç†è§£è¿™äº›å˜æ¢ã€å®ƒä»¬çš„è¡¨ç¤ºåŠå…¶ä¸å˜é‡ã€‚

### Exercises 5.1
ç»ƒä¹  5.1

1.  Verify that $T(x,y) = (3x-y, 2y)$ is a linear transformation on $\mathbb{R}^2$.
    éªŒè¯ $T(x,y) = (3x-y, 2y)$ æ˜¯å¦æ˜¯ $\mathbb{R}^2$ çš„çº¿æ€§å˜æ¢ã€‚
2.  Show that $T(x,y) = (x+1, y)$ is not linear. Which axiom fails?
    è¯æ˜ $T(x,y) = (x+1, y)$ ä¸æ˜¯çº¿æ€§çš„ã€‚å“ªæ¡å…¬ç†ä¸æˆç«‹ï¼Ÿ
3.  Prove that if $T$ and $S$ are linear transformations, then so is $T+S$.
    è¯æ˜å¦‚æœ $T$ å’Œ $S$ æ˜¯çº¿æ€§å˜æ¢ï¼Œé‚£ä¹ˆ $T+S$ ä¹Ÿæ˜¯çº¿æ€§å˜æ¢ã€‚
4.  Give an example of a linear transformation from $\mathbb{R}^3$ to $\mathbb{R}^2$.
    ç»™å‡ºä¸€ä¸ªä» $\mathbb{R}^3$ åˆ° $\mathbb{R}^2$ çš„çº¿æ€§å˜æ¢çš„ä¾‹å­ã€‚
5.  Let $T:\mathbb{R}[x] \to \mathbb{R}[x]$ be integration:
    ä»¤ $T:\mathbb{R}[x] \to \mathbb{R}[x]$ ä¸ºç§¯åˆ†ï¼š

$$
T(p(x)) = \int_0^x p(t)\\,dt.
$$

Prove that $T$ is a linear transformation.
è¯æ˜ $T$ æ˜¯çº¿æ€§å˜æ¢ã€‚

## 5.2 Matrix Representation of Linear Maps
5.2 çº¿æ€§æ˜ å°„çš„çŸ©é˜µè¡¨ç¤º

Every linear transformation between finite-dimensional vector spaces can be represented by a matrix. This correspondence is one of the central insights of linear algebra: it lets us use the tools of matrix arithmetic to study abstract transformations.
æœ‰é™ç»´å‘é‡ç©ºé—´ä¹‹é—´çš„æ‰€æœ‰çº¿æ€§å˜æ¢éƒ½å¯ä»¥ç”¨çŸ©é˜µè¡¨ç¤ºã€‚è¿™ç§å¯¹åº”å…³ç³»æ˜¯çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒæ´è§ä¹‹ä¸€ï¼šå®ƒè®©æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨çŸ©é˜µè¿ç®—å·¥å…·æ¥ç ”ç©¶æŠ½è±¡çš„å˜æ¢ã€‚

### From Linear Map to Matrix
ä»çº¿æ€§æ˜ å°„åˆ°çŸ©é˜µ

Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be a linear transformation. Choose the standard basis $\{ \mathbf{e}_1, \dots, \mathbf{e}_n \}$ of $\mathbb{R}^n$, where $\mathbf{e}_i$ has a 1 in the $i$\-th position and 0 elsewhere.
ä»¤ $T: \mathbb{R}^n \to \mathbb{R}^m$ ä¸ºçº¿æ€§å˜æ¢ã€‚é€‰å– $\mathbb{R}^n$ çš„æ ‡å‡†åŸº $\{ \mathbf{e}_1, \dots, \mathbf{e}_n \}$ ï¼Œå…¶ä¸­ ğ‘’ ğ‘– e i â€‹ ç¬¬ $i$ ä¸ªä½ç½®ä¸º 1ï¼Œå…¶ä»–åœ°æ–¹ä¸º 0ã€‚

The action of $T$ on each basis vector determines the entire transformation:
$T$ å¯¹æ¯ä¸ªåŸºå‘é‡çš„ä½œç”¨å†³å®šäº†æ•´ä¸ªå˜æ¢ï¼š

$$
T(\mathbf{e}\_j) = \begin{bmatrix}a_{1j} \\a_{2j} \\\vdots \\a_{mj} \end{bmatrix}.
$$

Placing these outputs as columns gives the matrix of $T$:
å°†è¿™äº›è¾“å‡ºä½œä¸ºåˆ—æ”¾ç½®ï¼Œå¾—åˆ°çŸ©é˜µ $T$ ï¼š

$$
[T] = A = \begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n} \\a_{21} & a_{22} & \cdots & a_{2n} \\\vdots & \vdots & \ddots & \vdots \\a_{m1} & a_{m2} & \cdots & a_{mn}\end{bmatrix}.
$$

Then for any vector $\mathbf{x} \in \mathbb{R}^n$:
ç„¶åå¯¹äºä»»æ„å‘é‡ $\mathbf{x} \in \mathbb{R}^n$ ï¼š

$$
T(\mathbf{x}) = A\mathbf{x}.
$$

### Examples
ç¤ºä¾‹

Example 5.2.1. Scaling in $\mathbb{R}^2$. Let $T(x,y) = (2x, 3y)$. Then
ä¾‹ 5.2.1. ç¼©æ”¾ $\mathbb{R}^2$ ã€‚è®¾ $T(x,y) = (2x, 3y)$ ã€‚ç„¶å

$$
T(\mathbf{e}_1) = (2,0), \quad T(\mathbf{e}_2) = (0,3).
$$

So the matrix is
æ‰€ä»¥çŸ©é˜µæ˜¯

$$
[T] = \begin{bmatrix}2 & 0 \\0 & 3\end{bmatrix}.
$$

Example 5.2.2. Rotation in the plane. The rotation transformation $R_\theta(x,y) = (x\cos\theta - y\sin\theta, \; x\sin\theta + y\cos\theta)$ has matrix
ä¾‹5.2.2. å¹³é¢æ—‹è½¬ã€‚ æ—‹è½¬å˜æ¢ $R_\theta(x,y) = (x\cos\theta - y\sin\theta, \; x\sin\theta + y\cos\theta)$ å…·æœ‰çŸ©é˜µ

$$
[R_\theta] = \begin{bmatrix}\cos\theta & -\sin\theta \\\sin\theta & \cos\theta\end{bmatrix}.
$$

Example 5.2.3. Projection onto the x-axis. The map $P(x,y) = (x,0)$ corresponds to
ä¾‹ 5.2.3. æŠ•å½±åˆ° x è½´ã€‚ åœ°å›¾ $P(x,y) = (x,0)$ å¯¹åº”äº

$$
[P] = \begin{bmatrix}1 & 0 \\0 & 0\end{bmatrix}.
$$

### Change of Basis
åŸºç¡€å˜æ›´

Matrix representations depend on the chosen basis. If $\mathcal{B}$ and $\mathcal{C}$ are bases of $\mathbb{R}^n$ and $\mathbb{R}^m$, then the matrix of $T: \mathbb{R}^n \to \mathbb{R}^m$ with respect to these bases is obtained by expressing $T(\mathbf{v}_j)$ in terms of $\mathcal{C}$ for each $\mathbf{v}_j \in \mathcal{B}$. Changing bases corresponds to conjugating the matrix by the appropriate change-of-basis matrices.
çŸ©é˜µè¡¨ç¤ºå–å†³äºæ‰€é€‰çš„åŸºã€‚å¦‚æœ $\mathcal{B}$ å’Œ $\mathcal{C}$ æ˜¯ $\mathbb{R}^n$ çš„åŸº å’Œ $\mathbb{R}^m$ ï¼Œåˆ™ $T: \mathbb{R}^n \to \mathbb{R}^m$ å…³äºè¿™äº›åŸºçš„çŸ©é˜µï¼Œå¯ä»¥é€šè¿‡å°† $T(\mathbf{v}_j)$ è¡¨ç¤ºä¸º $\mathcal{C}$ æ¥è·å¾—ï¼Œå…¶ä¸­ $\mathbf{v}_j \in \mathcal{B}$ è¡¨ç¤ºä¸º $T(\mathbf{v}_j)$ã€‚æ”¹å˜åŸºç›¸å½“äºå°†çŸ©é˜µä¸é€‚å½“çš„åŸºå˜æ¢çŸ©é˜µå…±è½­ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Matrices are not just convenient notation-they *are* linear maps once a basis is fixed. Every rotation, reflection, projection, shear, or scaling corresponds to multiplying by a specific matrix. Thus, studying linear transformations reduces to studying their matrices.
çŸ©é˜µä¸ä»…ä»…æ˜¯æ–¹ä¾¿çš„ç¬¦å·â€”â€”ä¸€æ—¦åŸºç¡®å®šï¼Œå®ƒä»¬*å°±æ˜¯*çº¿æ€§æ˜ å°„ã€‚æ‰€æœ‰æ—‹è½¬ã€åå°„ã€æŠ•å½±ã€å‰ªåˆ‡æˆ–ç¼©æ”¾éƒ½å¯¹åº”äºä¹˜ä»¥ä¸€ä¸ªç‰¹å®šçš„çŸ©é˜µã€‚å› æ­¤ï¼Œç ”ç©¶çº¿æ€§å˜æ¢å¯ä»¥å½’ç»“ä¸ºç ”ç©¶å®ƒä»¬çš„çŸ©é˜µã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Matrix representations make linear transformations computable. They connect abstract definitions to explicit calculations, enabling algorithms for solving systems, finding eigenvalues, and performing decompositions. Applications from graphics to machine learning depend on this translation.
çŸ©é˜µè¡¨ç¤ºä½¿çº¿æ€§å˜æ¢å¯è®¡ç®—ã€‚å®ƒä»¬å°†æŠ½è±¡å®šä¹‰ä¸æ˜ç¡®çš„è®¡ç®—è”ç³»èµ·æ¥ï¼Œä»è€Œæ”¯æŒæ±‚è§£ç³»ç»Ÿã€æŸ¥æ‰¾ç‰¹å¾å€¼å’Œæ‰§è¡Œåˆ†è§£çš„ç®—æ³•ã€‚ä»å›¾å½¢åˆ°æœºå™¨å­¦ä¹ ç­‰å„ç§åº”ç”¨éƒ½ä¾èµ–äºè¿™ç§è½¬æ¢ã€‚

### Exercises 5.2
ç»ƒä¹  5.2

1.  Find the matrix representation of $T:\mathbb{R}^2 \to \mathbb{R}^2$, $T(x,y) = (x+y, x-y)$.
    æ‰¾åˆ° $T:\mathbb{R}^2 \to \mathbb{R}^2$ , $T(x,y) = (x+y, x-y)$ çš„çŸ©é˜µè¡¨ç¤ºã€‚
2.  Determine the matrix of the linear transformation $T:\mathbb{R}^3 \to \mathbb{R}^2$, $T(x,y,z) = (x+z, y-2z)$.
    ç¡®å®šçº¿æ€§å˜æ¢çŸ©é˜µ $T:\mathbb{R}^3 \to \mathbb{R}^2$ ï¼Œ $T(x,y,z) = (x+z, y-2z)$ ã€‚
3.  What matrix represents reflection across the line $y=x$ in $\mathbb{R}^2$?
    å“ªä¸ªçŸ©é˜µè¡¨ç¤º $\mathbb{R}^2$ ä¸­æ²¿çº¿ $y=x$ çš„åå°„ï¼Ÿ
4.  Show that the matrix of the identity transformation on $\mathbb{R}^n$ is $I_n$.
    è¯æ˜ $\mathbb{R}^n$ ä¸Šçš„æ’ç­‰å˜æ¢çŸ©é˜µæ˜¯ ğ¼ ğ‘› I n â€‹ .
5.  For the differentiation map $D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]$, where $\mathbb{R}_k[x]$ is the space of polynomials of degree at most $k$, find the matrix of $D$ relative to the bases $\{1,x,x^2\}$ and $\{1,x\}$.
    å¯¹äºå¾®åˆ†æ˜ å°„ $D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]$ ï¼Œå…¶ä¸­ $\mathbb{R}_k[x]$ æ˜¯æ¬¡æ•°æœ€å¤šä¸º $k$ çš„å¤šé¡¹å¼ç©ºé—´ï¼Œæ±‚å‡º $D$ ç›¸å¯¹äºåŸºæ•° $\{1,x,x^2\}$ å’Œ $\{1,x\}$ çš„çŸ©é˜µã€‚

## 5.3 Kernel and Image
5.3 å†…æ ¸å’Œé•œåƒ

To understand a linear transformation deeply, we must examine what it kills and what it produces. These ideas are captured by the kernel and the image, two fundamental subspaces associated with any linear map.
è¦æ·±å…¥ç†è§£çº¿æ€§å˜æ¢ï¼Œæˆ‘ä»¬å¿…é¡»è€ƒå¯Ÿå®ƒæ¶ˆé™¤äº†ä»€ä¹ˆï¼Œåˆäº§ç”Ÿäº†ä»€ä¹ˆã€‚è¿™äº›æ¦‚å¿µå¯ä»¥é€šè¿‡æ ¸å’Œåƒæ¥ç†è§£ï¼Œå®ƒä»¬æ˜¯ä»»ä½•çº¿æ€§æ˜ å°„éƒ½ç›¸å…³çš„ä¸¤ä¸ªåŸºæœ¬å­ç©ºé—´ã€‚

### The Kernel
å†…æ ¸

The kernel (or null space) of a linear transformation $T: V \to W$ is the set of all vectors in $V$ that map to the zero vector in $W$:
çº¿æ€§å˜æ¢ $T: V \to W$ çš„æ ¸ï¼ˆæˆ–é›¶ç©ºé—´ï¼‰æ˜¯ $V$ ä¸­æ˜ å°„åˆ° $W$ ä¸­çš„é›¶å‘é‡çš„æ‰€æœ‰å‘é‡çš„é›†åˆï¼š

$$
\ker(T) = \{ \mathbf{v} \in V \mid T(\mathbf{v}) = \mathbf{0} \}.
$$

The kernel is always a subspace of $V$. It measures the degeneracy of the transformation-directions that collapse to nothing.
æ ¸å§‹ç»ˆæ˜¯ $V$ çš„å­ç©ºé—´ã€‚å®ƒè¡¡é‡çš„æ˜¯åç¼©ä¸ºé›¶çš„å˜æ¢æ–¹å‘çš„é€€åŒ–ç¨‹åº¦ã€‚

Example 5.3.1. Let $T:\mathbb{R}^3 \to \mathbb{R}^2$ be defined by
ä¾‹ 5.3.1ã€‚ è®© $T:\mathbb{R}^3 \to \mathbb{R}^2$ å®šä¹‰ä¸º

$$
T(x,y,z) = (x+y, y+z).
$$

In matrix form,
ä»¥çŸ©é˜µå½¢å¼ï¼Œ

$$
[T] = \begin{bmatrix}1 & 1 & 0 \\0 & 1 & 1\end{bmatrix}.
$$

To find the kernel, solve
è¦æ‰¾åˆ°å†…æ ¸ï¼Œè¯·è§£å†³

$$
\begin{bmatrix}1 & 1 & 0 \\0 & 1 & 1\end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix}= \begin{bmatrix} 0 \\ 0 \end{bmatrix}.
$$

This gives the equations $x + y = 0$, $y + z = 0$. Hence $x = -y, z = -y$. The kernel is
ç”±æ­¤å¾—åˆ°æ–¹ç¨‹ $x + y = 0$ ï¼Œ $y + z = 0$ ã€‚å› æ­¤ $x = -y, z = -y$ ã€‚æ ¸å‡½æ•°ä¸º

$$
\ker(T) = \{ (-t, t, -t) \mid t \in \mathbb{R} \},
$$

a line in $\mathbb{R}^3$.
$\mathbb{R}^3$ ä¸­çš„ä¸€è¡Œã€‚

### The Image
å›¾åƒ

The image (or range) of a linear transformation $T: V \to W$ is the set of all outputs:
çº¿æ€§å˜æ¢ $T: V \to W$ çš„å›¾åƒï¼ˆæˆ–èŒƒå›´ï¼‰æ˜¯æ‰€æœ‰è¾“å‡ºçš„é›†åˆï¼š

$$
\text{im}(T) = \{ T(\mathbf{v}) \mid \mathbf{v} \in V \} \subseteq W.
$$

Equivalently, it is the span of the columns of the representing matrix. The image is always a subspace of $W$.
ç­‰æ•ˆåœ°ï¼Œå®ƒæ˜¯è¡¨ç¤ºçŸ©é˜µçš„åˆ—çš„è·¨åº¦ã€‚å›¾åƒå§‹ç»ˆæ˜¯ $W$ çš„å­ç©ºé—´ã€‚

Example 5.3.2. For the same transformation as above,
ä¾‹ 5.3.2. å¯¹äºä¸ä¸Šè¿°ç›¸åŒçš„å˜æ¢ï¼Œ

$$
[T] = \begin{bmatrix}1 & 1 & 0 \\0 & 1 & 1\end{bmatrix},
$$

the columns are $(1,0)$, $(1,1)$, and $(0,1)$. Since $(1,1) = (1,0) + (0,1)$, the image is
åˆ—ä¸º $(1,0)$ ã€ $(1,1)$ å’Œ $(0,1)$ ã€‚ç”±äº $(1,1) = (1,0) + (0,1)$ ï¼Œå› æ­¤å›¾åƒä¸º

$$
\text{im}(T) = \text{span}\{ (1,0), (0,1) \} = \mathbb{R}^2.
$$

### Dimension Formula (Rankâ€“Nullity Theorem)
ç»´åº¦å…¬å¼ï¼ˆç§©-é›¶åº¦å®šç†ï¼‰

For a linear transformation $T: V \to W$ with $V$ finite-dimensional,
å¯¹äºçº¿æ€§å˜æ¢ $T: V \to W$ ä¸” $V$ ä¸ºæœ‰é™ç»´ï¼Œ

$$
\dim(\ker(T)) + \dim(\text{im}(T)) = \dim(V).
$$

This fundamental result connects the lost directions (kernel) with the achieved directions (image).
è¿™ä¸ªåŸºæœ¬ç»“æœå°†ä¸¢å¤±çš„æ–¹å‘ï¼ˆå†…æ ¸ï¼‰ä¸å®ç°çš„æ–¹å‘ï¼ˆå›¾åƒï¼‰è”ç³»èµ·æ¥ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   The kernel describes how the transformation flattens space (e.g., projecting a 3D object onto a plane).
    å†…æ ¸æè¿°äº†å˜æ¢å¦‚ä½•ä½¿ç©ºé—´å˜å¹³å¦ï¼ˆä¾‹å¦‚ï¼Œå°† 3D å¯¹è±¡æŠ•å½±åˆ°å¹³é¢ä¸Šï¼‰ã€‚
*   The image describes the target subspace reached by the transformation.
    è¯¥å›¾åƒæè¿°äº†å˜æ¢æ‰€è¾¾åˆ°çš„ç›®æ ‡å­ç©ºé—´ã€‚
*   The rankâ€“nullity theorem quantifies the tradeoff: the more dimensions collapse, the fewer remain in the image.
    ç§©é›¶å®šç†é‡åŒ–äº†è¿™ç§æƒè¡¡ï¼šç»´åº¦å´©æºƒå¾—è¶Šå¤šï¼Œå›¾åƒä¸­å‰©ä½™çš„ç»´åº¦å°±è¶Šå°‘ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Kernel and image capture the essence of a linear map. They classify transformations, explain when systems have unique or infinite solutions, and form the backbone of important results like the Rankâ€“Nullity Theorem, diagonalization, and spectral theory.
æ ¸å’Œå›¾åƒæ•æ‰äº†çº¿æ€§æ˜ å°„çš„æœ¬è´¨ã€‚å®ƒä»¬å¯¹å˜æ¢è¿›è¡Œåˆ†ç±»ï¼Œè§£é‡Šç³»ç»Ÿä½•æ—¶å…·æœ‰å”¯ä¸€æˆ–æ— é™è§£ï¼Œå¹¶æ„æˆç§©é›¶å®šç†ã€å¯¹è§’åŒ–å’Œè°±ç†è®ºç­‰é‡è¦ç»“æœçš„æ”¯æŸ±ã€‚

### Exercises 5.3
ç»ƒä¹  5.3

1.  Find the kernel and image of $T:\mathbb{R}^2 \to \mathbb{R}^2$, $T(x,y) = (x-y, x+y)$.
    æŸ¥æ‰¾ $T:\mathbb{R}^2 \to \mathbb{R}^2$ ã€ $T(x,y) = (x-y, x+y)$ çš„æ ¸å’Œå›¾åƒã€‚
2.  Let
    è®©

$$
A = \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \end{bmatrix}
$$

Find bases for $\ker(A)$ and $\text{im}(A)$. 3. For the projection map $P(x,y,z) = (x,y,0)$, describe the kernel and image. 4. Prove that $\ker(T)$ and $\text{im}(T)$ are always subspaces. 5. Verify the Rankâ€“Nullity Theorem for the transformation in Example 5.3.1.
æ‰¾åˆ° $\ker(A)$ å’Œ $\text{im}(A)$ çš„åŸºã€‚3. å¯¹äºæŠ•å½±å›¾ $P(x,y,z) = (x,y,0)$ ï¼Œæè¿°å…¶æ ¸å’Œå›¾åƒã€‚4. è¯æ˜ $\ker(T)$ å’Œ $\text{im}(T)$ å§‹ç»ˆæ˜¯å­ç©ºé—´ã€‚5. éªŒè¯ç¤ºä¾‹5.3.1ä¸­å˜æ¢çš„ç§©é›¶æ€§å®šç†ã€‚

## 5.4 Change of Basis
5.4 åŸºç¡€å˜æ›´

Linear transformations can look very different depending on the coordinate system we use. The process of rewriting vectors and transformations relative to a new basis is called a change of basis. This concept lies at the heart of diagonalization, orthogonalization, and many computational techniques.
æ ¹æ®æˆ‘ä»¬ä½¿ç”¨çš„åæ ‡ç³»ï¼Œçº¿æ€§å˜æ¢çœ‹èµ·æ¥å¯èƒ½éå¸¸ä¸åŒã€‚ç›¸å¯¹äºæ–°çš„åŸºé‡å†™å‘é‡å’Œå˜æ¢çš„è¿‡ç¨‹ç§°ä¸ºåŸºå˜æ¢ã€‚è¿™ä¸ªæ¦‚å¿µæ˜¯å¯¹è§’åŒ–ã€æ­£äº¤åŒ–ä»¥åŠè®¸å¤šè®¡ç®—æŠ€æœ¯çš„æ ¸å¿ƒã€‚

### Coordinate Change
åæ ‡å˜æ¢

Suppose $V$ is an $n$\-dimensional vector space, and let $\mathcal{B} = \{\mathbf{v}_1, \dots, \mathbf{v}_n\}$ be a basis. Every vector $\mathbf{x} \in V$ has a coordinate vector $[\mathbf{x}]_{\mathcal{B}} \in \mathbb{R}^n$.
å‡è®¾ $V$ æ˜¯ä¸€ä¸ª $n$ ç»´å‘é‡ç©ºé—´ï¼Œè®¾ $\mathcal{B} = \{\mathbf{v}_1, \dots, \mathbf{v}_n\}$ ä¸ºåŸºã€‚æ¯ä¸ªå‘é‡ $\mathbf{x} \in V$ éƒ½æœ‰ä¸€ä¸ªåæ ‡å‘é‡ $[\mathbf{x}]_{\mathcal{B}} \in \mathbb{R}^n$ ã€‚

If $P$ is the change-of-basis matrix from $\mathcal{B}$ to the standard basis, then
å¦‚æœ $P$ æ˜¯ä» $\mathcal{B}$ åˆ°æ ‡å‡†åŸºçš„åŸºå˜æ¢çŸ©é˜µï¼Œåˆ™

$$
\mathbf{x} = P [\mathbf{x}]_{\mathcal{B}}.
$$

Equivalently,
ç­‰ä»·åœ°ï¼Œ

$$
[\mathbf{x}]_{\mathcal{B}} = P^{-1} \mathbf{x}.
$$

Here, $P$ has the basis vectors of $\mathcal{B}$ as its columns:
è¿™é‡Œï¼Œ $P$ ä»¥ $\mathcal{B}$ çš„åŸºå‘é‡ä½œä¸ºå…¶åˆ—ï¼š

$$
P = \begin{bmatrix}\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n\end{bmatrix}.
$$

### Transformation of Matrices
çŸ©é˜µå˜æ¢

Let $T: V \to V$ be a linear transformation. Suppose its matrix in the standard basis is $A$. In the basis $\mathcal{B}$, the representing matrix becomes
ä»¤ $T: V \to V$ ä¸ºçº¿æ€§å˜æ¢ã€‚å‡è®¾å…¶åœ¨æ ‡å‡†åŸºä¸­çš„çŸ©é˜µä¸º $A$ ã€‚åœ¨åŸº $\mathcal{B}$ ä¸­ï¼Œè¡¨ç¤ºçŸ©é˜µå˜ä¸º

$$
[T]_{\mathcal{B}} = P^{-1} A P.
$$

Thus, changing basis corresponds to a similarity transformation of the matrix.
å› æ­¤ï¼Œæ”¹å˜åŸºç¡€å¯¹åº”äºçŸ©é˜µçš„ç›¸ä¼¼å˜æ¢ã€‚

### Example
ä¾‹å­

Example 5.4.1. Let $T:\mathbb{R}^2 \to \mathbb{R}^2$ be given by
ä¾‹ 5.4.1ã€‚ ä»¤ $T:\mathbb{R}^2 \to \mathbb{R}^2$ ä¸º

$$
T(x,y) = (3x + y, x + y).
$$

In the standard basis, its matrix is
åœ¨æ ‡å‡†åŸºç¡€ä¸Šï¼Œå…¶çŸ©é˜µä¸º

$$
A = \begin{bmatrix}3 & 1 \\1 & 1\end{bmatrix}.
$$

Now consider the basis $\mathcal{B} = \{ (1,1), (1,-1) \}$. The change-of-basis matrix is
ç°åœ¨è€ƒè™‘åŸº $\mathcal{B} = \{ (1,1), (1,-1) \}$ ã€‚åŸºå˜æ¢çŸ©é˜µä¸º

$$
P = \begin{bmatrix}1 & 1 \\1 & -1\end{bmatrix}.
$$

Then
ç„¶å

$$
[T]_{\mathcal{B}} = P^{-1} A P.
$$

Computing gives
è®¡ç®—å¾—å‡º

$$
[T]_{\mathcal{B}} =\begin{bmatrix}4 & 0 \\0 & 0\end{bmatrix}.
$$

In this new basis, the transformation is diagonal: one direction is scaled by 4, the other collapsed to 0.
åœ¨è¿™ä¸ªæ–°çš„åŸºç¡€ä¸Šï¼Œå˜æ¢æ˜¯å¯¹è§’çš„ï¼šä¸€ä¸ªæ–¹å‘ç¼©æ”¾ 4ï¼Œå¦ä¸€ä¸ªæ–¹å‘æŠ˜å ä¸º 0ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Change of basis is like rotating or skewing your coordinate grid. The underlying transformation does not change, but its description in numbers becomes simpler or more complicated depending on the basis. Finding a basis that simplifies a transformation (often a diagonal basis) is a key theme in linear algebra.
åŸºå˜æ¢å°±åƒæ—‹è½¬æˆ–å€¾æ–œåæ ‡ç½‘æ ¼ã€‚åº•å±‚å˜æ¢æœ¬èº«ä¸ä¼šæ”¹å˜ï¼Œä½†å…¶æ•°å€¼æè¿°ä¼šæ ¹æ®åŸºçš„å˜åŒ–è€Œå˜å¾—æ›´ç®€å•æˆ–æ›´å¤æ‚ã€‚å¯»æ‰¾èƒ½å¤Ÿç®€åŒ–å˜æ¢çš„åŸºï¼ˆé€šå¸¸æ˜¯å¯¹è§’åŸºï¼‰æ˜¯çº¿æ€§ä»£æ•°çš„ä¸€ä¸ªå…³é”®ä¸»é¢˜ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Change of basis connects the abstract notion of similarity to practical computation. It is the tool that allows us to diagonalize matrices, compute eigenvalues, and simplify complex transformations. In applications, it corresponds to choosing a more natural coordinate system-whether in geometry, physics, or machine learning.
åŸºå˜æ¢å°†ç›¸ä¼¼æ€§çš„æŠ½è±¡æ¦‚å¿µä¸å®é™…è®¡ç®—è”ç³»èµ·æ¥ã€‚å®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿå¯¹çŸ©é˜µè¿›è¡Œå¯¹è§’åŒ–ã€è®¡ç®—ç‰¹å¾å€¼å¹¶ç®€åŒ–å¤æ‚çš„å˜æ¢ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå®ƒç›¸å½“äºé€‰æ‹©ä¸€ä¸ªæ›´è‡ªç„¶çš„åæ ‡ç³»â€”â€”æ— è®ºæ˜¯åœ¨å‡ ä½•ã€ç‰©ç†è¿˜æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸã€‚

### Exercises 5.4
ç»ƒä¹  5.4

1.  Let
    è®©

$$
A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}
$$

Compute its representation in the basis $\{(1,0),(1,1)\}$. 2. Find the change-of-basis matrix from the standard basis of $\mathbb{R}^2$ to $\{(2,1),(1,1)\}$. 3. Prove that similar matrices (related by $P^{-1}AP$) represent the same linear transformation under different bases. 4. Diagonalize the matrix
è®¡ç®—å…¶åœ¨åŸº $\{(1,0),(1,1)\}$ ä¸­çš„è¡¨ç¤ºã€‚2. æ±‚å‡ºä» $\mathbb{R}^2$ åˆ° $\{(2,1),(1,1)\}$ çš„æ ‡å‡†åŸºå˜æ¢çŸ©é˜µã€‚3. è¯æ˜ç›¸ä¼¼çš„çŸ©é˜µï¼ˆç”± $P^{-1}AP$ å…³è”ï¼‰åœ¨ä¸åŒåŸºä¸‹è¡¨ç¤ºç›¸åŒçš„çº¿æ€§å˜æ¢ã€‚4. å¯¹è§’åŒ–çŸ©é˜µ

$$
A = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}
$$

in the basis $\{(1,1),(1,-1)\}$. 5. In $\mathbb{R}^3$, let $\mathcal{B} = \{(1,0,0),(1,1,0),(1,1,1)\}$. Construct the change-of-basis matrix $P$ and compute $P^{-1}$.
åœ¨åŸº $\{(1,1),(1,-1)\}$ ä¸­ã€‚5. åœ¨ $\mathbb{R}^3$ ä¸­ï¼Œä»¤ $\mathcal{B} = \{(1,0,0),(1,1,0),(1,1,1)\}$ ã€‚æ„é€ åŸºå˜æ¢çŸ©é˜µ $P$ å¹¶è®¡ç®— $P^{-1}$ ã€‚
