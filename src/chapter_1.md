# Chapter 1. Vectors
ç¬¬ 1 ç«  å‘é‡

## 1.1 Scalars and Vectors
1.1 æ ‡é‡å’ŒçŸ¢é‡

A scalar is a single numerical quantity, most often taken from the real numbers, denoted by $\mathbb{R}$. Scalars are the fundamental building blocks of arithmetic: they can be added, subtracted, multiplied, and, except in the case of zero, divided. In linear algebra, scalars play the role of coefficients, scaling factors, and entries of larger structures such as vectors and matrices. They provide the weights by which more complex objects are measured and combined. A vector is an ordered collection of scalars, arranged either in a row or a column. When the scalars are real numbers, the vector is said to belong to *real* $n$\-dimensional space, written


æ ‡é‡æ˜¯ä¸€ä¸ªå•ä¸€çš„æ•°å€¼ï¼Œé€šå¸¸å–è‡ªå®æ•°ï¼Œç”¨ $\mathbb{R}$ è¡¨ç¤ºã€‚æ ‡é‡æ˜¯ç®—æœ¯çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼šå®ƒä»¬å¯ä»¥è¿›è¡ŒåŠ ã€å‡ã€ä¹˜å’Œé™¤ï¼ˆé›¶é™¤å¤–ï¼‰ã€‚åœ¨çº¿æ€§ä»£æ•°ä¸­ï¼Œæ ‡é‡å……å½“ç³»æ•°ã€æ¯”ä¾‹å› å­ä»¥åŠå‘é‡å’ŒçŸ©é˜µç­‰æ›´å¤§ç»“æ„ä¸­çš„å…ƒç´ ã€‚å®ƒä»¬æä¾›æƒé‡ï¼Œç”¨äºæµ‹é‡å’Œç»„åˆæ›´å¤æ‚çš„å¯¹è±¡ã€‚å‘é‡æ˜¯æŒ‰è¡Œæˆ–åˆ—æ’åˆ—çš„æ ‡é‡çš„æœ‰åºé›†åˆã€‚å½“æ ‡é‡ä¸ºå®æ•°æ—¶ï¼Œè¯¥å‘é‡è¢«ç§°ä¸ºå±äº*å®* $n$ ç»´ç©ºé—´ï¼Œå†™ä¸º

$$
\mathbb{R}^n = \{ (x_1, x_2, \dots, x_n) \mid x_i \in \mathbb{R} \}.
$$

An element of $\mathbb{R}^n$ is called a vector of dimension $n$ or an *n*\-vector. The number $n$ is called the dimension of the vector space. Thus $\mathbb{R}^2$ is the space of all ordered pairs of real numbers, $\mathbb{R}^3$ the space of all ordered triples, and so on.

$\mathbb{R}^n$ ä¸­çš„ä¸€ä¸ªå…ƒç´ ç§°ä¸ºç»´åº¦ä¸º $n$ çš„å‘é‡æˆ– *n* å‘é‡ã€‚æ•°å­— $n$ ç§°ä¸ºå‘é‡ç©ºé—´çš„ç»´æ•°ã€‚å› æ­¤ï¼Œ $\mathbb{R}^2$ æ˜¯æ‰€æœ‰æœ‰åºå®æ•°å¯¹çš„ç©ºé—´ï¼Œ $\mathbb{R}^3$ æ˜¯æ‰€æœ‰æœ‰åºä¸‰å…ƒç»„çš„ç©ºé—´ï¼Œç­‰ç­‰ã€‚

Example 1.1.1.
ä¾‹ 1.1.1ã€‚

*   A 2-dimensional vector: $(3, -1) \in \mathbb{R}^2$.

    äºŒç»´å‘é‡ï¼š $(3, -1) \in \mathbb{R}^2$ ã€‚
*   A 3-dimensional vector: $(2, 0, 5) \in \mathbb{R}^3$.

    ä¸‰ç»´å‘é‡ï¼š $(2, 0, 5) \in \mathbb{R}^3$ ã€‚
*   A 1-dimensional vector: $(7) \in \mathbb{R}^1$, which corresponds to the scalar 7 itself.

    ä¸€ç»´å‘é‡ï¼š $(7) \in \mathbb{R}^1$ ï¼Œå¯¹åº”äºæ ‡é‡ 7 æœ¬èº«ã€‚

Vectors are often written vertically in column form, which emphasizes their role in matrix multiplication:

å‘é‡é€šå¸¸ä»¥åˆ—çš„å½¢å¼å‚ç›´ä¹¦å†™ï¼Œè¿™å¼ºè°ƒäº†å®ƒä»¬åœ¨çŸ©é˜µä¹˜æ³•ä¸­çš„ä½œç”¨ï¼š

$$
\mathbf{v} = \begin{bmatrix}2 \\0 \\5 \end{bmatrix} \in \mathbb{R}^3.
$$

The vertical layout makes the structure clearer when we consider linear combinations or multiply matrices by vectors.

å½“æˆ‘ä»¬è€ƒè™‘çº¿æ€§ç»„åˆæˆ–çŸ©é˜µä¹˜ä»¥å‘é‡æ—¶ï¼Œå‚ç›´å¸ƒå±€ä½¿ç»“æ„æ›´åŠ æ¸…æ™°ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

In $\mathbb{R}^2$, a vector $(x_1, x_2)$ can be visualized as an arrow starting at the origin $(0,0)$ and ending at the point $(x_1, x_2)$. Its length corresponds to the distance from the origin, and its orientation gives a direction in the plane. In $\mathbb{R}^3$, the same picture extends into three dimensions: a vector is an arrow from the origin to $(x_1, x_2, x_3)$. Beyond three dimensions, direct visualization is no longer possible, but the algebraic rules of vectors remain identical. Even though we cannot draw a vector in $\mathbb{R}^{10}$, it behaves under addition, scaling, and transformation exactly as a 2- or 3-dimensional vector does. This abstract point of view is what allows linear algebra to apply to data science, physics, and machine learning, where data often lives in very high-dimensional spaces. Thus a vector may be regarded in three complementary ways:

åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œå‘é‡ $(x_1, x_2)$ å¯ä»¥å¯è§†åŒ–ä¸ºä¸€ä¸ªä»åŸç‚¹ $(0,0)$ å¼€å§‹åˆ°ç‚¹ $(x_1, x_2)$ ç»“æŸçš„ç®­å¤´ã€‚å®ƒçš„é•¿åº¦å¯¹åº”äºä¸åŸç‚¹çš„è·ç¦»ï¼Œå…¶æ–¹å‘ç»™å‡ºäº†å¹³é¢å†…çš„æ–¹å‘ã€‚åœ¨ $\mathbb{R}^3$ ä¸­ï¼ŒåŒæ ·çš„å›¾åƒå»¶ä¼¸åˆ°ä¸‰ç»´ç©ºé—´ï¼šå‘é‡æ˜¯ä¸€ä¸ªä»åŸç‚¹æŒ‡å‘ $(x_1, x_2, x_3)$ çš„ç®­å¤´ã€‚è¶…è¿‡ä¸‰ç»´ç©ºé—´åï¼Œç›´æ¥å¯è§†åŒ–å°±ä¸å†å¯èƒ½ï¼Œä½†å‘é‡çš„ä»£æ•°è§„åˆ™ä¿æŒä¸å˜ã€‚å³ä½¿æˆ‘ä»¬æ— æ³•åœ¨ $\mathbb{R}^{10}$ ä¸­ç»˜åˆ¶å‘é‡ï¼Œå®ƒåœ¨åŠ æ³•ã€ç¼©æ”¾å’Œå˜æ¢ä¸‹çš„è¡Œä¸ºä¸äºŒç»´æˆ–ä¸‰ç»´å‘é‡å®Œå…¨ç›¸åŒã€‚è¿™ç§æŠ½è±¡çš„è§‚ç‚¹ä½¿å¾—çº¿æ€§ä»£æ•°èƒ½å¤Ÿåº”ç”¨äºæ•°æ®ç§‘å­¦ã€ç‰©ç†å­¦å’Œæœºå™¨å­¦ä¹ ï¼Œè¿™äº›é¢†åŸŸçš„æ•°æ®é€šå¸¸å­˜åœ¨äºéâ€‹â€‹å¸¸é«˜ç»´çš„ç©ºé—´ä¸­ã€‚å› æ­¤ï¼Œå‘é‡å¯ä»¥ä»ä¸‰ä¸ªäº’è¡¥çš„è§’åº¦æ¥çœ‹å¾…ï¼š

1.  As a point in space, described by its coordinates.

    ä½œä¸ºç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œç”±å…¶åæ ‡æè¿°ã€‚
2.  As a displacement or arrow, described by a direction and a length.

    ä½œä¸ºä½ç§»æˆ–ç®­å¤´ï¼Œç”±æ–¹å‘å’Œé•¿åº¦æè¿°ã€‚
3.  As an abstract element of a vector space, whose properties follow algebraic rules independent of geometry.

    ä½œä¸ºå‘é‡ç©ºé—´çš„æŠ½è±¡å…ƒç´ ï¼Œå…¶å±æ€§éµå¾ªä¸å‡ ä½•æ— å…³çš„ä»£æ•°è§„åˆ™ã€‚

### Notation
ç¬¦å·

*   Vectors are written in boldface lowercase letters: $\mathbf{v}, \mathbf{w}, \mathbf{x}$.

    å‘é‡ä»¥ç²—ä½“å°å†™å­—æ¯è¡¨ç¤ºï¼š $\mathbf{v}, \mathbf{w}, \mathbf{x}$ ã€‚
*   The *i*\-th entry of a vector $\mathbf{v}$ is written $v_i$, where indices begin at 1.

    å‘é‡ $\mathbf{v}$ çš„ç¬¬ - ä¸ªå…ƒç´ å†™ä¸º $v_i$â€‹ ï¼Œå…¶ä¸­ç´¢å¼•ä» 1 å¼€å§‹ã€‚
*   The set of all *n*\-dimensional vectors over $\mathbb{R}$ is denoted $\mathbb{R}^n$.

    $\mathbb{R}$ ä¸Šçš„æ‰€æœ‰ *n* ç»´å‘é‡çš„é›†åˆè®°ä¸º $\mathbb{R}^n$ ã€‚
*   Column vectors will be the default form unless otherwise stated.

    é™¤éå¦æœ‰è¯´æ˜ï¼Œåˆ—å‘é‡å°†æ˜¯é»˜è®¤å½¢å¼ã€‚

### Why begin here?
ä¸ºä»€ä¹ˆä»è¿™é‡Œå¼€å§‹ï¼Ÿ

Scalars and vectors form the atoms of linear algebra. Every structure we will build-vector spaces, linear transformations, matrices, eigenvalues-relies on the basic notions of number and ordered collection of numbers. Once vectors are understood, we can define operations such as addition and scalar multiplication, then generalize to subspaces, bases, and coordinate systems. Eventually, this framework grows into the full theory of linear algebra, with powerful applications to geometry, computation, and data.

æ ‡é‡å’Œå‘é‡æ„æˆäº†çº¿æ€§ä»£æ•°çš„åŸå­ã€‚æˆ‘ä»¬å°†è¦æ„å»ºçš„æ¯ä¸€ä¸ªç»“æ„â€”â€”å‘é‡ç©ºé—´ã€çº¿æ€§å˜æ¢ã€çŸ©é˜µã€ç‰¹å¾å€¼â€”â€”éƒ½ä¾èµ–äºæ•°å’Œæœ‰åºæ•°é›†çš„åŸºæœ¬æ¦‚å¿µã€‚ä¸€æ—¦ç†è§£äº†å‘é‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰è¯¸å¦‚åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ä¹‹ç±»çš„è¿ç®—ï¼Œç„¶åæ¨å¹¿åˆ°å­ç©ºé—´ã€åŸºå’Œåæ ‡ç³»ã€‚æœ€ç»ˆï¼Œè¿™ä¸ªæ¡†æ¶å°†å‘å±•æˆä¸ºå®Œæ•´çš„çº¿æ€§ä»£æ•°ç†è®ºï¼Œå¹¶åœ¨å‡ ä½•ã€è®¡ç®—å’Œæ•°æ®é¢†åŸŸæ‹¥æœ‰å¼ºå¤§çš„åº”ç”¨ã€‚

### Exercises 1.1
ç»ƒä¹  1.1

1.  Write three different vectors in $\mathbb{R}^2$ and sketch them as arrows from the origin. Identify their coordinates explicitly.

    åœ¨ $\mathbb{R}^2$ ä¸­å†™å‡ºä¸‰ä¸ªä¸åŒçš„å‘é‡ï¼Œå¹¶å°†å®ƒä»¬ç”»æˆä»åŸç‚¹å‡ºå‘çš„ç®­å¤´ã€‚æ˜ç¡®æŒ‡å‡ºå®ƒä»¬çš„åæ ‡ã€‚
2.  Give an example of a vector in $\mathbb{R}^4$. Can you visualize it directly? Explain why high-dimensional visualization is challenging.

    ç»™å‡º $\mathbb{R}^4$ ä¸­ä¸€ä¸ªå‘é‡çš„ä¾‹å­ã€‚ä½ èƒ½ç›´æ¥å°†å…¶å¯è§†åŒ–å—ï¼Ÿè§£é‡Šä¸ºä»€ä¹ˆé«˜ç»´å¯è§†åŒ–å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
3.  Let $\mathbf{v} = (4, -3, 2)$. Write $\mathbf{v}$ in column form and state $v_1, v_2, v_3$.

    ä»¤ $\mathbf{v} = (4, -3, 2)$ ã€‚å°† $\mathbf{v}$ å†™æˆåˆ—å½¢å¼ï¼Œå¹¶è¯´æ˜ $v_1, v_2, v_3$.
4.  In what sense is the set $\mathbb{R}^1$ both a line and a vector space? Illustrate with examples.

    åœ¨ä»€ä¹ˆæ„ä¹‰ä¸Šé›†åˆ $\mathbb{R}^1$ æ—¢æ˜¯çº¿ç©ºé—´åˆæ˜¯å‘é‡ç©ºé—´ï¼Ÿè¯·ä¸¾ä¾‹è¯´æ˜ã€‚
5.  Consider the vector $\mathbf{u} = (1,1,\dots,1) \in \mathbb{R}^n$. What is special about this vector when $n$ is large? What might it represent in applications?

    è€ƒè™‘å‘é‡ $\mathbf{u} = (1,1,\dots,1) \in \mathbb{R}^n$ ã€‚å½“ $n$ å¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªå‘é‡æœ‰ä»€ä¹ˆç‰¹æ®Šä¹‹å¤„ï¼Ÿå®ƒåœ¨åº”ç”¨ä¸­å¯èƒ½ä»£è¡¨ä»€ä¹ˆï¼Ÿ

## 1.2 Vector Addition and Scalar Multiplication
1.2 å‘é‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•

Vectors in linear algebra are not static objects; their power comes from the operations we can perform on them. Two fundamental operations define the structure of vector spaces: addition and scalar multiplication. These operations satisfy simple but far-reaching rules that underpin the entire subject.

çº¿æ€§ä»£æ•°ä¸­çš„å‘é‡å¹¶éé™æ€å¯¹è±¡ï¼›å®ƒä»¬çš„åŠ›é‡æºäºæˆ‘ä»¬å¯ä»¥å¯¹å®ƒä»¬æ‰§è¡Œçš„è¿ç®—ã€‚ä¸¤ä¸ªåŸºæœ¬è¿ç®—å®šä¹‰äº†å‘é‡ç©ºé—´çš„ç»“æ„ï¼šåŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ã€‚è¿™ä¸¤ä¸ªè¿ç®—æ»¡è¶³ä¸€äº›ç®€å•å´å½±å“æ·±è¿œçš„è§„åˆ™ï¼Œè¿™äº›è§„åˆ™æ„æˆäº†æ•´ä¸ªçº¿æ€§ä»£æ•°å­¦ç§‘çš„åŸºç¡€ã€‚

### Vector Addition
å‘é‡åŠ æ³•

Given two vectors of the same dimension, their sum is obtained by adding corresponding entries. Formally, if

ç»™å®šä¸¤ä¸ªç›¸åŒç»´åº¦çš„å‘é‡ï¼Œå®ƒä»¬çš„å’Œå¯ä»¥é€šè¿‡æ·»åŠ ç›¸åº”çš„å…ƒç´ æ¥è·å¾—ã€‚å½¢å¼ä¸Šï¼Œå¦‚æœ

$$
\mathbf{u} = (u_1, u_2, \dots, u_n), \quad\mathbf{v} = (v_1, v_2, \dots, v_n),
$$

then their sum is

é‚£ä¹ˆå®ƒä»¬çš„æ€»å’Œæ˜¯

$$
\mathbf{u} + \mathbf{v} = (u_1+v_1, u_2+v_2, \dots, u_n+v_n).
$$

Example 1.2.1. Let $\mathbf{u} = (2, -1, 3)$ and $\mathbf{v} = (4, 0, -5)$. Then

ä¾‹ 1.2.1ã€‚ è®¾ $\mathbf{u} = (2, -1, 3)$ å’Œ $\mathbf{v} = (4, 0, -5)$ ã€‚åˆ™

$$
\mathbf{u} + \mathbf{v} = (2+4, -1+0, 3+(-5)) = (6, -1, -2).
$$

Geometrically, vector addition corresponds to the *parallelogram rule*. If we draw both vectors as arrows from the origin, then placing the tail of one vector at the head of the other produces the sum. The diagonal of the parallelogram they form represents the resulting vector.

ä»å‡ ä½•å­¦ä¸Šè®²ï¼Œå‘é‡åŠ æ³•å¯¹åº”äº*å¹³è¡Œå››è¾¹å½¢æ³•åˆ™* ã€‚å¦‚æœæˆ‘ä»¬å°†ä¸¤ä¸ªå‘é‡éƒ½ç”»æˆä»åŸç‚¹å‡ºå‘çš„ç®­å¤´ï¼Œé‚£ä¹ˆå°†ä¸€ä¸ªå‘é‡çš„å°¾éƒ¨æ”¾åœ¨å¦ä¸€ä¸ªå‘é‡çš„å¤´éƒ¨ï¼Œå°±èƒ½å¾—åˆ°å‘é‡çš„å’Œã€‚å®ƒä»¬æ„æˆçš„å¹³è¡Œå››è¾¹å½¢çš„å¯¹è§’çº¿ä»£è¡¨æœ€ç»ˆçš„å‘é‡ã€‚

### Scalar Multiplication
æ ‡é‡ä¹˜æ³•

Multiplying a vector by a scalar stretches or shrinks the vector while preserving its direction, unless the scalar is negative, in which case the vector is also reversed. If $c \in \mathbb{R}$ and

å°†çŸ¢é‡ä¹˜ä»¥æ ‡é‡ä¼šæ‹‰ä¼¸æˆ–æ”¶ç¼©çŸ¢é‡ï¼ŒåŒæ—¶ä¿æŒå…¶æ–¹å‘ï¼Œé™¤éæ ‡é‡ è´Ÿæ•°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹å‘é‡ä¹Ÿä¼šåè½¬ã€‚å¦‚æœ $c \in \mathbb{R}$ å’Œ

$$
\mathbf{v} = (v_1, v_2, \dots, v_n),
$$

then

ç„¶å

$$
c \mathbf{v} = (c v_1, c v_2, \dots, c v_n).
$$

Example 1.2.2. Let $\mathbf{v} = (3, -2)$ and $c = -2$. Then

ä¾‹ 1.2.2ã€‚ è®¾ $\mathbf{v} = (3, -2)$ å’Œ $c = -2$ ã€‚åˆ™

$$
c\mathbf{v} = -2(3, -2) = (-6, 4).
$$

This corresponds to flipping the vector through the origin and doubling its length.

è¿™ç›¸å½“äºé€šè¿‡åŸç‚¹ç¿»è½¬å‘é‡å¹¶ä½¿å…¶é•¿åº¦åŠ å€ã€‚

### Linear Combinations
çº¿æ€§ç»„åˆ

The interaction of addition and scalar multiplication allows us to form *linear combinations*. A linear combination of vectors $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k$ is any vector of the form

åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•çš„ç›¸äº’ä½œç”¨ä½¿æˆ‘ä»¬èƒ½å¤Ÿå½¢æˆ*çº¿æ€§ç»„åˆ* ã€‚å‘é‡ğ‘£çš„çº¿æ€§ç»„åˆ 1 , ğ‘£ 2 , â€¦ , ğ‘£ ğ‘˜ v 1 â€‹ ï¼Œv 2 â€‹ ï¼Œâ€¦ï¼Œv k â€‹ æ˜¯ä»»æ„å½¢å¼çš„å‘é‡

$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k, \quad c_i \in \mathbb{R}.
$$

Linear combinations are the mechanism by which we generate new vectors from existing ones. The span of a set of vectors-the collection of all their linear combinations-will later lead us to the idea of a subspace.

çº¿æ€§ç»„åˆæ˜¯ä¸€ç§ä»ç°æœ‰å‘é‡ç”Ÿæˆæ–°å‘é‡çš„æœºåˆ¶ã€‚ä¸€ç»„å‘é‡çš„è·¨åº¦â€”â€”å®ƒä»¬æ‰€æœ‰çº¿æ€§ç»„åˆçš„é›†åˆâ€”â€”ç¨åä¼šå¼•å‡ºå­ç©ºé—´çš„æ¦‚å¿µã€‚

Example 1.2.3. Let $\mathbf{v}_1 = (1,0)$ and $\mathbf{v}_2 = (0,1)$. Then any vector $(a,b)\in\mathbb{R}^2$ can be expressed as

ä¾‹ 1.2.3ã€‚ è®¾ $\mathbf{v}_1 = (1,0)$ å’Œ $\mathbf{v}_2 = (0,1)$ ã€‚åˆ™ä»»æ„å‘é‡ $(a,b)\in\mathbb{R}^2$ å¯ä»¥è¡¨ç¤ºä¸º

$$
a\mathbf{v}_1 + b\mathbf{v}_2.
$$

Thus $(1,0)$ and $(0,1)$ form the basic building blocks of the plane.

å› æ­¤ $(1,0)$ å’Œ $(0,1)$ æ„æˆäº†å¹³é¢çš„åŸºæœ¬æ„é€ å—ã€‚

### Notation
ç¬¦å·

*   Addition: $\mathbf{u} + \mathbf{v}$ means component-wise addition.

    åŠ æ³•ï¼š $\mathbf{u} + \mathbf{v}$ è¡¨ç¤ºé€ä¸ªç»„ä»¶çš„åŠ æ³•ã€‚
*   Scalar multiplication: $c\mathbf{v}$ scales each entry of $\mathbf{v}$ by $c$.

    æ ‡é‡ä¹˜æ³•ï¼š $c\mathbf{v}$ å°† $\mathbf{v}$ çš„æ¯ä¸ªæ¡ç›®ä¹˜ä»¥ $c$ ã€‚
*   Linear combination: a sum of the form $c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k$.

    çº¿æ€§ç»„åˆï¼šğ‘ å½¢å¼çš„å’Œ 1 ğ‘£ 1 + â‹¯ + ğ‘ ğ‘˜ ğ‘£ ğ‘˜ c 1 â€‹ v 1 â€‹ +â‹¯+c k â€‹ v k â€‹ .

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Vector addition and scalar multiplication are the defining operations of linear algebra. They give structure to vector spaces, allow us to describe geometric phenomena like translation and scaling, and provide the foundation for solving systems of equations. Everything that follows-basis, dimension, transformations-builds on these simple but profound rules.

å‘é‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•æ˜¯çº¿æ€§ä»£æ•°çš„å®šä¹‰è¿ç®—ã€‚å®ƒä»¬èµ‹äºˆå‘é‡ç©ºé—´ç»“æ„ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæè¿°å¹³ç§»å’Œç¼©æ”¾ç­‰å‡ ä½•ç°è±¡ï¼Œå¹¶ä¸ºæ–¹ç¨‹ç»„çš„æ±‚è§£å¥ å®šåŸºç¡€ã€‚ä¹‹åçš„ä¸€åˆ‡â€”â€”åŸºã€ç»´åº¦ã€å˜æ¢â€”â€”éƒ½å»ºç«‹åœ¨è¿™äº›ç®€å•è€Œæ·±åˆ»çš„è§„åˆ™ä¹‹ä¸Šã€‚

### Exercises 1.2
ç»ƒä¹  1.2

1.  Compute $\mathbf{u} + \mathbf{v}$ where $\mathbf{u} = (1,2,3)$ and $\mathbf{v} = (4, -1, 0)$.

    è®¡ç®— $\mathbf{u} + \mathbf{v}$ ï¼Œå…¶ä¸­ $\mathbf{u} = (1,2,3)$ å’Œ $\mathbf{v} = (4, -1, 0)$ ã€‚
2.  Find $3\mathbf{v}$ where $\mathbf{v} = (-2,5)$. Sketch both vectors to illustrate the scaling.

    æ±‚ $3\mathbf{v} $ å…¶ä¸­ $ \mathbf{v} = (-2,5)$ã€‚ç”»å‡ºä¸¤ä¸ªå‘é‡çš„ç¤ºæ„å›¾ï¼Œä»¥è¯´æ˜ç¼©æ”¾å…³ç³»ã€‚
3.  Show that $(5,7)$ can be written as a linear combination of $(1,0)$ and $(0,1)$.

    è¯æ˜ $(5,7)$ å¯ä»¥å†™æˆ $(1,0)$ å’Œ $(0,1)$ çš„çº¿æ€§ç»„åˆã€‚
4.  Write $(4,4)$ as a linear combination of $(1,1)$ and $(1,-1)$.
    å°† $(4,4)$ å†™ä¸º $(1,1)$ å’Œ $(1,-1)$ çš„çº¿æ€§ç»„åˆã€‚
5.  Prove that if $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$, then $(c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} + c\mathbf{v} + d\mathbf{u} + d\mathbf{v}$ for scalars $c,d \in \mathbb{R}$.

    è¯æ˜å¦‚æœ $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$ ï¼Œåˆ™å¯¹äºæ ‡é‡ $c,d \in \mathbb{R}$ æœ‰ $(c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} + c\mathbf{v} + d\mathbf{u} + d\mathbf{v}$ ã€‚

## 1.3 Dot Product, Norms, and Angles
1.3 ç‚¹ç§¯ã€èŒƒæ•°å’Œè§’

The dot product is the fundamental operation that links algebra and geometry in vector spaces. It allows us to measure lengths, compute angles, and determine orthogonality. From this single definition flow the notions of *norm* and *angle*, which give geometry to abstract vector spaces.

ç‚¹ç§¯æ˜¯å‘é‡ç©ºé—´ä¸­è¿æ¥ä»£æ•°å’Œå‡ ä½•çš„åŸºæœ¬è¿ç®—ã€‚å®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿæµ‹é‡é•¿åº¦ã€è®¡ç®—è§’åº¦å¹¶ç¡®å®šæ­£äº¤æ€§ã€‚ä»è¿™ä¸ªå•ä¸€å®šä¹‰ä¸­è¡ç”Ÿå‡º*èŒƒ*æ•°å’Œ *è§’åº¦* ï¼Œå®ƒä¸ºæŠ½è±¡å‘é‡ç©ºé—´æä¾›å‡ ä½•å½¢çŠ¶ã€‚

### The Dot Product
ç‚¹ç§¯

For two vectors in $\mathbb{R}^n$, the dot product (also called the inner product) is defined by

å¯¹äº $\mathbb{R}^n$ ä¸­çš„ä¸¤ä¸ªå‘é‡ï¼Œç‚¹ç§¯ï¼ˆä¹Ÿç§°ä¸ºå†…ç§¯ï¼‰å®šä¹‰ä¸º

$$
\mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
$$

Equivalently, in matrix notation:

ç­‰æ•ˆåœ°ï¼Œç”¨çŸ©é˜µè¡¨ç¤ºæ³•è¡¨ç¤ºï¼š

$$
\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T \mathbf{v}.
$$

Example 1.3.1. Let $\mathbf{u} = (2, -1, 3)$ and $\mathbf{v} = (4, 0, -2)$. Then

ä¾‹ 1.3.1ã€‚ è®¾ $\mathbf{u} = (2, -1, 3)$ å’Œ $\mathbf{v} = (4, 0, -2)$ ã€‚åˆ™

$$
\mathbf{u} \cdot \mathbf{v} = 2\cdot 4 + (-1)\cdot 0 + 3\cdot (-2) = 8 - 6 = 2.
$$

The dot product outputs a single scalar, not another vector.

ç‚¹ç§¯è¾“å‡ºå•ä¸ªæ ‡é‡ï¼Œè€Œä¸æ˜¯å¦ä¸€ä¸ªå‘é‡ã€‚

### Norms (Length of a Vector)
èŒƒæ•°ï¼ˆå‘é‡çš„é•¿åº¦ï¼‰

The *Euclidean norm* of a vector is the square root of its dot product with itself:

å‘é‡çš„*æ¬§å‡ é‡Œå¾—èŒƒæ•°*æ˜¯å…¶ä¸è‡ªèº«çš„ç‚¹ç§¯çš„å¹³æ–¹æ ¹ï¼š

$$
\|\mathbf{v}\| = \sqrt{\mathbf{v} \cdot \mathbf{v}} = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.
$$

This generalizes the Pythagorean theorem to arbitrary dimensions.

è¿™å°†å‹¾è‚¡å®šç†æ¨å¹¿åˆ°ä»»æ„ç»´åº¦ã€‚

Example 1.3.2. For $\mathbf{v} = (3, 4)$,

ä¾‹ 1.3.2ã€‚ å¯¹äº $\mathbf{v} = (3, 4)$ ï¼Œ

$$
\|\mathbf{v}\| = \sqrt{3^2 + 4^2} = \sqrt{25} = 5.
$$

This is exactly the length of the vector as an arrow in the plane.

è¿™æ­£æ˜¯å¹³é¢ä¸­ç®­å¤´æ‰€æŒ‡çš„çŸ¢é‡çš„é•¿åº¦ã€‚

### Angles Between Vectors
å‘é‡ä¹‹é—´çš„è§’åº¦

The dot product also encodes the angle between two vectors. For nonzero vectors $\mathbf{u}, \mathbf{v}$,

ç‚¹ç§¯ä¹Ÿç¼–ç äº†ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è§’åº¦ã€‚å¯¹äºéé›¶å‘é‡ $\mathbf{u}, \mathbf{v}$ ï¼Œ

$$
\mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \, \|\mathbf{v}\| \cos \theta,
$$

where $\theta$ is the angle between them. Thus,

å…¶ä¸­ $\theta$ æ˜¯å®ƒä»¬ä¹‹é—´çš„è§’åº¦ã€‚å› æ­¤ï¼Œ

$$
\cos \theta = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\|\|\mathbf{v}\|}.
$$

Example 1.3.3. Let $\mathbf{u} = (1,0)$ and $\mathbf{v} = (0,1)$. Then

ä¾‹ 1.3.3ã€‚ è®¾ $\mathbf{u} = (1,0)$ å’Œ $\mathbf{v} = (0,1)$ ã€‚åˆ™

$$
\mathbf{u} \cdot \mathbf{v} = 0, \quad \|\mathbf{u}\| = 1, \quad \|\mathbf{v}\| = 1.
$$

Hence

å› æ­¤

$$
\cos \theta = \frac{0}{1\cdot 1} = 0 \quad \Rightarrow \quad \theta = \frac{\pi}{2}.
$$

The vectors are perpendicular.

è¿™äº›å‘é‡æ˜¯å‚ç›´çš„ã€‚

### Orthogonality
æ­£äº¤æ€§

Two vectors are said to be orthogonal if their dot product is zero:

å¦‚æœä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ä¸ºé›¶ï¼Œåˆ™ç§°å®ƒä»¬æ­£äº¤ï¼š

$$
\mathbf{u} \cdot \mathbf{v} = 0.
$$

Orthogonality generalizes the idea of perpendicularity from geometry to higher dimensions.

æ­£äº¤æ€§å°†å‚ç›´æ€§çš„æ¦‚å¿µä»å‡ ä½•å­¦æ¨å¹¿åˆ°æ›´é«˜ç»´åº¦ã€‚

### Notation
ç¬¦å·

*   Dot product: $\mathbf{u} \cdot \mathbf{v}$.

    ç‚¹ç§¯ï¼š $\mathbf{u} \cdot \mathbf{v}$ ã€‚
*   Norm (length): $|\mathbf{v}|$.

    è§„èŒƒï¼ˆé•¿åº¦ï¼‰ï¼š $|\mathbf{v}|$ ã€‚
*   Orthogonality: $\mathbf{u} \perp \mathbf{v}$ if $\mathbf{u} \cdot \mathbf{v} = 0$.

    æ­£äº¤æ€§ï¼šå¦‚æœä¸º $\mathbf{u} \cdot \mathbf{v} = 0$ ï¼Œåˆ™ä¸º $\mathbf{u} \perp \mathbf{v}$ ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

The dot product turns vector spaces into geometric objects: vectors gain lengths, angles, and notions of perpendicularity. This foundation will later support the study of orthogonal projections, Gramâ€“Schmidt orthogonalization, eigenvectors, and least squares problems.

ç‚¹ç§¯å°†å‘é‡ç©ºé—´è½¬åŒ–ä¸ºå‡ ä½•å¯¹è±¡ï¼šå‘é‡è·å¾—é•¿åº¦ã€è§’åº¦å’Œå‚ç›´åº¦çš„æ¦‚å¿µã€‚è¿™ä¸€åŸºç¡€å°†ä¸ºåç»­çš„æ­£äº¤æŠ•å½±ã€æ ¼æ‹‰å§†-æ–½å¯†ç‰¹æ­£äº¤åŒ–ã€ç‰¹å¾å‘é‡å’Œæœ€å°äºŒä¹˜é—®é¢˜çš„ç ”ç©¶å¥ å®šåŸºç¡€ã€‚

### Exercises 1.3
ç»ƒä¹  1.3

1.  Compute $\mathbf{u} \cdot \mathbf{v}$ for $\mathbf{u} = (1,2,3)$, $\mathbf{v} = (4,5,6)$.

    è®¡ç®— $\mathbf{u} = (1,2,3)$ ã€ $\mathbf{v} = (4,5,6)$ çš„ $\mathbf{u} \cdot \mathbf{v}$ ã€‚
2.  Find the norm of $\mathbf{v} = (2, -2, 1)$.

    æ±‚å‡º $\mathbf{v} = (2, -2, 1)$ çš„èŒƒæ•°ã€‚
3.  Determine whether $\mathbf{u} = (1,1,0)$ and $\mathbf{v} = (1,-1,2)$ are orthogonal.

    ç¡®å®š $\mathbf{u} = (1,1,0)$ å’Œ $\mathbf{v} = (1,-1,2)$ æ˜¯å¦æ­£äº¤ã€‚
4.  Let $\mathbf{u} = (3,4)$, $\mathbf{v} = (4,3)$. Compute the angle between them.

    ä»¤ $\mathbf{u} = (3,4)$ , $\mathbf{v} = (4,3)$ ã€‚è®¡ç®—å®ƒä»¬ä¹‹é—´çš„è§’åº¦ã€‚
5.  Prove that $|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}$. This identity is the algebraic version of the Law of Cosines.

    è¯æ˜ $|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}$ ã€‚è¿™ä¸ªæ’ç­‰å¼æ˜¯ä½™å¼¦å®šç†çš„ä»£æ•°å½¢å¼ã€‚

## 1.4 Orthogonality
1.4 æ­£äº¤æ€§

Orthogonality captures the notion of perpendicularity in vector spaces. It is one of the most important geometric ideas in linear algebra, allowing us to decompose vectors, define projections, and construct special bases with elegant properties.

æ­£äº¤æ€§æ•æ‰äº†å‘é‡ç©ºé—´ä¸­å‚ç›´æ€§çš„æ¦‚å¿µã€‚å®ƒæ˜¯çº¿æ€§ä»£æ•°ä¸­æœ€é‡è¦çš„å‡ ä½•æ¦‚å¿µä¹‹ä¸€ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ†è§£å‘é‡ã€å®šä¹‰æŠ•å½±ï¼Œå¹¶æ„é€ å…·æœ‰ä¼˜é›…æ€§è´¨çš„ç‰¹æ®ŠåŸºã€‚

### Definition
å®šä¹‰

Two vectors $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$ are said to be orthogonal if their dot product is zero:

å¦‚æœä¸¤ä¸ªå‘é‡ $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$ çš„ç‚¹ç§¯ä¸ºé›¶ï¼Œåˆ™ç§°å®ƒä»¬æ­£äº¤ï¼š

$$
\mathbf{u} \cdot \mathbf{v} = 0.
$$

This condition ensures that the angle between them is $\pi/2$ radians (90 degrees).

æ­¤æ¡ä»¶ç¡®ä¿å®ƒä»¬ä¹‹é—´çš„è§’åº¦ä¸º $\pi/2$ å¼§åº¦ï¼ˆ90 åº¦ï¼‰ã€‚

Example 1.4.1. In $\mathbb{R}^2$, the vectors $(1,2)$ and $(2,-1)$ are orthogonal since

ä¾‹ 1.4.1ã€‚ åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œå‘é‡ $(1,2)$ å’Œ $(2,-1)$ æ˜¯æ­£äº¤çš„ï¼Œå› ä¸º

$$
(1,2) \cdot (2,-1) = 1\cdot 2 + 2\cdot (-1) = 0.
$$

### Orthogonal Sets
æ­£äº¤é›†

A collection of vectors is called orthogonal if every distinct pair of vectors in the set is orthogonal. If, in addition, each vector has norm 1, the set is called orthonormal.

å¦‚æœä¸€ç»„å‘é‡ä¸­æ¯å¯¹ä¸åŒçš„å‘é‡éƒ½æ˜¯æ­£äº¤çš„ï¼Œåˆ™ç§°è¯¥é›†åˆä¸ºæ­£äº¤å‘é‡ã€‚æ­¤å¤–ï¼Œå¦‚æœæ¯ä¸ªå‘é‡çš„èŒƒæ•°å‡ä¸º 1ï¼Œåˆ™è¯¥é›†åˆç§°ä¸ºæ ‡å‡†æ­£äº¤å‘é‡é›†ã€‚

Example 1.4.2. In $\mathbb{R}^3$, the standard basis vectors

ä¾‹ 1.4.2ã€‚ åœ¨ $\mathbb{R}^3$ ä¸­ï¼Œæ ‡å‡†åŸºå‘é‡

$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$

form an orthonormal set: each has length 1, and their dot products vanish when the indices differ.

å½¢æˆä¸€ä¸ªæ­£äº¤é›†ï¼šæ¯ä¸ªé›†çš„é•¿åº¦ä¸º 1ï¼Œå¹¶ä¸”å½“ç´¢å¼•ä¸åŒæ—¶ï¼Œå®ƒä»¬çš„ç‚¹ç§¯æ¶ˆå¤±ã€‚

### Projections
é¢„æµ‹

Orthogonality makes possible the decomposition of a vector into two components: one parallel to another vector, and one orthogonal to it. Given a nonzero vector $\mathbf{u}$ and any vector $\mathbf{v}$, the projection of $\mathbf{v}$ onto $\mathbf{u}$ is

æ­£äº¤æ€§ä½¿å¾—å°†ä¸€ä¸ªå‘é‡åˆ†è§£ä¸ºä¸¤ä¸ªåˆ†é‡æˆä¸ºå¯èƒ½ï¼šä¸€ä¸ªä¸å¦ä¸€ä¸ªå‘é‡å¹³è¡Œï¼Œå¦ä¸€ä¸ª ä¸å…¶æ­£äº¤ã€‚ç»™å®šä¸€ä¸ªéé›¶å‘é‡ $\mathbf{u}$ å’Œä»»æ„å‘é‡ $\mathbf{v}$ ï¼Œåˆ™ $\mathbf{v}$ çš„æŠ•å½± åˆ° $\mathbf{u}$ æ˜¯

$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}.
$$

The difference

åŒºåˆ«

$$
\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})
$$

is orthogonal to $\mathbf{u}$. Thus every vector can be decomposed uniquely into a parallel and perpendicular part with respect to another vector.

ä¸ $\mathbf{u}$ æ­£äº¤ã€‚å› æ­¤ï¼Œæ¯ä¸ªå‘é‡éƒ½å¯ä»¥å”¯ä¸€åœ°åˆ†è§£ä¸ºç›¸å¯¹äºå¦ä¸€ä¸ªå‘é‡å¹³è¡Œå’Œå‚ç›´çš„éƒ¨åˆ†ã€‚

Example 1.4.3. Let $\mathbf{u} = (1,0)$, $\mathbf{v} = (2,3)$. Then

ä¾‹ 1.4.3ã€‚ ä»¤ $\mathbf{u} = (1,0)$ ï¼Œ $\mathbf{v} = (2,3)$ ã€‚ç„¶å

$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{(1,0)\cdot(2,3)}{(1,0)\cdot(1,0)} (1,0)= \frac{2}{1}(1,0) = (2,0).
$$

Thus
å› æ­¤

$$
\mathbf{v} = (2,3) = (2,0) + (0,3),
$$

where $(2,0)$ is parallel to $(1,0)$ and $(0,3)$ is orthogonal to it.

å…¶ä¸­ $(2,0)$ ä¸ $(1,0)$ å¹³è¡Œï¼Œ $(0,3)$ ä¸ $(1,0)$ æ­£äº¤ã€‚

### Orthogonal Decomposition
æ­£äº¤åˆ†è§£

In general, if $\mathbf{u} \neq \mathbf{0}$ and $\mathbf{v} \in \mathbb{R}^n$, then

ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœ $\mathbf{u} \neq \mathbf{0}$ å’Œ $\mathbf{v} \in \mathbb{R}^n$ ï¼Œé‚£ä¹ˆ

$$
\mathbf{v} = \text{proj}\_{\mathbf{u}}(\mathbf{v}) + \big(\mathbf{v} - \text{proj}\_{\mathbf{u}}(\mathbf{v})\big),
$$

where the first term is parallel to $\mathbf{u}$ and the second term is orthogonal. This decomposition underlies methods such as least squares approximation and the Gramâ€“Schmidt process.

å…¶ä¸­ç¬¬ä¸€é¡¹å¹³è¡Œäº $\mathbf{u}$ ï¼Œç¬¬äºŒé¡¹æ­£äº¤ã€‚è¿™ç§åˆ†è§£æ˜¯æœ€å°äºŒä¹˜è¿‘ä¼¼å’Œæ ¼æ‹‰å§†-æ–½å¯†ç‰¹è¿‡ç¨‹ç­‰æ–¹æ³•çš„åŸºç¡€ã€‚

### Notation
ç¬¦å·

*   $\mathbf{u} \perp \mathbf{v}$: vectors $\mathbf{u}$ and $\mathbf{v}$ are orthogonal.

    $\mathbf{u} \perp \mathbf{v}$ ï¼šå‘é‡ $\mathbf{u}$ å’Œ $\mathbf{v}$ æ­£äº¤ã€‚
*   An orthogonal set: vectors pairwise orthogonal.

    æ­£äº¤é›†ï¼šå‘é‡ä¸¤ä¸¤æ­£äº¤ã€‚
*   An orthonormal set: pairwise orthogonal, each of norm 1.

    æ­£äº¤é›†ï¼šä¸¤ä¸¤æ­£äº¤ï¼Œæ¯ç»„èŒƒæ•°ä¸º 1ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Orthogonality gives structure to vector spaces. It provides a way to separate independent directions cleanly, simplify computations, and minimize errors in approximations. Many powerful algorithms in numerical linear algebra and data science (QR decomposition, least squares regression, PCA) rely on orthogonality.

æ­£äº¤æ€§èµ‹äºˆå‘é‡ç©ºé—´ç»“æ„ã€‚å®ƒæä¾›äº†ä¸€ç§æ¸…æ™°åœ°åˆ†ç¦»ç‹¬ç«‹æ–¹å‘ã€ç®€åŒ–è®¡ç®—å¹¶æœ€å°åŒ–è¿‘ä¼¼è¯¯å·®çš„æ–¹æ³•ã€‚æ•°å€¼çº¿æ€§ä»£æ•°å’Œæ•°æ®ç§‘å­¦ä¸­è®¸å¤šå¼ºå¤§çš„ç®—æ³•ï¼ˆä¾‹å¦‚ QR åˆ†è§£ã€æœ€å°äºŒä¹˜å›å½’ã€ä¸»æˆåˆ†åˆ†æï¼‰éƒ½ä¾èµ–äºæ­£äº¤æ€§ã€‚

### Exercises 1.4
ç»ƒä¹  1.4

1.  Verify that the vectors $(1,2,2)$ and $(2,0,-1)$ are orthogonal.

    éªŒè¯å‘é‡ $(1,2,2)$ å’Œ $(2,0,-1)$ æ˜¯å¦æ­£äº¤ã€‚
2.  Find the projection of $(3,4)$ onto $(1,1)$.

    æ‰¾åˆ° $(3,4)$ åˆ° $(1,1)$ çš„æŠ•å½±ã€‚
3.  Show that any two distinct standard basis vectors in $\mathbb{R}^n$ are orthogonal.

    è¯æ˜ $\mathbb{R}^n$ ä¸­çš„ä»»æ„ä¸¤ä¸ªä¸åŒçš„æ ‡å‡†åŸºå‘é‡éƒ½æ˜¯æ­£äº¤çš„ã€‚
4.  Decompose $(5,2)$ into components parallel and orthogonal to $(2,1)$.

    å°† $(5,2)$ åˆ†è§£ä¸ºä¸ $(2,1)$ å¹³è¡Œä¸”æ­£äº¤çš„åˆ†é‡ã€‚
5.  Let $\mathbf{u}, \mathbf{v}$ be orthogonal nonzero vectors. (a) Show that $(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=\lVert \mathbf{u}\rVert^2-\lVert \mathbf{v}\rVert^2.$ (b) For what condition on $\mathbf{u}$ and $\mathbf{v}$ does $(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=0$?

    ä»¤ $\mathbf{u}, \mathbf{v}$ ä¸ºæ­£äº¤éé›¶å‘é‡ã€‚
    
    ï¼ˆaï¼‰è¯æ˜ $(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=\lVert \mathbf{u}\rVert^2-\lVert \mathbf{v}\rVert^2.$ 
    
    ï¼ˆbï¼‰ $(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=0$ å¯¹ $\mathbf{u}$ å’Œ $\mathbf{v}$ æ»¡è¶³ä»€ä¹ˆæ¡ä»¶ï¼Ÿ